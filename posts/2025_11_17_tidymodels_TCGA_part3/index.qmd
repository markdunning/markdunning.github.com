---
title: "Tidymodels for omics data: Part 3"
author: Mark Dunning
date: 2025-11-19
theme: darkly
image: preview.png
description: "Machine Learning with glmnet and random forests in R"
---

```{r silentLoad}
#| echo: false
#| message: false
#| warning: false

knitr::opts_chunk$set(
  warning = FALSE,
  message = FALSE
)

library(dplyr)
library(ggplot2)
library(tidymodels)
```

# Pre-amble

In the previous section we described how to download TCGA data for breast cancers and manipulated them using a combination of `tidybulk` and `dplyr` to retain a set of expressed, variable genes plus a set of known cancer genes.

There is a *very extensive* set of clinical information recorded for each sample / patient, but to keep things simple we will start with a task for being able to predict **Estrogen Receptor status** from the expression data, which can be used as an indicator of whether a patient will respond to certain treatments. This is clearly not going to get us a Nature paper or a Nobel prize, but it should work well and introduce some of the key concepts of machine learning. There are a set of packages that we will need:-

```{r installIfNeeded, eval=FALSE}
if(!require(tidymodels)) install.packages("tidymodels")

## install BiocManager to install Bioconductor packages
if(!require(BiocManager)) install.packages("BiocManager")

if(!require(tidybulk)) BiocManager::install("tidybulk")
if(!require(SummarizedExperiment)) BiocManager::install("SummarizedExperiment")

if(!require(dplyr)) install.packages("dplyr")
if(!require(ggplot2)) install.packages("ggplot2")

```

You also need the processed data from the [first section](https://mdbioinformatics.com/posts/2025_11_06_tidymodels_TCGA_part1/) and the code to download this is:-

```{r downloadIfNeeded, eval=FALSE}
### get the saved RDS
dir.create("raw_data", showWarnings = FALSE)
if(!file.exists("raw_data/brca_train_tidy.rds")) download.file("https://github.com/markdunning/markdunning.github.com/raw/refs/heads/master/posts/2025_11_06_tidymodels_TCGA_part1/brca_train_tidy.rds", destfile = "raw_data/brca_train_tidy.rds")
if(!file.exists("raw_data/brca_test_tidy.rds")) download.file("https://github.com/markdunning/markdunning.github.com/raw/refs/heads/master/posts/2025_11_06_tidymodels_TCGA_part1/brca_test_tidy.rds", destfile = "raw_data/brca_test_tidy.rds")

```


Load the pre-prepared tidy data

```{r loadTidy}
brca_train_tidy <- readRDS("raw_data/brca_train_tidy.rds")
brca_test_tidy <- readRDS("raw_data/brca_test_tidy.rds")
```


The previous section was hopefully a gentle introduction to Machine Learning, and we didn't set the bar too high for what for our task. The simple models we built using `ESR1` to classify breast cancer patients into Estrogen Receptor `Positive` or `Negative` were extremely effective (as measured by accuracy, sensitivity and specificity). In reality, faced with this problem there wouldn't be much be much justification in exploring other models when simple models, that are easily explainable perform very well.

Rather than congratulating ourselves lets see how we can cope without ESR1 in our dataset, or any genes that correlate highly with it.

```{r calcESRCor}
brca_train_tidy %>% 
  select(CLEC3A:BRINP2) %>% 
  cor() %>% 
  data.frame() %>% 
  tibble::rownames_to_column("Cor_with_Gene") %>% 
  select(ESR1, Cor_with_Gene) %>% arrange(desc(abs(ESR1))) 
```

Let's proceed by picking genes with an *absolute* correlation < 0.7 with ESR1 as our possible predictors.

```{r keep_genes}
kept_genes <- brca_train_tidy %>% 
  select(CLEC3A:BRINP2) %>% 
  cor() %>% 
  data.frame() %>% 
  tibble::rownames_to_column("Cor_with_Gene") %>% 
  select(ESR1, Cor_with_Gene) %>% #
  filter(abs(ESR1) <0.7) %>% 
  pull(Cor_with_Gene)

```

Now we restrict our data to just these genes, and the Estrogen Receptor status (renamed to `ER` for convenience).

```{r make_train_and_test}
er_train <- brca_train_tidy %>% 
  select(all_of(kept_genes), ER = er_status_by_ihc) %>% 
  mutate(ER = as.factor(ER))

er_test <- brca_test_tidy %>% 
  select(all_of(kept_genes), ER = er_status_by_ihc) %>% 
  mutate(ER = as.factor(ER))
```

## GLMNet

With the number of features (genes) we have in our dataset, even after filtering, there is still the potential for us to suffer from the "curse of dimensionality" and overfit our data. In other words, we could build a model so specific that it is of little use beyond the dataset that it was trained upon.


Methods that provide regularization are very appealing for this reason because they constrain the model, helping it find the optimal combination of features. One such method that implements these concepts in `tidymodels` is called "glmnet".


The amount to which the contribution of less-informative features is "shrunk" is controlled by two arguments: `penalty` (known as $\mathbf{\lambda}$ in the literature) and `mixture` (known as $\mathbf{\alpha}$). $\mathbf{\lambda}$ is a single, non-negative numerical value that controls the strength of the regularization (shrinkage) applied to the model. If $\mathbf{\lambda = 0}$, no penalty is applied, and the model reverts to standard, unregularized Logistic Regression. If $\mathbf{\lambda > 0}$, coefficients are shrunk towards zero to prevent overfitting.$\mathbf{\alpha}$ controls how the features are shrunk and defines the characteristics of the resulting model, ranging between 0 and 1.

**Lasso** ($\mathbf{\alpha = 1}$) forces the coefficients of less important genes exactly to zero, resulting in a sparse model (automatic feature selection). **Ridge** ($\mathbf{\alpha = 0}$) shrinks all coefficients towards zero but never removes any gene completely. Any value between $\mathbf{0 < \alpha < 1}$ is known as the **Elastic Net** penalty. This blend ensures the model benefits from Lasso's feature selection while retaining the stability that Ridge provides when dealing with highly correlated features (a common characteristic in RNA-seq data).

Values such as these, that can take a range of values and affect the fitting of the model, are known as **Hyperparameters**. This is a distinct concept from a parameter, which is a quantity that is estimated during the model fit, such as the coefficients ($\beta$ values) of a regression model. 

We will pick some values of penalty and mixture to see how the model works. In practice, we would want to use a range of values and tune our model to find the best combination, which will be a topic for another time though.

As with other models we have used, the first stage is to create a model specification.

```{r glmnet_spec}
library(tidymodels)

lasso_spec_fixed <- logistic_reg(
  penalty = 0.01,  # Fixed penalty (lambda)
  mixture = 1 # set alpha = 1 for lasso      
) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

```

As before we are using data that have already been processed using domain-specific tools, so there is no need for much in our "recipe" (sometimes you will see removing small variances and filtering here). However, we need to introduce a step that will normalize (or standardise) our features (genes) by subtracting the mean and dividing by the standard deviation. In other words to create a "z-score" that will make all the features comparable and stop any one feature from dominating the model or obscuring the contribution from other features. Features with a higher mean or variance could also be unfairly shrunken by lasso.

```{r glm_recipe}
er_recipe_multigene <- recipe(ER ~ ., data = er_train)  %>% 
  step_normalize(all_predictors())

```

We now combine the recipe and specification into a workflow and fitting the model to the train data should hopefully follow a familiar pattern:-

```{r glm_fit}
# Define the workflow using the fixed specification and your multi-gene recipe
lasso_workflow_fixed <- workflow() %>%
  add_model(lasso_spec_fixed) %>%
  add_recipe(er_recipe_multigene)

# Fit the model directly to your training data
fixed_lasso_fit <- lasso_workflow_fixed %>%
  fit(data = er_train)
```

Printing the model gives us some insight into the modeling process. After printing the recipe and model specification, we see how the model is learning from the data. 

- Lambda ($\lambda$) "**Penalty Strength**". This is the $\lambda$ value applied at that specific step. It decreases as you move down the table.

- Df (Degrees of Freedom) This is the count of genes whose coefficients are non-zero at that $\lambda$ value.

- \%Dev (% Deviance Explained) - **Model Fit on Training Data**.  This is the percentage of deviance (a measure of error, similar to $R^2$) that is explained by the model. A higher value is better.



```{r glm_fit_look}
fixed_lasso_fit
```
Even though we have told the model what value of $\lambda$ we want to use it is trying out different values of $\lambda$ and assessing the model fit at each point. In practical terms, this would allow us to tune the behaviour of the model but that is beyond the scope of this particular section. By printing the coefficients of the model we can see what genes it has shruken to zero and effectively discarded from the model


```{r glm_coef}
# View the coefficients to see the shrinkage/zeroing effect
tidy(fixed_lasso_fit) %>% arrange(estimate)
```

Or as a plot. The magnitude and sign of the coefficient (`estimate`) tell us something about the influence of that gene on the model, which larger (in absolute terms) estimates being more predictive.

```{r fig.height=8}
tidy(fixed_lasso_fit) %>% 
  filter(term != "(Intercept)", estimate != 0) %>% 
  ggplot(aes(y = forcats::fct_reorder(term, estimate),x = estimate, fill = estimate > 0)) + geom_col() + xlab("Model Coefficient") + ylab("Gene")
```

Just to make sure that we understand we can plot the most positive coefficients against `ER` and see these genes are all higher expressed in ER `Positive` tumours.

```{r}
max_coefs <- tidy(fixed_lasso_fit) %>% 
  filter(term != "(Intercept)") %>% 
  slice_max(estimate, n = 5) %>% 
  pull(term)

er_train %>% 
  select(ER, all_of(max_coefs)) %>% 
  pivot_longer(-ER,names_to = "Gene", values_to = "count") %>% 
ggplot(aes(x = ER, y = count)) + geom_boxplot() + facet_wrap(~Gene)

```
Conversely, genes where a negative coefficient was predicted are all higher in ER `Negative` (or equivalently lower in ER `Positive`)

```{r}
min_coefs <- tidy(fixed_lasso_fit) %>% 
  filter(term != "(Intercept)") %>% 
  slice_min(estimate, n = 5) %>% 
  pull(term)

er_train %>% 
  select(ER, all_of(min_coefs)) %>% 
  pivot_longer(-ER,names_to = "Gene", values_to = "count") %>% 
ggplot(aes(x = ER, y = count)) + geom_boxplot() + facet_wrap(~Gene)
```
And the next step is of course to see how the model performs on *un-seen* or new data, and we created the `er_test` dataset for this purpose. The `tidymodels` eco-system uses `yardstick` to efficiently compile metrics and we will use accuracy, specificity and sensitivity as defined in the previous section.

```{r}
class_metrics <- metric_set(accuracy, specificity, sensitivity)

fixed_lasso_fit |>
  predict(new_data = er_test) |>
  bind_cols(er_test) |>
  class_metrics(truth = ER, estimate = .pred_class)
```

Overall the model is doing quite well even though we didn't use `ESR1`. The specificity is quite high, meaning that it doesn't pick too many false positives. On the other hand, the lower sensitivity means that some samples that are truly `Positive` are being missed. 

::: {.callout-important}

## Clinical context

In a clinical context, the high specificity is good as you wouldn't progress too many patients to potentially aggressive treatment they wouldn't benefit from. On the other hand, some patients may miss the opportunity to take a potentially life-changing or life-extending treatment.

:::


There is clearly an opportunity for some improvement, and possible that could be achieved by better choice of *hyper-parameters*. For now though lets see how other methods perform starting with the decision tree.

## Decision Trees to "Random Forests"

Thanks for the `tidymodels` philosophy, we can actually *reuse* quite a lot of the code from before. We first need a specification.

```{r}
decision_tree_spec <- decision_tree() %>% 
  set_engine("rpart") %>%       
  set_mode("classification")
```

The workflow then combines the specifiction with the same recipe from above.

```{r}
decision_tree_workflow_fixed <- workflow() %>%
  add_model(decision_tree_spec) %>%
  add_recipe(er_recipe_multigene)

# Fit the model directly to your training data
decision_tree_fit <- decision_tree_workflow_fixed %>%
  fit(data = er_train)

decision_tree_fit
```
The fit looks quite complex, but let's plot it out.


```{r}
library(rpart.plot)
rpart.plot(extract_fit_engine(decision_tree_fit))

```
Now that the decision tree doesn't have `ESR1` to rely on,the model found the next strongest predictor available: **TFF1**, splitting at a scaled value of $-0.86$. Although TFF1 is strongly correlated with ER status, it is not powerful enough to separate the classes cleanly on its own.

Furthermore, unlike the initial `ESR1`-based tree (seen in the previous section), this tree is very deep and involves splits on numerous secondary genes (**GRPR**, **KIF1A**, **YP2B7P**, **HOTAIR**, etc.). The model is relying on the cumulative effect of many weak, complex, and potentially non-linear interactions among many genes to achieve classification. There is a danger that we might be over-fitting.

Nevertheless, we can see how the model performs:-


```{r}
decision_tree_fit |>
  predict(new_data = er_test) |>
  bind_cols(er_test) |>
  class_metrics(truth = ER, estimate = .pred_class)
```

The specificity is roughly the same as our Lasso from above, but the sensitivity is worse still. It seems like it is about time to introduce a method that is an evolution from the Decision Tree - that of a Random Forest.

## Introducing the Random Forest

The random forest is a natural extension of a decision tree, as it harnesses the power of many different decision tree and uses the predictions from each individual tree to form a consensus or majority vote for each sample to be classified.


```{r}
set.seed(42)

random_forest_spec <- rand_forest(
    mode = "classification", 
    trees = 1000 # Using 1000 trees for robustness
  ) %>%
  set_engine("ranger", importance = "none")

# 2. Create the Random Forest Workflow
# We reuse the existing recipe (er_recipe_multigene)
random_forest_workflow <- workflow() %>%
  add_model(random_forest_spec) %>%
  add_recipe(er_recipe_multigene)

# 3. Fit the model to the training data
# This step trains 1000 trees on your ESR1-free data.
random_forest_fit <- random_forest_workflow %>%
  fit(data = er_train)

random_forest_fit
```

```{r}
  random_forest_fit %>% 
  predict(new_data = er_test) |>
  bind_cols(er_test) |>
  class_metrics(truth = ER, estimate = .pred_class)
```

```{r}
rf_importance_spec <- rand_forest(
    mode = "classification", 
    trees = 1000 
  ) %>%
  set_engine("ranger", importance = "permutation") # <--- CRITICAL CHANGE

# 2. Re-fit the workflow
rf_importance_workflow <- random_forest_workflow %>%
  update_model(rf_importance_spec) # Update the model engine setting
  
rf_importance_fit <- rf_importance_workflow %>%
  fit(data = er_train)

# 3. Extract the importance scores
# The scores are typically accessed via the pull_workflow_fit() function
importance_df <- pull_workflow_fit(rf_importance_fit)$fit$variable.importance %>%
  enframe(name = "Gene", value = "Importance") %>%
  arrange(desc(Importance))

# Display the top 10 most important genes
head(importance_df, 10)
```

## Other models: SVM


## Other models: KNN