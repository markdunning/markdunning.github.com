---
title: "Tidymodels for omics data: Part 4"
author: Mark Dunning
date: 2025-11-22
theme: darkly
image: preview.png
description: "Optimising Machine Leaerning Performance by cross-validation and tuning"
---

```{r silentLoad}
#| echo: false
#| message: false
#| warning: false

knitr::opts_chunk$set(
  warning = FALSE,
  message = FALSE
)

library(dplyr)
library(ggplot2)
library(tidymodels)
```

## Pre-amble

In [Part 1 of this series](https://mdbioinformatics.com/posts/2025_11_06_tidymodels_TCGA_part1/index.html) we described how to download TCGA data for breast cancers and manipulated them using a combination of `tidybulk` and `dplyr` to retain a set of expressed, variable genes. There are a set of packages that we will need for this Part 3b:-

```{r installIfNeeded, eval=FALSE}
if(!require(tidymodels)) install.packages("tidymodels")


if(!require(dplyr)) install.packages("dplyr")
if(!require(ggplot2)) install.packages("ggplot2")
```

You will need the processed data from the [Part one of this series](https://mdbioinformatics.com/posts/2025_11_06_tidymodels_TCGA_part1/) and the code to download this is:-

```{r downloadIfNeeded, eval=FALSE}
### get the saved RDS
dir.create("raw_data", showWarnings = FALSE)
if(!file.exists("raw_data/brca_train_tidy.rds")) download.file("https://github.com/markdunning/markdunning.github.com/raw/refs/heads/master/posts/2025_11_06_tidymodels_TCGA_part1/brca_train_tidy.rds", destfile = "raw_data/brca_train_tidy.rds")
if(!file.exists("raw_data/brca_test_tidy.rds")) download.file("https://github.com/markdunning/markdunning.github.com/raw/refs/heads/master/posts/2025_11_06_tidymodels_TCGA_part1/brca_test_tidy.rds", destfile = "raw_data/brca_test_tidy.rds")

```

The main purpose of this Part of the series is to introduce some other classification methods before proceeding to more complex topics to as tuning our models. First we will load the pre-prepared tidy data

```{r loadTidy}
brca_train_tidy <- readRDS("raw_data/brca_train_tidy.rds")
brca_test_tidy <- readRDS("raw_data/brca_test_tidy.rds")
```

Let's proceed by picking genes with an *absolute* correlation \< 0.7 with ESR1 as our possible predictors.

```{r keep_genes}
kept_genes <- brca_train_tidy %>% 
  select(CLEC3A:BRINP2) %>% 
  cor() %>% 
  data.frame() %>% 
  tibble::rownames_to_column("Cor_with_Gene") %>% 
  select(ESR1, Cor_with_Gene) %>% #
  filter(abs(ESR1) <0.7) %>% 
  pull(Cor_with_Gene)

```

Now we restrict our data to just these genes, and the Estrogen Receptor status (renamed to `ER` for convenience). We will also create some ID rows in the data, which will be helpful in a short while.

```{r make_train_and_test}
er_train <- brca_train_tidy %>% 
  select(all_of(kept_genes), ER = er_status_by_ihc) %>% 
  mutate(ER = as.factor(ER)) %>% 
  mutate(ID = paste0("Training_", row_number())) %>% 
  relocate(ID)

er_test <- brca_test_tidy %>% 
  select(all_of(kept_genes), ER = er_status_by_ihc) %>% 
  mutate(ER = as.factor(ER)) %>% 
  mutate(ID = paste0("Training_", row_number())) %>% 
  relocate(ID)
```

In [Part 3a](https://mdbioinformatics.com/posts/2025_11_17_tidymodels_TCGA_part3/#introducing-glmnet) we introduced some machine learning methods that relied on hyperparameters in order to be run. This included logistic regression using glmnet that can "shrink" the influence of features in our data. The amount of shrinkage is controlled by the arguments `penalty` ($\lambda$) and `mixture` ($\alpha$) in the tidymodels specification.

We just set these to some arbitrary, fixed values. So, the question is: How can we choose the "best" values for these parameters without endless trial and error, and what data should we base that decision on?

Way back in [Part 1](https://mdbioinformatics.com/posts/2025_11_06_tidymodels_TCGA_part1/#preparing-the-data-for-machine-learning) we split a TCGA breast cancer dataset into training and testing portions that were completely independent. The roles of these two portions are strictly defined:

- Training Dataset: Used only to estimate the internal coefficients of our model and create the final fit. In our example of a logsitic regression this would be the amount to which each gene affects the log-odds of a particular sample being ER `Positive`.

- Testing Dataset: Used only for the final, independent validation of the model's performance on completely unseen data.

Neither of these is appropriate for tuning. You might think the training data would be a good choice, but using it directly to pick the hyperparameters would make our model overfit. It would be optimised for the training data and potentially perform poorly on the test set or other data.

So the solution is to create some new, temporary validation sets, based on our original training data, solely for the purpose of identifying (or "tuning") the best hyperparameters. This technique is known as Cross-Validation (CV).



## Cross-Validation

K-fold cross-validation (which `tidymodels` refers to as V-fold cross-validation) splits data into **v** equally-sized partitions to be used to tune hyper-parameters.

```{r setup_v_folds}
set.seed(42)

er_folds <- er_train %>%
  vfold_cv(v = 10, 
           strata = ER)

```

Each of our `v` splits comprises a training and testing set

```{r show_training_split}
er_folds$splits[[1]]
er_folds$splits[[1]] %>% training()
```

```{r show_split_testing}
er_folds$splits[[1]] %>% testing()
```


```{r check_training_testing_rows}
nrow(training(er_folds$splits[[1]])) + nrow(testing(er_folds$splits[[1]]))
nrow(er_train)
```

With the same reasoning as creating our initial training and testing set, none of the samples included in the training set at each split is found in the corresponding testing set.

```{r training_testing_intersect}
intersect(training(er_folds$splits[[1]])$ID, testing(er_folds$splits[[1]])$ID)
```


## Model specifiction and recipe

Now when we create our model specification we declare that `penalty` and `mixture` are going to be tuned. Our "recipe" uses the same formula as before, but need to be careful as we introduce an ID column into the data. The workflow is also created at this stage.

```{r}
lasso_spec_tune <- logistic_reg(
  penalty = tune(), #Penalty (lambda): Use tune() to test different shrinkage strengths.
     # Mixture (alpha): Use tune() to test the balance between Lasso (1) and Ridge (0) regularization, defining the Elastic Net.
  mixture = tune() 
) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

er_recipe_multigene <- recipe(ER ~ ., data = er_train)  %>% 
  update_role(ID, new_role = "ID") %>% 
  step_normalize(all_numeric_predictors())  
   ## Need to say that our ID column is an identifier - not something to be included in the model


elastic_net_wflow <- workflow() %>%
  add_recipe(er_recipe_multigene) %>%
  add_model(lasso_spec_tune)
```


Unfortunately there are an *infinite* number of possibilities that both these arguments can take and we clearly cannot test them all. What we do instead is try and explore the full range of values, and find roughly the region where the optimal values are located. In practical terms this is often perfectly adequate. 

Infrastructure for tuning parameters is support by `dials` and `tune` which are both part of the `tidymodels` eco-system. i.e. they get installed and loaded when you install and load `tidymodels`.

- [https://dials.tidymodels.org/](https://dials.tidymodels.org/)
- [https://tune.tidymodels.org/](https://tune.tidymodels.org/)

There are a couple of convenient helper functions `penalty` and `mixture` that will create templates for choosing values for tuning. We then have to create a *grid* of values that we want to test. This is simply a data frame (/tibble) containing combinations of `penalty` and `mixture` and can be created with the `grid_regular` function. This function will ensure that the full range of values is tested. In other words that the maximum and minimum values are included in the search. With the `levels` argument we can control how finely we split the ranges.

Another option is to use `grid_random` to create a *random* set of combinations of the parameters. Instead of `levels` we say how many combinations we want in total using `size`.

```{r}
penalty_range <- penalty(range = c(-5, 0)) # use helper function to define range of penalty values
mixture_range <- mixture() # use helper function to define a range of mixture values

elastic_net_grid_regular <- grid_regular(
  penalty_range, 
  mixture_range,
  levels = c(penalty = 5, mixture = 3) # Test 5 penalty values x 3 mixture values = 15 combinations
)

elastic_net_grid_random <- grid_random(
  penalty_range, 
  mixture_range,
  size = 15
)

```

With the setup we have just created there are 15 different models to be fit (one model for each combination in our grid), and the 10-fold strategy we defined with `vfolds_cv` means we have to fit each of these 15 models 10 times. In total we have 150 models, and for each we want to assess the performance. This is quite a lot of work! Fortunately we can do this quite efficiently with `tidymodels` and get an output that we can assess. The `tune_grid` function will use the cross-validation strategy defined with `vfolds_cv`, a grid structure and compute performance metrics for your workflow. 

```{r}
elastic_net_tune_res <- tune_grid(
  elastic_net_wflow, 
  resamples = er_folds, 
  grid = elastic_net_grid_regular,
  metrics = metric_set(accuracy, specificity, sensitivity, roc_auc)
)
```


```{r}
elastic_net_tune_res
```

```{r}
show_best(elastic_net_tune_res, metric = "roc_auc", n = 5)

```

```{r}
show_best(elastic_net_tune_res, metric = "sensitivity", n=5)
```

