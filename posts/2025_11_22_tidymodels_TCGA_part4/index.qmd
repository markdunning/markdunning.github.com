---
title: "Tidymodels for omics data: Part 4"
author: Mark Dunning
date: 2025-11-22
theme: darkly
image: preview.png
description: "Optimising Machine Leaerning Performance by cross-validation and tuning"
---

```{r silentLoad}
#| echo: false
#| message: false
#| warning: false

knitr::opts_chunk$set(
  warning = FALSE,
  message = FALSE
)

library(dplyr)
library(ggplot2)
library(tidymodels)
```

## Pre-amble

In [Part 1 of this series](https://mdbioinformatics.com/posts/2025_11_06_tidymodels_TCGA_part1/index.html) we described how to download TCGA data for breast cancers and manipulated them using a combination of `tidybulk` and `dplyr` to retain a set of expressed, variable genes. There are a set of packages that we will need for this Part 3b:-

```{r installIfNeeded, eval=FALSE}
if(!require(tidymodels)) install.packages("tidymodels")


if(!require(dplyr)) install.packages("dplyr")
if(!require(ggplot2)) install.packages("ggplot2")
```

You will need the processed data from the [Part one of this series](https://mdbioinformatics.com/posts/2025_11_06_tidymodels_TCGA_part1/) and the code to download this is:-

```{r downloadIfNeeded, eval=FALSE}
### get the saved RDS
dir.create("raw_data", showWarnings = FALSE)
if(!file.exists("raw_data/brca_train_tidy.rds")) download.file("https://github.com/markdunning/markdunning.github.com/raw/refs/heads/master/posts/2025_11_06_tidymodels_TCGA_part1/brca_train_tidy.rds", destfile = "raw_data/brca_train_tidy.rds")
if(!file.exists("raw_data/brca_test_tidy.rds")) download.file("https://github.com/markdunning/markdunning.github.com/raw/refs/heads/master/posts/2025_11_06_tidymodels_TCGA_part1/brca_test_tidy.rds", destfile = "raw_data/brca_test_tidy.rds")

```

The main purpose of this Part of the series is to introduce some other classification methods before proceeding to more complex topics to as tuning our models. First we will load the pre-prepared tidy data

```{r loadTidy}
brca_train_tidy <- readRDS("raw_data/brca_train_tidy.rds")
brca_test_tidy <- readRDS("raw_data/brca_test_tidy.rds")
```

Let's proceed by picking genes with an *absolute* correlation \< 0.7 with ESR1 as our possible predictors.

```{r keep_genes}
kept_genes <- brca_train_tidy %>% 
  select(CLEC3A:BRINP2) %>% 
  cor() %>% 
  data.frame() %>% 
  tibble::rownames_to_column("Cor_with_Gene") %>% 
  select(ESR1, Cor_with_Gene) %>% #
  filter(abs(ESR1) <0.7) %>% 
  pull(Cor_with_Gene)

```

Now we restrict our data to just these genes, and the Estrogen Receptor status (renamed to `ER` for convenience). We will also create some ID rows in the data, which will be helpful in a short while.

```{r make_train_and_test}
er_train <- brca_train_tidy %>% 
  select(all_of(kept_genes), ER = er_status_by_ihc) %>% 
  mutate(ER = as.factor(ER)) %>% 
  mutate(ID = paste0("Training_", row_number())) %>% 
  relocate(ID)

er_test <- brca_test_tidy %>% 
  select(all_of(kept_genes), ER = er_status_by_ihc) %>% 
  mutate(ER = as.factor(ER)) %>% 
  mutate(ID = paste0("Training_", row_number())) %>% 
  relocate(ID)
```

In [Part 3a](https://mdbioinformatics.com/posts/2025_11_17_tidymodels_TCGA_part3/#introducing-glmnet) we introduced some machine learning methods that relied on hyperparameters in order to be run. This included logistic regression using glmnet that can "shrink" the influence of features in our data. The amount of shrinkage is controlled by the arguments `penalty` ($\lambda$) and `mixture` ($\alpha$) in the tidymodels specification.

We just set these to some arbitrary, fixed values. So, the question is: How can we choose the "best" values for these parameters without endless trial and error, and what data should we base that decision on?

Way back in [Part 1](https://mdbioinformatics.com/posts/2025_11_06_tidymodels_TCGA_part1/#preparing-the-data-for-machine-learning) we split a TCGA breast cancer dataset into training and testing portions that were completely independent. The roles of these two portions are strictly defined:

-   Training Dataset: Used only to estimate the internal coefficients of our model and create the final fit. In our example of a logsitic regression this would be the amount to which each gene affects the log-odds of a particular sample being ER `Positive`.

-   Testing Dataset: Used only for the final, independent validation of the model's performance on completely unseen data.

Neither of these is appropriate for tuning. You might think the training data would be a good choice, but using it directly to pick the hyperparameters would make our model overfit. It would be optimised for the training data and potentially perform poorly on the test set or other data.

So the solution is to create some new, temporary validation sets, based on our original training data, solely for the purpose of identifying (or "tuning") the best hyperparameters. This technique is known as Cross-Validation (CV).

::: callout-important
## Expectation setting

The purpose of tuning our models is really just thatâ€”fine-tuning to get the absolute best performance. It will not suddenly elevate performance metrics (like $\text{ROC AUC}$ or Accuracy) from around $0.6$ to $0.9$. Tuning should only be used in circumstances where you are already satisfied with the fundamental performance of your model and just require some minor, incremental adjustments. The performance gains from tuning are typically small (e.g. ).

That is to say that you should consider other choices of model, and even the fundamental data processing and feature engineering first, before trying to "tune" a model that is performing sub-par.

**The greatest performance gains come from better data and model choice; tuning only optimizes the model you already have.**
:::

## Cross-Validation

K-fold cross-validation (which `tidymodels` refers to as V-fold cross-validation) splits data into **v** equally-sized partitions to be used to tune hyper-parameters.

```{r setup_v_folds}
set.seed(42)

er_folds <- er_train %>%
  vfold_cv(v = 10, 
           strata = ER)

```

Each of our `v` splits comprises a training and testing set, which can be accessed using the same `training` and `testing` functions from before.

```{r show_training_split}
er_folds$splits[[1]]
er_folds$splits[[1]] %>% training()
```

```{r show_split_testing}
er_folds$splits[[1]] %>% testing()
```

With the same reasoning as creating our initial training and testing set, none of the samples included in the training set at each split is found in the corresponding testing set.

```{r training_testing_intersect}
intersect(training(er_folds$splits[[1]])$ID, testing(er_folds$splits[[1]])$ID)
```

## Model specifiction and recipe

Now when we create our model specification we declare that `penalty` and `mixture` are going to be tuned. Our "recipe" uses the same formula as before, but need to be careful as we introduce an ID column into the data. The workflow is also created at this stage.

```{r}
lasso_spec_tune <- logistic_reg(
  penalty = tune(), #Penalty (lambda): Use tune() to test different shrinkage strengths.
     # Mixture (alpha): Use tune() to test the balance between Lasso (1) and Ridge (0) regularization, defining the Elastic Net.
  mixture = tune() 
) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

er_recipe_multigene <- recipe(ER ~ ., data = er_train)  %>% 
  update_role(ID, new_role = "ID") %>% 
  step_normalize(all_numeric_predictors())  
   ## Need to say that our ID column is an identifier - not something to be included in the model


elastic_net_wflow <- workflow() %>%
  add_recipe(er_recipe_multigene) %>%
  add_model(lasso_spec_tune)
```

## Setup a grid for tuning

Unfortunately there are an *infinite* number of possibilities that both these arguments can take and we clearly cannot test them all. What we do instead is try and explore the full range of values, and find roughly the region where the optimal values are located. In practical terms this is often perfectly adequate.

Infrastructure for tuning parameters is support by `dials` and `tune` which are both part of the `tidymodels` eco-system. i.e. they get installed and loaded when you install and load `tidymodels`.

-   <https://dials.tidymodels.org/>
-   <https://tune.tidymodels.org/>

There are a couple of convenient helper functions `penalty` and `mixture` that will create templates for choosing values for tuning. We then have to create a *grid* of values that we want to test. This is simply a data frame (/tibble) containing combinations of `penalty` and `mixture` and can be created with the `grid_regular` function. This function will ensure that the full range of values is tested. In other words that the maximum and minimum values are included in the search. With the `levels` argument we can control how finely we split the ranges.

Another option is to use `grid_random` to create a *random* set of combinations of the parameters. Instead of `levels` we say how many combinations we want in total using `size`.

```{r setup_grids}
penalty_range <- penalty(range = c(-5, 0)) # use helper function to define range of penalty values
mixture_range <- mixture() # use helper function to define a range of mixture values

elastic_net_grid_regular <- grid_regular(
  penalty_range, 
  mixture_range,
  levels = c(penalty = 5, mixture = 3) # Test 5 penalty values x 3 mixture values = 15 combinations
)

elastic_net_grid_random <- grid_random(
  penalty_range, 
  mixture_range,
  size = 15
)

```

With the setup we have just created there are 15 different models to be fit (one model for each combination in our grid), and the 10-fold strategy we defined with `vfolds_cv` means we have to fit each of these 15 models 10 times. In total we have 150 models, and for each we want to assess the performance. This is quite a lot of work! Fortunately we can do this quite efficiently with `tidymodels` and get an output that we can assess. The `tune_grid` function will use the cross-validation strategy defined with `vfolds_cv`, a grid structure and compute performance metrics for your workflow.

## Inspect tuning results

```{r tune_grid}
elastic_net_tune_res <- tune_grid(
  elastic_net_wflow, 
  resamples = er_folds, 
  grid = elastic_net_grid_regular,
  metrics = metric_set(accuracy, specificity, sensitivity, roc_auc)
)
```

The actual object itself is not particularly informative when printed:-

```{r print_tune_res}
elastic_net_tune_res 
```

There is however a handy plotting method that will give a quick overview of the metrics achieved for different runs of the model. On the whole, it looks like a mixture of 0 is best with little difference as the penalty (amount of regularization) changes. Recall that a mixture of 0 corresponds to *Ridge*, as opposed to *Lasso* meaning that the final model that we fit will not set any coefficients to zero.

```{r plot_tune_res}
elastic_net_tune_res %>% autoplot
```

We can also request details of the "best" models according to a particular metric. ROC AUC is generally a good idea as it balances the specificity and sensitivity. The metrics shown in the table are averaged over the v-folds, which is 10 in our case.

```{r best_auc}
show_best(elastic_net_tune_res, metric = "roc_auc", n = 5)

```

If we want to concentrate on a specific measure (such as sensitivity), we can look at that too.

```{r best_sens}
show_best(elastic_net_tune_res, metric = "sensitivity", n=5)
```

## Create final model and fit

Once we are happy, we can save the details of the best model.

```{r get_best_params}
best_params <- select_best(elastic_net_tune_res, metric = "roc_auc")
best_params
```

And the choice of parameters can be fed back to our workflow. Our workflow up to this point only has placeholders for the `mixture` and `penalty`.

```{r workflow_before_tuning}
elastic_net_wflow
```

With `finalize_workflow` these values get updated according to our final choices.

```{r finalise_wflow}

final_wflow <- elastic_net_wflow %>%
  finalize_workflow(best_params)
final_wflow
```

Remember that the purpose of tuning was to find an optimal set of hyperparameters, and we did that using a newly-created set of cross-validation subsets. We still need to fit, our create, the final model using those paramaters that we chose. This still needs to be performed on our entire training dataset.

```{r final_fit}
final_model_fit <- fit(final_wflow, data = er_train)
final_model_fit %>% tidy
```

Finally we can make predictions using our testing data **which hasn't been used in any of the previous steps** and assess the metrics

```{r}
class_metrics <- metric_set(accuracy, sensitivity, specificity)
select(er_test ,ER) %>% 
  bind_cols(predict(final_model_fit, new_data = er_test)) %>% 
  class_metrics(truth = ER, estimate = .pred_class)
```

The accuracy, sensitivity and specificity from our previous logistic regression were 0.920, 0.857 and 0.939 respectively so it looks like we are improving our sensitivity at the cost of a slight decrease in specificity. After tuning we have ended up with a model that is doing better at predicting both positive and negative cases .ðŸŽ‰

## Training for Random Forests

We'll now walk through an example of tuning a random forest, and hopefully you will see that the process is roughly the same. A useful starting point is the `show_model_info` function that will give us plenty of information about using a particular machine learning methods, the "engines" that can be used and what arguments can be set.

```{r recall_random_forest}
show_model_info("rand_forest")
```


In the output it lists `mtry`, `trees` and `min_n` all as arguments that can be tuned.


```{r create_rf_spec}
rf_spec_tune <- rand_forest(
  mtry = tune(),min_n = tune()) %>%
  set_engine("ranger") %>%
  set_mode("classification")
```

and now create the workflow

```{r create_rf_workflow}
rf_workflow <- workflow() %>% 
  add_model(rf_spec_tune) %>% 
  add_recipe(er_recipe_multigene)
  
```


```{r make_rf_grid}
mtry_range <- mtry(range = c(5, 50)) 

min_n_range <- min_n(range = c(5, 40))

trees_range <- trees(range = c(500, 2000))

set.seed(42) # Set seed for reproducibility of random sampling
rf_grid_random <- grid_random(
  mtry_range, 
  min_n_range, 
#  trees_range,
  size = 5 # Test 15 random combinations of the three hyperparameters
)

rf_grid_random

```

```{r tune_rf_params}
elastic_net_tune_res <- tune_grid(
  rf_workflow, 
  resamples = er_folds, 
  grid = rf_grid_random,
  metrics = metric_set(accuracy, specificity, sensitivity)
)
```

```{r}
elastic_net_tune_res %>% autoplot()
```

```{r}
show_best(elastic_net_tune_res)
```

