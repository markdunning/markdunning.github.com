[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dr.¬†Mark Dunning",
    "section": "",
    "text": "I‚Äôm a Bioinformatician with a background in Mathematics and Computer Science with over 20 years of experience. I specialise in the visualisation and interpretation of complex data and have a wealth of experience in making results accessible and easy to interrogate for wet-lab biologists. I also have experience in developing and delivering training courses and workships, along with formal teaching.\nI obtained my PhD in the Statistics and Computational Biology group of Simon Tavar√© at the University of Cambridge / Cancer Research Uk. During this time, I developed the beadarray Bioconductor package for the analysis of Illumina microarray data. My PhD thesis is available online, should you be interested.\nAfter my PhD, I worked as a Bioinformatics Analyst within the Bioinformatics Core; consulting on, and assisting in, the analysis on all types of high-throughput datasets. During this time I worked on studies to define subtypes of Breast and Prostate cancer and established pipelines for the analysis of whole-genome and exome resequencing data.\nI held the role of ‚ÄúBioinformatics Training Coordinator‚Äù in the Bioinformatics Core of Cancer Research Uk Cambridge Institute. I organised, developed and facilitated Bioinformatics training courses to teach computational and analytical skills to wet-lab biologists - along with a series of ‚ÄúBioinformatics Summer Schools‚Äù for Cancer Research funded staff nationwide.\nFrom October 2017 I established the Bioinformatics Core at The University of Sheffield which ran until August 2025. Along with supporting clinicians and researchers across campus, I also contributed to various MSC and undergraduate programs as well as obtaining training / teaching qualifications from thecarpentries and AdvanceHE (FHEA)"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "training/r/index.html",
    "href": "training/r/index.html",
    "title": "Introduction to R",
    "section": "",
    "text": "Disclaimer\n\nAlthough R is well-regarded as a tool for performing statistical analysis, this workshop will not explicitly teach stats. Instead we give introduce the tools that we allow you to manipulate and interrogate your data into a form with which you can execute statistical tests.\n\n\n\nSetup\nIf you are following these notes independently (outside one of our workshops)\nFrom the RStudio menus, Choose the File -&gt; New Project option and select New Directory from the new window\n\nThen for the Project Type pick New Project.\n\nIt will ask you to pick a new Directory name, and where to create that directory (e.g.¬†your Home directory or directory where you usually save your work)\n\nRStudio should now refresh itself. You can now download the data required for the workshop by copying and pasting the following into the R console (as shown in the screenshot)\n\ndownload.file(\"https://github.com/sheffield-bioinformatics-core/r-online/raw/master/CourseData.zip\", destfile = \"CourseData.zip\")\n\n The files from the zip file can be extracted using the command:-\n\nunzip(\"CourseData.zip\")\n\nYour RStudio screen should look like:-\n\nYou will need to install some R packages and download some data before you start. You can install the packages by copying and pasting the following into an R console and pressing ENTER\n\ninstall.packages(\"dplyr\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"readr\")\ninstall.packages(\"rmarkdown\")\ninstall.packages(\"tidyr\")\n\nYou can check that this worked by copying and pasting the following:-\n\nsource(\"https://raw.githubusercontent.com/sheffield-bioinformatics-core/r-online/master/check_packages.R\")\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nThe dplyr package has been installed\n\n\nThe ggplot2 package has been installed\n\n\nThe readr package has been installed\n\n\nThe rmarkdown package has been installed\n\n\nThe tidyr package has been installed\n\n\nYou have successfully installed all the packages required for the course\n\n\nIf you want to follow along with the R code on this webpage, you can open the file part1.Rmd from the bottom-right corner of RStudio\n\nThere are equivalent markdown files (part2.Rmd, part3.Rmd) for the other sections of the course. Enjoy!"
  },
  {
    "objectID": "training.html",
    "href": "training.html",
    "title": "Training Materials",
    "section": "",
    "text": "Here is a collection of Bioinformatics and Data Analysis materials that I have created and taught over the years. Please feel free to browse, and get in touch if you would like them to be taught at your institute.\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to R - Part 1\n\n14 min\n\nIn these materials, we explore fundamental operations of R and load some example data\n\n\n\nOct 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to R - Part 2\n\n20 min\n\nStarting to play with data with the dplyr and ggplot2 packages\n\n\n\nOct 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to R - Part 3\n\n21 min\n\nFurther data exploration and manipulation with ggplot2 and dplyr\n\n\n\nOct 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to RNA-Seq - Part 1\n\n28 min\n\nImporting ‚Äòraw‚Äô RNA-seq counts into R and performing quality assessment\n\n\n\nOct 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to RNA-Seq - Part 2\n\n28 min\n\nPerforming differential expression on Bulk RNA-seq data using DESeq2\n\n\n\nOct 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to RNA-Seq - Part 3\n\n50 min\n\nFurther exploration of differential expression, followed by identification of biological pathways of interest\n\n\n\nOct 30, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "training/r_part2/index.html",
    "href": "training/r_part2/index.html",
    "title": "Introduction to R - Part 2",
    "section": "",
    "text": "Choosing which columns to show from the data\nChoosing what rows to keep in the data\nAdding / altering columns\nSorting the rows in our data\nIntroduction to plotting"
  },
  {
    "objectID": "training/bulk-rnaseq_1/index.html",
    "href": "training/bulk-rnaseq_1/index.html",
    "title": "Introduction to RNA-Seq - Part 1",
    "section": "",
    "text": "High-throughput sequencing is now established as a standard technique for many functional genomics studies; allowing the researcher to compare and contrast the transcriptomes of many individuals to obtain biological insight. A high-volume of data are generated from these experimental techniques and thus require robust and reproducible tools to be employed in the analysis.\nIn this workshop, you will be learning how to analyse RNA-seq count data, using R. This will include reading the data into R, quality control and performing differential expression analysis and gene set testing, with a focus on the well-respected DESEq2 analysis workflow. You will learn how to generate common plots for analysis and visualisation of gene expression data, such as boxplots and heatmaps.\n\n\n\n\n\n\nNote\n\n\n\nWe will be discussing Bulk RNA-seq only, although some of the methods and techniques will be applicable to single-cell RNA-seq. I am planning some materials on single-cell RNA-seq in the future. In the meantime, the homepage for Seurat (a popular R package for single-cell analysis) has lots of useful tutorials.\n\nSeurat homepage"
  },
  {
    "objectID": "training/r_part1/index.html",
    "href": "training/r_part1/index.html",
    "title": "Introduction to R - Part 1",
    "section": "",
    "text": "Basic calculations in R\nUsing functions\nGetting help\nSaving data using variables\nReading a spreadsheet into R"
  },
  {
    "objectID": "training/r_part3/index.html",
    "href": "training/r_part3/index.html",
    "title": "Introduction to R - Part 3",
    "section": "",
    "text": "Customising ggplot2 plots\nSummarising data\nGroup-based summaries\nJoining data\nData Cleaning\n\nLets make sure we have read the gapminder data into R and have the relevant packages loaded.\n\n## Checks if the required file is present, and downloads if not\n\nif(!file.exists(\"raw_data/gapminder.csv\")) {\n  dir.create(\"raw_data/\",showWarnings = FALSE)\ndownload.file(\"https://raw.githubusercontent.com/markdunning/markdunning.github.com/refs/heads/master/files/training/r/raw_data/gapminder.csv\", destfile = \"raw_data/gapminder.csv\")\n}\n\nWe also discussed in the previous part(s) how to read the example dataset into R. We will also load the libraries needed.\n\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\ngapminder &lt;- read_csv(\"raw_data/gapminder.csv\")"
  },
  {
    "objectID": "training/r_part1/index.html#topics-covered",
    "href": "training/r_part1/index.html#topics-covered",
    "title": "Introduction to R - Part 1",
    "section": "",
    "text": "Basic calculations in R\nUsing functions\nGetting help\nSaving data using variables\nReading a spreadsheet into R"
  },
  {
    "objectID": "training/r_part1/index.html#variables",
    "href": "training/r_part1/index.html#variables",
    "title": "Introduction to R - Part 1",
    "section": "Variables",
    "text": "Variables\nA variable is a letter or word which takes (or contains) a value. We use the assignment ‚Äòoperator‚Äô, &lt;- to create a variable and store some value in it.\n\nx &lt;- 10\nx\n\n[1] 10\n\nmyNumber &lt;- 25\nmyNumber\n\n[1] 25\n\n\nWe also can perform arithmetic on variables using functions:\n\nsqrt(myNumber)\n\n[1] 5\n\n\nWe can add variables together:\n\nx + myNumber\n\n[1] 35\n\n\nWe can change the value of an existing variable:\n\nx &lt;- 21\nx\n\n[1] 21\n\n\nWe can set one variable to equal the value of another variable:\n\nx &lt;- myNumber\nx\n\n[1] 25\n\n\nWhen we are feeling lazy we might give our variables short names (x, y, i‚Ä¶etc), but a better practice would be to give them meaningful names. There are some restrictions on creating variable names. They cannot start with a number or contain characters such as . and ‚Äò-‚Äô. Naming variables the same as in-built functions in R, such as c, T, mean should also be avoided.\nNaming variables is a matter of taste. Some conventions exist such as a separating words with - or using camelCaps. Whatever convention you decided, stick with it!"
  },
  {
    "objectID": "training/r_part1/index.html#functions",
    "href": "training/r_part1/index.html#functions",
    "title": "Introduction to R - Part 1",
    "section": "Functions",
    "text": "Functions\nFunctions in R perform operations on arguments (the inputs(s) to the function). We have already used:\n\nsin(x)\n\n[1] -0.1323518\n\n\nthis returns the sine of x. In this case the function has one argument: x. Arguments are always contained in parentheses ‚Äì curved brackets, () ‚Äì separated by commas.\nArguments can be named or unnamed, but if they are unnamed they must be ordered (we will see later how to find the right order). The names of the arguments are determined by the author of the function and can be found in the help page for the function. When testing code, it is easier and safer to name the arguments. seq is a function for generating a numeric sequence from and to particular numbers. Type ?seq to get the help page for this function.\n\nseq(from = 3, to = 20, by = 4)\n\n[1]  3  7 11 15 19\n\nseq(3, 20, 4)\n\n[1]  3  7 11 15 19\n\n\nArguments can have default values, meaning we do not need to specify values for these in order to run the function.\nrnorm is a function that will generate a series of values from a normal distribution. In order to use the function, we need to tell R how many values we want\n\n## this will produce a random set of numbers, so everyone will get a different set of numbers\nrnorm(n=10)\n\n [1] -0.6810711  0.4260667  1.7392298 -0.9309962  0.2588760  0.6624768\n [7] -0.2175299  0.3225311 -1.0049098  0.1479833\n\n\nThe normal distribution is defined by a mean (average) and standard deviation (spread). However, in the above example we didn‚Äôt tell R what mean and standard deviation we wanted. So how does R know what to do? All arguments to a function and their default values are listed in the help page\n(N.B sometimes help pages can describe more than one function)\n\n?rnorm\n\nIn this case, we see that the defaults for mean and standard deviation are 0 and 1. We can change the function to generate values from a distribution with a different mean and standard deviation using the mean and sd arguments. It is important that we get the spelling of these arguments exactly right, otherwise R will an error message, or (worse?) do something unexpected.\n\nrnorm(n=10, mean=2,sd=3)\n\n [1]  2.857028098  0.943095548 -0.755982527  4.979057859  0.016365279\n [6]  6.538872486  0.005534581  0.869902919  4.415095832  2.029449998\n\nrnorm(10, 2, 3)\n\n [1]  4.9235954  1.4908304  4.3804156  7.4590927  1.3395246  1.4895211\n [7]  2.3824205 -1.7516956  1.7891827  0.6307512\n\n\nIn the examples above, seq and rnorm were both outputting a series of numbers, which is called a vector in R and is the most-fundamental data-type.\nJust as we can save single numbers as a variable, we can also save a vector. In fact a single number is still a vector.\n\nmy_seq &lt;- seq(from = 3, to = 20, by = 4)\n\nThe arithmetic operations we have seen can be applied to these vectors; exactly the same as a single number.\n\nmy_seq + 2\n\n[1]  5  9 13 17 21\n\n\n\nmy_seq * 2\n\n[1]  6 14 22 30 38\n\n\nThese so-called ‚Äúvectorised operations‚Äù are a really nice feature of R and will come in useful when dealing with more complex data.\n\n\n\n\nExercise\n\n\nWhat is the value of pi to 3 decimal places?\n\nsee the help for round ?round\n\nHow can we a create a sequence from 2 to 20 comprised of 5 equally-spaced numbers?\n\ni.e.¬†not specifying the by argument and getting R to work-out the intervals\ncheck the help page for seq ?seq\n\nCreate a variable containing 1000 random numbers with a mean of 2 and a standard deviation of 3\n\nwhat is the maximum and minimum of these numbers?\nwhat is the average?\nHINT: see the help pages for functions min, max and mean\n\n\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\n## The digits argument needs to be changed\nround(pi,digits = 3)\n\n[1] 3.142\n\n## Use the length.out argument\nseq(from = 2, to = 20, length.out = 5)\n\n[1]  2.0  6.5 11.0 15.5 20.0\n\n## Make sure you create a variable\n\nmy_numbers &lt;- rnorm(n = 1000, mean = 2, sd = 3)\n\nmax(my_numbers)\n\n[1] 11.83916\n\nmin(my_numbers)\n\n[1] -6.247082\n\nmean(my_numbers)\n\n[1] 2.231517\n\n\n\n\n\nSo far we have only used functions that come with every version of R. To do something more specialised we will need to install some add-on packages.\n\n\n\n\n\n\nAbout random numbers‚Ä¶\n\n\n\nSometimes we just want to create some numbers or data that we can play around with. However, most likely we will be concerned about the reproducibility of our R code. In circumstances where randomness is involved it is common to set a ‚Äúseed‚Äù which ensures the same random numbers are generated each time.\n\nset.seed(123)\nrnorm(10)\n\n [1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499\n [7]  0.46091621 -1.26506123 -0.68685285 -0.44566197"
  },
  {
    "objectID": "training/r_part1/index.html#saving-your-notebook",
    "href": "training/r_part1/index.html#saving-your-notebook",
    "title": "Introduction to R - Part 1",
    "section": "Saving your notebook",
    "text": "Saving your notebook\nIf you want to re-visit your code at any point, you will need to save a copy.\n\nFile &gt; Save &gt;"
  },
  {
    "objectID": "training/r_part1/index.html#packages-in-r",
    "href": "training/r_part1/index.html#packages-in-r",
    "title": "Introduction to R - Part 1",
    "section": "Packages in R",
    "text": "Packages in R\nSo far we have used functions that are available with the base distribution of R; the functions you get with a clean install of R. The open-source nature of R encourages others to write their own functions for their particular data-type or analyses.\nPackages are distributed through repositories. The most-common ones are CRAN and Bioconductor. CRAN alone has many thousands of packages.\n\n\nThe meta cran website can be used to browse packages available in CRAN\nBioconductor packages can be browsed here\n\n\nCRAN and Bioconductor have some level of curation so should be the first place to look. Researchers sometimes make their packages available on github. However, there is no straightforward way of searching github for a particular package and no guarentee of quality.\nThe Packages tab in the bottom-right panel of RStudio lists all packages that you currently have installed. Clicking on a package name will show a list of functions that available once that package has been loaded.\nThere are functions for installing packages within R. If your package is part of the main CRAN repository, you can use install.packages.\nWe will be using a set of tidyverse R packages in this practical. To install them, we would do.\n\n## You should already have installed these as part of the course setup\n\ninstall.packages(\"readr\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"dplyr\")\n# to install the entire set of tidyverse packages, we can do install.packages(\"tidyverse\"). But this will take some time\n\nA package may have several dependencies; other R packages from which it uses functions or data types (re-using code from other packages is strongly-encouraged). If this is the case, the other R packages will be located and installed too.\n\n\n\n\n\n\nInstalling packages can sometimes take a long time. Fortunately we will only have to do it once **as long as we stick to the same version of R.\nNote that you can install newer versions of RStudio without having to re-install R and any packages.\n\n\n\nOnce a package is installed, the library function is used to load a package and make it‚Äôs functions / data available in your current R session. You need to do this every time you load a new RStudio session. Let‚Äôs go ahead and load the readr so we can import some data.\n\n## readr is a packages to import spreadsheets into R\nlibrary(readr)"
  },
  {
    "objectID": "training/r_part1/index.html#reading-in-data",
    "href": "training/r_part1/index.html#reading-in-data",
    "title": "Introduction to R - Part 1",
    "section": "Reading in data",
    "text": "Reading in data\nAny .csv file can be imported into R by supplying the path to the file to readr function read_csv and assigning it to a new object to store the result. A useful sanity check is the file.exists function which will print TRUE is the file can be found in the working directory.\n\n## This will print the current location of our working directory\ngetwd()\n\n[1] \"C:/work/personal_development/markdunning.github.com/training/r_part1\"\n\ngapminder_path &lt;- \"raw_data/gapminder.csv\"\nfile.exists(gapminder_path)\n\n[1] TRUE\n\n\n\n\n\n\n\n\nThe getwd(), and file.exists(...) functions have been used here as you may find them useful in your own work. If we are confident that we know where a file is located we can use read_csv as below.\n\n\n\nAssuming the file can be found, we can use read_csv to import. Other functions can be used to read tab-delimited files (read_delim) or a generic read.table function. A data frame object is created.\n\nlibrary(readr)\ngapminder_path &lt;- \"raw_data/gapminder.csv\"\ngapminder &lt;- read_csv(gapminder_path)\n\nRows: 1704 Columns: 6\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (2): country, continent\ndbl (4): year, lifeExp, pop, gdpPercap\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\nFile paths\n\n\n\nü§î\nWhy would specifying gapminder_path as Users/mark/Documents/workflows/workshops/r-crash-course/raw_data/gapminder.csv be a bad idea? Would you be able to re-run the analysis on another machine?\n\n\n\n\n\n\n\n\nReading from Excel (xls/xlsx) files\n\n\n\nYou can also read excel (.xls or .xlsx) files into R, but you will have to use the readxl package instead.\n\ninstall.packages(\"readxl\")\nlibrary(readxl)\n## Replace PATH_TO_MY_XLS with the name of the file you want to read\ndata &lt;- read_xls(PATH_TO_MY_XLS)\n## Replace PATH_TO_MY_XLSX with the name of the file you want to read\ndata &lt;- read_xlsx(PATH_TO_MY_XLSX)\n\n\n\n\n\n\n\n\n\nIf you get really stuck importing data, there is a File -&gt; Import Dataset option that should guide you through the process. It will also show the corresponding R code that you can use in future.\n\n\n\nThe data frame object in R allows us to work with ‚Äútabular‚Äù data, like we might be used to dealing with in Excel, where our data can be thought of having rows and columns. The values in each column have to all be of the same type (i.e.¬†all numbers or all text).\nIn Rstudio, you can view the contents of the data frame we have just created using function View(). This is useful for interactive exploration of the data, but not so useful for automation, scripting and analyses.\n\n## Make sure that you use a capital letter V\n\nView(gapminder)\n\n\n\n# A tibble: 1,704 √ó 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ‚Ñπ 1,694 more rows\n\n\nWe should always check the data frame that we have created. Sometimes R will happily read data using an inappropriate function and create an object without raising an error. However, the data might be unusable. Consider:-\n\ntest &lt;- read_table(gapminder_path)\n\n\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\ncols(\n  `\"country\",\"continent\",\"year\",\"lifeExp\",\"pop\",\"gdpPercap\"` = col_character()\n)\n\n\nWarning: 324 parsing failures.\nrow col  expected    actual                     file\n145  -- 1 columns 3 columns 'raw_data/gapminder.csv'\n146  -- 1 columns 3 columns 'raw_data/gapminder.csv'\n147  -- 1 columns 3 columns 'raw_data/gapminder.csv'\n148  -- 1 columns 3 columns 'raw_data/gapminder.csv'\n149  -- 1 columns 3 columns 'raw_data/gapminder.csv'\n... ... ......... ......... ........................\nSee problems(...) for more details.\n\n\n\nView(test)\n\n\n\n# A tibble: 1,704 √ó 1\n   `\"country\",\"continent\",\"year\",\"lifeExp\",\"pop\",\"gdpPercap\"` \n   &lt;chr&gt;                                                      \n 1 \"\\\"Afghanistan\\\",\\\"Asia\\\",1952,28.801,8425333,779.4453145\" \n 2 \"\\\"Afghanistan\\\",\\\"Asia\\\",1957,30.332,9240934,820.8530296\" \n 3 \"\\\"Afghanistan\\\",\\\"Asia\\\",1962,31.997,10267083,853.10071\"  \n 4 \"\\\"Afghanistan\\\",\\\"Asia\\\",1967,34.02,11537966,836.1971382\" \n 5 \"\\\"Afghanistan\\\",\\\"Asia\\\",1972,36.088,13079460,739.9811058\"\n 6 \"\\\"Afghanistan\\\",\\\"Asia\\\",1977,38.438,14880372,786.11336\"  \n 7 \"\\\"Afghanistan\\\",\\\"Asia\\\",1982,39.854,12881816,978.0114388\"\n 8 \"\\\"Afghanistan\\\",\\\"Asia\\\",1987,40.822,13867957,852.3959448\"\n 9 \"\\\"Afghanistan\\\",\\\"Asia\\\",1992,41.674,16317921,649.3413952\"\n10 \"\\\"Afghanistan\\\",\\\"Asia\\\",1997,41.763,22227415,635.341351\" \n# ‚Ñπ 1,694 more rows\n\n\nüò¨\n\n\n\n\n\n\nThe problem here is that we incorrectly told R that our file was ‚Äútab-delimited‚Äù rather than ‚Äúcomma-separated‚Äù. Tab-delimited means that a ‚Äútab‚Äù (four spaces) is used to distinguish the columns in the file. Therefore R cannot tell where the columns are, and the resulting data frame has a single column. R will not automatically use the appropriate read_csv or read_delim etc function, so you need to be careful\n\n\n\nQuick sanity checks can also be performed by inspecting details in the environment tab. A useful check in RStudio is to use the head function, which prints the first 6 rows of the data frame to the screen.\n\nhead(gapminder)\n\n# A tibble: 6 √ó 6\n  country     continent  year lifeExp      pop gdpPercap\n  &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 Afghanistan Asia       1952    28.8  8425333      779.\n2 Afghanistan Asia       1957    30.3  9240934      821.\n3 Afghanistan Asia       1962    32.0 10267083      853.\n4 Afghanistan Asia       1967    34.0 11537966      836.\n5 Afghanistan Asia       1972    36.1 13079460      740.\n6 Afghanistan Asia       1977    38.4 14880372      786.\n\n\n\n\n\n\n\n\nWe have used a nice, clean, dataset as our example for the workshop. Other datasets out in the wild might not be so ameanable for analysis in R. If your data look like this, you might have problems:-\n\nWe recommend the Data Carpentry materials on spreadsheet organisation for an overview of common pitfalls - and how to address them\n\nhttps://datacarpentry.org/spreadsheet-ecology-lesson/\n\nAlthough R has many functions for data cleaning, if you are new to the language the best approach to such data would be to clean them before attempting to read into R."
  },
  {
    "objectID": "training/r_part1/index.html#accessing-data-in-columns",
    "href": "training/r_part1/index.html#accessing-data-in-columns",
    "title": "Introduction to R - Part 1",
    "section": "Accessing data in columns",
    "text": "Accessing data in columns\nIn the next section we will explore in more detail how to control the columns and rows from a data frame that are displayed in RStudio. For now, accessing all the observations from a particular column can be achieved by typing the $ symbol after the name of the data frame followed by the name of a column you are interested in.\nRStudio is able to ‚Äútab-complete‚Äù the column name, so typing the following and pressing the TAB key will bring-up a list of possible columns. The contents of the column that you select are then printed to the screen.\n\ngapminder$c\n\nRather than merely printing to the screen we can also create a variable\n\nyears &lt;- gapminder$year\n\nWe can then use some of the functions we have seen before\n\nmin(years)\n\n[1] 1952\n\nmax(years)\n\n[1] 2007\n\nmedian(years)\n\n[1] 1979.5\n\n\nAlthough we don‚Äôt have to save the values in the column as a variable first\n\nmin(gapminder$year)\n\n[1] 1952"
  },
  {
    "objectID": "training/r_part1/index.html#creating-a-new-r-notebook",
    "href": "training/r_part1/index.html#creating-a-new-r-notebook",
    "title": "Introduction to R - Part 1",
    "section": "Creating a new R notebook",
    "text": "Creating a new R notebook\nYou will probably want to create a new R notebook file to perform your analysis. This can be done by following the menus:-\n\nFile -&gt; New File -&gt; R notebook\n\nA new pane should open that includes some example code. You can delete everything apart from lines 1 to 4\n\nYou can now insert R code chunks using the insert menu.\nBefore generating a report you will need to save the file with the menu File -&gt; Save. You will then be able to create a report using the Preview button. N.B. you may need to install extra software before doing this.\n\n\n\n\nExercise before the next session\n\n\nCreate a new R notebook using the instructions above and create a code chunk to read the gapminder.csv file. Answer the following questions and generate a report\n\nThe function tail is similar to head except it prints the last lines in a file. Use this function to print the last 10 lines in the data frame (you will have to consult the help on tail to see how to change the default arguments.)\nWhat is the largest observed population?\nWhat is the lowest life expectancy"
  },
  {
    "objectID": "training/r_part2/index.html#topics-covered",
    "href": "training/r_part2/index.html#topics-covered",
    "title": "Introduction to R - Part 2",
    "section": "",
    "text": "Choosing which columns to show from the data\nChoosing what rows to keep in the data\nAdding / altering columns\nSorting the rows in our data\nIntroduction to plotting"
  },
  {
    "objectID": "training/r_part2/index.html#manipulating-columns",
    "href": "training/r_part2/index.html#manipulating-columns",
    "title": "Introduction to R - Part 2",
    "section": "Manipulating Columns",
    "text": "Manipulating Columns\nWe are going to use functions from the dplyr package to manipulate the data frame we have just created. It is perfectly possible to work with data frames using the functions provided as part of ‚Äúbase R‚Äù. However, many find it easy to read and write code using dplyr.\nThere are many more functions available in dplyr than we will cover today. An overview of all functions is given in a cheatsheet.\n\n\n\n\n\n\nHelp with dplyr functions\n\n\n\n\ndplyr cheatsheet. The ‚Äúcheatsheet‚Äù is also available through the RStudio Help menu. However, I don‚Äôt think of this as cheating to have such information to hand. There are far too many functions to remember all of them!\n\n\n\nBefore using any of these functions, we need to load the library:-\n\nlibrary(dplyr)\n\n\nselecting columns\nWe can access the columns of a data frame using the select function. This lets us have control over what is printed to the screen. Admitedly the dataset we are using here is rather small (being only six columns), but these useful functions really shine when faced with 10s or 100s of columns\n\nby name\nFirstly, we can select column by name, by adding bare column names (i.e.¬†not requiring quote marks around the name) after the name of the data frame, separated by a , .\n\nselect(gapminder, country, continent)\n\n# A tibble: 862 √ó 2\n   country     continent\n   &lt;chr&gt;       &lt;chr&gt;    \n 1 Afghanistan Asia     \n 2 Afghanistan Asia     \n 3 Afghanistan Asia     \n 4 Afghanistan Asia     \n 5 Afghanistan Asia     \n 6 Afghanistan Asia     \n 7 Afghanistan Asia     \n 8 Afghanistan Asia     \n 9 Afghanistan Asia     \n10 Afghanistan Asia     \n# ‚Ñπ 852 more rows\n\n\nNow lets imagine that we want to see all the columns apart from country. It would quickly become tedious, not to mention and prone to error, if we had to type every column name we wanted to keep by-hand.\nThankfully, we can also omit columns from the ouput by putting a minus (-) in front of the column name. Note that this is not the same as removing the column from the data permanently.\n\nselect(gapminder, -country)\n\n# A tibble: 862 √ó 5\n   continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Asia       1952    28.8  8425333      779.\n 2 Asia       1957    30.3  9240934      821.\n 3 Asia       1962    32.0 10267083      853.\n 4 Asia       1967    34.0 11537966      836.\n 5 Asia       1972    36.1 13079460      740.\n 6 Asia       1977    38.4 14880372      786.\n 7 Asia       1982    39.9 12881816      978.\n 8 Asia       1987    40.8 13867957      852.\n 9 Asia       1992    41.7 16317921      649.\n10 Asia       1997    41.8 22227415      635.\n# ‚Ñπ 852 more rows\n\n\n\n\n\n\n\n\nThe dplyr package has been carefully developed over the years with the needs of the data analyst in mind. Ideally we would rather be spending our time exploring and understanding data than writing reams of code. For this reason, you will often find a helpful function for a common task.\nIf you find yourself having to write lots of code to achieve a data manipulation task, the chances are the a convenient function already exists.\n\n\n\n\n\nrange of columns\nA range of columns can be selected by the : operator.\n\nselect(gapminder, lifeExp:gdpPercap)\n\n# A tibble: 862 √ó 3\n   lifeExp      pop gdpPercap\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1    28.8  8425333      779.\n 2    30.3  9240934      821.\n 3    32.0 10267083      853.\n 4    34.0 11537966      836.\n 5    36.1 13079460      740.\n 6    38.4 14880372      786.\n 7    39.9 12881816      978.\n 8    40.8 13867957      852.\n 9    41.7 16317921      649.\n10    41.8 22227415      635.\n# ‚Ñπ 852 more rows\n\n\n\n\nhelper functions\nThere are a number of helper functions can be employed if we are unsure about the exact name of the column.\n\nselect(gapminder, starts_with(\"co\"))\n\n# A tibble: 862 √ó 2\n   country     continent\n   &lt;chr&gt;       &lt;chr&gt;    \n 1 Afghanistan Asia     \n 2 Afghanistan Asia     \n 3 Afghanistan Asia     \n 4 Afghanistan Asia     \n 5 Afghanistan Asia     \n 6 Afghanistan Asia     \n 7 Afghanistan Asia     \n 8 Afghanistan Asia     \n 9 Afghanistan Asia     \n10 Afghanistan Asia     \n# ‚Ñπ 852 more rows\n\nselect(gapminder, contains(\"life\"))\n\n# A tibble: 862 √ó 1\n   lifeExp\n     &lt;dbl&gt;\n 1    28.8\n 2    30.3\n 3    32.0\n 4    34.0\n 5    36.1\n 6    38.4\n 7    39.9\n 8    40.8\n 9    41.7\n10    41.8\n# ‚Ñπ 852 more rows\n\n# selecting the last and penultimate columns\nselect(gapminder, last_col(1),last_col())\n\n# A tibble: 862 √ó 2\n        pop gdpPercap\n      &lt;dbl&gt;     &lt;dbl&gt;\n 1  8425333      779.\n 2  9240934      821.\n 3 10267083      853.\n 4 11537966      836.\n 5 13079460      740.\n 6 14880372      786.\n 7 12881816      978.\n 8 13867957      852.\n 9 16317921      649.\n10 22227415      635.\n# ‚Ñπ 852 more rows\n\n\nIt is also possible to use the column number in the selection.\n\nselect(gapminder, 4:6)\n\n# A tibble: 862 √ó 3\n   lifeExp      pop gdpPercap\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1    28.8  8425333      779.\n 2    30.3  9240934      821.\n 3    32.0 10267083      853.\n 4    34.0 11537966      836.\n 5    36.1 13079460      740.\n 6    38.4 14880372      786.\n 7    39.9 12881816      978.\n 8    40.8 13867957      852.\n 9    41.7 16317921      649.\n10    41.8 22227415      635.\n# ‚Ñπ 852 more rows\n\n\nThe select function can be used with just a single column name - in a similar manner to the $ operation we saw in Part 1. However, select always returns a data frame whereas $ gives a vector. Compare the output of the following code chunks\n\nselect(gapminder, pop)\n\n# A tibble: 862 √ó 1\n        pop\n      &lt;dbl&gt;\n 1  8425333\n 2  9240934\n 3 10267083\n 4 11537966\n 5 13079460\n 6 14880372\n 7 12881816\n 8 13867957\n 9 16317921\n10 22227415\n# ‚Ñπ 852 more rows\n\n\n\ngapminder$pop\n\nThe consequence of this is that you cannot use functions such as mean in combination with select\n\npops &lt;- select(gapminder, pop)\nmean(pops)\n\nIn the next session we will see how to calculate summary statistics on particular columns in our data. For now, a useful function is pull that will return the correct type of data required for a function such as mean.\n\npops &lt;- pull(gapminder,pop)\nmean(pops)\n\n[1] 40918865"
  },
  {
    "objectID": "training/r_part2/index.html#restricting-rows-with-filter",
    "href": "training/r_part2/index.html#restricting-rows-with-filter",
    "title": "Introduction to R - Part 2",
    "section": "Restricting rows with filter",
    "text": "Restricting rows with filter\nSo far we have been returning all the rows in the output. We can use what we call a logical test to filter the rows in a data frame. This logical test will be applied to each row and give either a TRUE or FALSE result. When filtering, only rows with a TRUE result get returned.\nFor example we filter for rows where the lifeExp variable is less than 40. You can think of R looking at each row of the data frame in turn and deciding whether the lifeExp value in that row is less than 40. If so, that row will be shown on the screen.\n\nfilter(gapminder, lifeExp &lt; 40)\n\n# A tibble: 65 √ó 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Angola      Africa     1952    30.0  4232095     3521.\n 9 Angola      Africa     1957    32.0  4561361     3828.\n10 Angola      Africa     1962    34    4826015     4269.\n# ‚Ñπ 55 more rows\n\n\nTesting for equality can be done using ==. This will only give TRUE for entries that are exactly the same as the test string.\n\nfilter(gapminder, country == \"Zambia\")\n\n# A tibble: 0 √ó 6\n# ‚Ñπ 6 variables: country &lt;chr&gt;, continent &lt;chr&gt;, year &lt;dbl&gt;, lifeExp &lt;dbl&gt;,\n#   pop &lt;dbl&gt;, gdpPercap &lt;dbl&gt;\n\n\nN.B. For partial matches, the grepl function and / or regular expressions (if you know them) can be used.\n\nfilter(gapminder, grepl(\"land\", country))\n\n# A tibble: 36 √ó 6\n   country continent  year lifeExp     pop gdpPercap\n   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 Finland Europe     1952    66.6 4090500     6425.\n 2 Finland Europe     1957    67.5 4324000     7545.\n 3 Finland Europe     1962    68.8 4491443     9372.\n 4 Finland Europe     1967    69.8 4605744    10922.\n 5 Finland Europe     1972    70.9 4639657    14359.\n 6 Finland Europe     1977    72.5 4738902    15605.\n 7 Finland Europe     1982    74.6 4826933    18533.\n 8 Finland Europe     1987    74.8 4931729    21141.\n 9 Finland Europe     1992    75.7 5041039    20647.\n10 Finland Europe     1997    77.1 5134406    23724.\n# ‚Ñπ 26 more rows\n\n\nWe can also test if rows are not equal to a value using !=\n\nfilter(gapminder, continent != \"Europe\")\n\n# A tibble: 670 √ó 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ‚Ñπ 660 more rows\n\n\n\ntesting more than one condition\nThere are a couple of ways of testing for more than one pattern. The first uses an or | statement. i.e.¬†testing if the value of country is Zambia or the value is Zimbabwe. Remember to use double = sign to test for string equality; ==.\n\nfilter(gapminder, country == \"Zambia\" | country == \"Zimbabwe\")\n\n# A tibble: 0 √ó 6\n# ‚Ñπ 6 variables: country &lt;chr&gt;, continent &lt;chr&gt;, year &lt;dbl&gt;, lifeExp &lt;dbl&gt;,\n#   pop &lt;dbl&gt;, gdpPercap &lt;dbl&gt;\n\n\n\n\n\n\n\n\nThe difference between ‚Äúand‚Äù and ‚Äúor‚Äù\n\n\n\nConsider the following code. Is the output as you expect? ü§î\n\nfilter(gapminder, country == \"Zambia\", country == \"Zimbabwe\")\n\n# A tibble: 0 √ó 6\n# ‚Ñπ 6 variables: country &lt;chr&gt;, continent &lt;chr&gt;, year &lt;dbl&gt;, lifeExp &lt;dbl&gt;,\n#   pop &lt;dbl&gt;, gdpPercap &lt;dbl&gt;\n\n\n\n\nThe %in% function is a convenient function for testing which items in a vector correspond to a defined set of values.\n\nfilter(gapminder, country %in% c(\"Zambia\", \"Zimbabwe\"))\n\n# A tibble: 0 √ó 6\n# ‚Ñπ 6 variables: country &lt;chr&gt;, continent &lt;chr&gt;, year &lt;dbl&gt;, lifeExp &lt;dbl&gt;,\n#   pop &lt;dbl&gt;, gdpPercap &lt;dbl&gt;\n\n\nWe can require that two or more tests are TRUE, e.g.¬†which years in Zambia had a life expectancy less than 40, by separating conditional statements by a ,. This performs an AND test so only rows that meet both conditions are returned.\n\nfilter(gapminder, country == \"Zambia\", lifeExp &lt; 40)\n\n# A tibble: 0 √ó 6\n# ‚Ñπ 6 variables: country &lt;chr&gt;, continent &lt;chr&gt;, year &lt;dbl&gt;, lifeExp &lt;dbl&gt;,\n#   pop &lt;dbl&gt;, gdpPercap &lt;dbl&gt;\n\n\n\n\n\n\n\n\nYou may have noticed that filter will always output the same number of columns as the input data frame. filter never changes the columns that are displayed. There are ways of using filter in conjunction with select as we will see later."
  },
  {
    "objectID": "training/r_part2/index.html#exercise",
    "href": "training/r_part2/index.html#exercise",
    "title": "Introduction to R - Part 2",
    "section": "Exercise",
    "text": "Exercise\n\n\nCreate a subset of the data where the population less than a million in the year 2002\nCreate a subset of the data where the life expectancy is greater than 75 in the years prior to 1987\nCreate a subset of the European data where the life expectancy is between 75 and 80 in the years 2002 or 2007.\nIf you are finished with these, try to explore alternative ways of performing the same filtering\n\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\n# Create a subset of the data where the population less than a million in the year 2002\nfilter(gapminder, pop &lt; 1e6, year == 2002)\n\n# A tibble: 5 √ó 6\n  country           continent  year lifeExp    pop gdpPercap\n  &lt;chr&gt;             &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1 Bahrain           Asia       2002    74.8 656397    23404.\n2 Comoros           Africa     2002    63.0 614382     1076.\n3 Djibouti          Africa     2002    53.4 447416     1908.\n4 Equatorial Guinea Africa     2002    49.3 495627     7703.\n5 Iceland           Europe     2002    80.5 288030    31163.\n\n# Create a subset of the data where the life expectancy is greater than 75 in the years prior to 1987\n\nfilter(gapminder, lifeExp &gt; 75, year &lt; 1987)\n\n# A tibble: 7 √ó 6\n  country          continent  year lifeExp       pop gdpPercap\n  &lt;chr&gt;            &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Canada           Americas   1982    75.8  25201900    22899.\n2 Greece           Europe     1982    75.2   9786480    15268.\n3 Hong Kong, China Asia       1982    75.4   5264500    14561.\n4 Iceland          Europe     1977    76.1    221823    19655.\n5 Iceland          Europe     1982    77.0    233997    23270.\n6 Japan            Asia       1977    75.4 113872473    16610.\n7 Japan            Asia       1982    77.1 118454974    19384.\n\n# Create a subset of the European data where the life expectancy is between 75 and 80 in the years 2002 or 2007\n\nfilter(gapminder, continent == \"Europe\", lifeExp &gt; 75, lifeExp &lt; 80 , year == 2002 | year == 2007)\n\n# A tibble: 20 √ó 6\n   country        continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;          &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Albania        Europe     2002    75.7  3508512     4604.\n 2 Albania        Europe     2007    76.4  3600523     5937.\n 3 Austria        Europe     2002    79.0  8148312    32418.\n 4 Austria        Europe     2007    79.8  8199783    36126.\n 5 Belgium        Europe     2002    78.3 10311970    30486.\n 6 Belgium        Europe     2007    79.4 10392226    33693.\n 7 Croatia        Europe     2007    75.7  4493312    14619.\n 8 Czech Republic Europe     2002    75.5 10256295    17596.\n 9 Czech Republic Europe     2007    76.5 10228744    22833.\n10 Denmark        Europe     2002    77.2  5374693    32167.\n11 Denmark        Europe     2007    78.3  5468120    35278.\n12 Finland        Europe     2002    78.4  5193039    28205.\n13 Finland        Europe     2007    79.3  5238460    33207.\n14 France         Europe     2002    79.6 59925035    28926.\n15 Germany        Europe     2002    78.7 82350671    30036.\n16 Germany        Europe     2007    79.4 82400996    32170.\n17 Greece         Europe     2002    78.3 10603863    22514.\n18 Greece         Europe     2007    79.5 10706290    27538.\n19 Ireland        Europe     2002    77.8  3879155    34077.\n20 Ireland        Europe     2007    78.9  4109086    40676.\n\n# A different version using a built-in dplyr function called between\n\nfilter(gapminder, continent == \"Europe\", \n       between(lifeExp, 75,80), \n       year %in% c(2002,2007))\n\n# A tibble: 20 √ó 6\n   country        continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;          &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Albania        Europe     2002    75.7  3508512     4604.\n 2 Albania        Europe     2007    76.4  3600523     5937.\n 3 Austria        Europe     2002    79.0  8148312    32418.\n 4 Austria        Europe     2007    79.8  8199783    36126.\n 5 Belgium        Europe     2002    78.3 10311970    30486.\n 6 Belgium        Europe     2007    79.4 10392226    33693.\n 7 Croatia        Europe     2007    75.7  4493312    14619.\n 8 Czech Republic Europe     2002    75.5 10256295    17596.\n 9 Czech Republic Europe     2007    76.5 10228744    22833.\n10 Denmark        Europe     2002    77.2  5374693    32167.\n11 Denmark        Europe     2007    78.3  5468120    35278.\n12 Finland        Europe     2002    78.4  5193039    28205.\n13 Finland        Europe     2007    79.3  5238460    33207.\n14 France         Europe     2002    79.6 59925035    28926.\n15 Germany        Europe     2002    78.7 82350671    30036.\n16 Germany        Europe     2007    79.4 82400996    32170.\n17 Greece         Europe     2002    78.3 10603863    22514.\n18 Greece         Europe     2007    79.5 10706290    27538.\n19 Ireland        Europe     2002    77.8  3879155    34077.\n20 Ireland        Europe     2007    78.9  4109086    40676."
  },
  {
    "objectID": "training/r_part2/index.html#manipulating-the-values-in-a-column-creating-new-columns",
    "href": "training/r_part2/index.html#manipulating-the-values-in-a-column-creating-new-columns",
    "title": "Introduction to R - Part 2",
    "section": "Manipulating the values in a column / creating new columns",
    "text": "Manipulating the values in a column / creating new columns\nAs well as selecting existing columns in the data frame, new columns can be created and existing ones manipulated using the mutate function. Typically a function or mathematical expression is applied to data in existing columns by row, and the result either stored in a new column or reassigned to an existing one. In other words, the number of values returned by the function must be the same as the number of input values. Multiple mutations can be performed in one line of code.\nHere, we create a new column of population in millions (PopInMillions) and round lifeExp to the nearest integer.\n\nmutate(gapminder, PopInMillions = pop / 1e6,\n       lifeExp = round(lifeExp))\n\n# A tibble: 862 √ó 7\n   country     continent  year lifeExp      pop gdpPercap PopInMillions\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n 1 Afghanistan Asia       1952      29  8425333      779.          8.43\n 2 Afghanistan Asia       1957      30  9240934      821.          9.24\n 3 Afghanistan Asia       1962      32 10267083      853.         10.3 \n 4 Afghanistan Asia       1967      34 11537966      836.         11.5 \n 5 Afghanistan Asia       1972      36 13079460      740.         13.1 \n 6 Afghanistan Asia       1977      38 14880372      786.         14.9 \n 7 Afghanistan Asia       1982      40 12881816      978.         12.9 \n 8 Afghanistan Asia       1987      41 13867957      852.         13.9 \n 9 Afghanistan Asia       1992      42 16317921      649.         16.3 \n10 Afghanistan Asia       1997      42 22227415      635.         22.2 \n# ‚Ñπ 852 more rows\n\n\n\n\n\n\n\n\nSomething to think about\n\n\n\nIn the previous code we created a new column called PopInMillions. Why does the following code now produce an error?\n\nselect(gapminder, PopInMillions)\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nThis code shows what a data frame looks like with a new column called PopInMillions.\n\nmutate(gapminder, PopInMillions = pop / 1e6,\n       lifeExp = round(lifeExp))\n\n# A tibble: 862 √ó 7\n   country     continent  year lifeExp      pop gdpPercap PopInMillions\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n 1 Afghanistan Asia       1952      29  8425333      779.          8.43\n 2 Afghanistan Asia       1957      30  9240934      821.          9.24\n 3 Afghanistan Asia       1962      32 10267083      853.         10.3 \n 4 Afghanistan Asia       1967      34 11537966      836.         11.5 \n 5 Afghanistan Asia       1972      36 13079460      740.         13.1 \n 6 Afghanistan Asia       1977      38 14880372      786.         14.9 \n 7 Afghanistan Asia       1982      40 12881816      978.         12.9 \n 8 Afghanistan Asia       1987      41 13867957      852.         13.9 \n 9 Afghanistan Asia       1992      42 16317921      649.         16.3 \n10 Afghanistan Asia       1997      42 22227415      635.         22.2 \n# ‚Ñπ 852 more rows\n\n\nIt does not alter the gapminder dataset itself. If we wanted to continue to work with PopInMillions, we would either need to create a new variable or overwrite the original gapminder dataset (not recommended)\n\ngapminder2 &lt;- mutate(gapminder, PopInMillions = pop / 1e6,\n       lifeExp = round(lifeExp))\n\nselect(gapminder2, PopInMillions)\n\n# A tibble: 862 √ó 1\n   PopInMillions\n           &lt;dbl&gt;\n 1          8.43\n 2          9.24\n 3         10.3 \n 4         11.5 \n 5         13.1 \n 6         14.9 \n 7         12.9 \n 8         13.9 \n 9         16.3 \n10         22.2 \n# ‚Ñπ 852 more rows\n\n\n\n\n\n\n\nSimilar to mutate, if we want to rename existing columns, and not create any extra columns, we can use the rename function.\n\nrename(gapminder, GDP=gdpPercap)\n\n# A tibble: 862 √ó 6\n   country     continent  year lifeExp      pop   GDP\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333  779.\n 2 Afghanistan Asia       1957    30.3  9240934  821.\n 3 Afghanistan Asia       1962    32.0 10267083  853.\n 4 Afghanistan Asia       1967    34.0 11537966  836.\n 5 Afghanistan Asia       1972    36.1 13079460  740.\n 6 Afghanistan Asia       1977    38.4 14880372  786.\n 7 Afghanistan Asia       1982    39.9 12881816  978.\n 8 Afghanistan Asia       1987    40.8 13867957  852.\n 9 Afghanistan Asia       1992    41.7 16317921  649.\n10 Afghanistan Asia       1997    41.8 22227415  635.\n# ‚Ñπ 852 more rows"
  },
  {
    "objectID": "training/r_part2/index.html#ordering-sorting",
    "href": "training/r_part2/index.html#ordering-sorting",
    "title": "Introduction to R - Part 2",
    "section": "Ordering / sorting",
    "text": "Ordering / sorting\nThe whole data frame can be re-ordered according to the values in one column using the arrange function. So to order the table according to population size:-\n\narrange(gapminder, pop)\n\n# A tibble: 862 √ó 6\n   country  continent  year lifeExp    pop gdpPercap\n   &lt;chr&gt;    &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1 Djibouti Africa     1952    34.8  63149     2670.\n 2 Djibouti Africa     1957    37.3  71851     2865.\n 3 Djibouti Africa     1962    39.7  89898     3021.\n 4 Bahrain  Asia       1952    50.9 120447     9867.\n 5 Djibouti Africa     1967    42.1 127617     3020.\n 6 Bahrain  Asia       1957    53.8 138655    11636.\n 7 Iceland  Europe     1952    72.5 147962     7268.\n 8 Comoros  Africa     1952    40.7 153936     1103.\n 9 Kuwait   Asia       1952    55.6 160000   108382.\n10 Iceland  Europe     1957    73.5 165110     9244.\n# ‚Ñπ 852 more rows\n\n\nThe default is smallest --&gt; largest but we can change this using the desc function\n\narrange(gapminder, desc(pop))\n\n# A tibble: 862 √ó 6\n   country continent  year lifeExp        pop gdpPercap\n   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 China   Asia       2007    73.0 1318683096     4959.\n 2 China   Asia       2002    72.0 1280400000     3119.\n 3 China   Asia       1997    70.4 1230075000     2289.\n 4 China   Asia       1992    68.7 1164970000     1656.\n 5 India   Asia       2007    64.7 1110396331     2452.\n 6 China   Asia       1987    67.3 1084035000     1379.\n 7 India   Asia       2002    62.9 1034172547     1747.\n 8 China   Asia       1982    65.5 1000281000      962.\n 9 India   Asia       1997    61.8  959000000     1459.\n10 China   Asia       1977    64.0  943455000      741.\n# ‚Ñπ 852 more rows\n\n\narrange also works on character vectors, arrange them alpha-numerically.\n\narrange(gapminder, desc(country))\n\n# A tibble: 862 √ó 6\n   country continent  year lifeExp     pop gdpPercap\n   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 Kuwait  Asia       1952    55.6  160000   108382.\n 2 Kuwait  Asia       1957    58.0  212846   113523.\n 3 Kuwait  Asia       1962    60.5  358266    95458.\n 4 Kuwait  Asia       1967    64.6  575003    80895.\n 5 Kuwait  Asia       1972    67.7  841934   109348.\n 6 Kuwait  Asia       1977    69.3 1140357    59265.\n 7 Kuwait  Asia       1982    71.3 1497494    31354.\n 8 Kuwait  Asia       1987    74.2 1891487    28118.\n 9 Kuwait  Asia       1992    75.2 1418095    34933.\n10 Kuwait  Asia       1997    76.2 1765345    40301.\n# ‚Ñπ 852 more rows\n\n\nWe can even order by more than one condition\n\narrange(gapminder, year, pop)\n\n# A tibble: 862 √ó 6\n   country           continent  year lifeExp    pop gdpPercap\n   &lt;chr&gt;             &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1 Djibouti          Africa     1952    34.8  63149     2670.\n 2 Bahrain           Asia       1952    50.9 120447     9867.\n 3 Iceland           Europe     1952    72.5 147962     7268.\n 4 Comoros           Africa     1952    40.7 153936     1103.\n 5 Kuwait            Asia       1952    55.6 160000   108382.\n 6 Equatorial Guinea Africa     1952    34.5 216964      376.\n 7 Gambia            Africa     1952    30   284320      485.\n 8 Gabon             Africa     1952    37.0 420702     4293.\n 9 Botswana          Africa     1952    47.6 442308      851.\n10 Guinea-Bissau     Africa     1952    32.5 580653      300.\n# ‚Ñπ 852 more rows\n\n\n\narrange(gapminder, year, continent, pop)\n\n# A tibble: 862 √ó 6\n   country                  continent  year lifeExp     pop gdpPercap\n   &lt;chr&gt;                    &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 Djibouti                 Africa     1952    34.8   63149     2670.\n 2 Comoros                  Africa     1952    40.7  153936     1103.\n 3 Equatorial Guinea        Africa     1952    34.5  216964      376.\n 4 Gambia                   Africa     1952    30    284320      485.\n 5 Gabon                    Africa     1952    37.0  420702     4293.\n 6 Botswana                 Africa     1952    47.6  442308      851.\n 7 Guinea-Bissau            Africa     1952    32.5  580653      300.\n 8 Congo, Rep.              Africa     1952    42.1  854885     2126.\n 9 Central African Republic Africa     1952    35.5 1291695     1071.\n10 Eritrea                  Africa     1952    35.9 1438760      329.\n# ‚Ñπ 852 more rows"
  },
  {
    "objectID": "training/r_part2/index.html#saving-data-frames",
    "href": "training/r_part2/index.html#saving-data-frames",
    "title": "Introduction to R - Part 2",
    "section": "Saving data frames",
    "text": "Saving data frames\nA final point on data frames is that we can write them to disk once we have done our data processing.\nLet‚Äôs create a folder in which to store such processed, ‚Äúanalysis-ready‚Äù data for sharing\n\ndir.create(\"out_data\",showWarnings = FALSE)\n## showWarnings will stop a message from appearing if the directory already exists\n\n\nbyWealth &lt;- arrange(gapminder, desc(gdpPercap))\n# check the output before writing\nhead(byWealth)\n\n# A tibble: 6 √ó 6\n  country continent  year lifeExp     pop gdpPercap\n  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1 Kuwait  Asia       1957    58.0  212846   113523.\n2 Kuwait  Asia       1972    67.7  841934   109348.\n3 Kuwait  Asia       1952    55.6  160000   108382.\n4 Kuwait  Asia       1962    60.5  358266    95458.\n5 Kuwait  Asia       1967    64.6  575003    80895.\n6 Kuwait  Asia       1977    69.3 1140357    59265.\n\nwrite_csv(byWealth, file = \"out_data/by_wealth.csv\")\n\nWe will now try an exercise that involves using several steps of these operations"
  },
  {
    "objectID": "training/r_part2/index.html#exercise-1",
    "href": "training/r_part2/index.html#exercise-1",
    "title": "Introduction to R - Part 2",
    "section": "Exercise",
    "text": "Exercise\n\n\nFilter the data to include just observations from the year 2002\nRe-arrange the table so that the countries from each continent are ordered according to decreasing wealth. i.e.¬†the wealthiest countries first\nSelect all the columns apart from year\nWrite the data frame out to a file in out_data/ folder\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ngapminder2 &lt;- filter(gapminder, year == 2002)\ngapminder3 &lt;- arrange(gapminder2, continent, country, desc(gdpPercap))\ngapminder4 &lt;- select(gapminder3, -year)\nwrite_csv(gapminder4, \"out_data/gapminder_2002.csv\")"
  },
  {
    "objectID": "training/r_part2/index.html#piping",
    "href": "training/r_part2/index.html#piping",
    "title": "Introduction to R - Part 2",
    "section": "‚ÄúPiping‚Äù",
    "text": "‚ÄúPiping‚Äù\nAs have have just seen, we will often need to perform an analysis, or clean a dataset, using several dplyr functions in sequence. e.g.¬†filtering, mutating, then selecting columns of interest (possibly followed by plotting - see shortly).\nAs a small example; if we wanted to filter our results to just Europe the continent column becomes redundant so we might as well remove it.\nThe following is perfectly valid R code, but invites the user to make mistakes and copy-and-paste errors when writing it. We also have to create multiple copies of the same data frame, which would not be desirable for large datasets.\n\ntmp &lt;- filter(gapminder, continent == \"Europe\")\ntmp2 &lt;- select(tmp, -continent)\ntmp2\n\n# A tibble: 192 √ó 5\n   country  year lifeExp     pop gdpPercap\n   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 Albania  1952    55.2 1282697     1601.\n 2 Albania  1957    59.3 1476505     1942.\n 3 Albania  1962    64.8 1728137     2313.\n 4 Albania  1967    66.2 1984060     2760.\n 5 Albania  1972    67.7 2263554     3313.\n 6 Albania  1977    68.9 2509048     3533.\n 7 Albania  1982    70.4 2780097     3631.\n 8 Albania  1987    72   3075321     3739.\n 9 Albania  1992    71.6 3326498     2497.\n10 Albania  1997    73.0 3428038     3193.\n# ‚Ñπ 182 more rows\n\n\nIn R, dplyr commands to be linked together and form a workflow. The symbol %&gt;% is pronounced then. With a %&gt;% the input to a function is assumed to be the output of the previous line. All the dplyr functions that we have seen so far take a data frame as an input and return an altered data frame as an output, so are amenable to this type of programming.\nThe example we gave of filtering just the European countries and removing the continent column becomes:-\n\nfilter(gapminder, continent==\"Europe\") %&gt;% \n  select(-continent)\n\n# A tibble: 192 √ó 5\n   country  year lifeExp     pop gdpPercap\n   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 Albania  1952    55.2 1282697     1601.\n 2 Albania  1957    59.3 1476505     1942.\n 3 Albania  1962    64.8 1728137     2313.\n 4 Albania  1967    66.2 1984060     2760.\n 5 Albania  1972    67.7 2263554     3313.\n 6 Albania  1977    68.9 2509048     3533.\n 7 Albania  1982    70.4 2780097     3631.\n 8 Albania  1987    72   3075321     3739.\n 9 Albania  1992    71.6 3326498     2497.\n10 Albania  1997    73.0 3428038     3193.\n# ‚Ñπ 182 more rows\n\n\nHopefully you will agree that the code is much cleaner and easier to read and write.\n\n\n\n\nExercise\n\n\nRe-write your solution to the previous exercise, but using the %&gt;% symbol\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nfilter(gapminder, year == 2002) %&gt;% \n  arrange(continent, desc(gdpPercap)) %&gt;% \n  select(-year) %&gt;% \nwrite_csv(\"out_data/gapminder_piped_2002.csv\")\n\n\n\n\nWe will leave dplyr for the moment (although it will never be far away from us, since it is such a fundamental tool‚Ä¶) and start to look at making some nice graphs to understand our data.\n\n\n\n\n\n\nCeci n‚Äôest pas une pipe\n\n\n\nThe %&gt;% operation was introduced as part of the magrittr package, which gets loaded automatically as part of dplyr. However, the dplyr package is quite large and involves a lot of dependencies. If you only wanted to use the %&gt;% and not any other part of dplyr it would be quite inefficient to load the entire dplyr package as part of your code.\nAn equivalent |&gt; operation is available as part of base R. This means you can use piping without having to load the whole of dplyr. This is not an issue for these materials since we are working with dplyr quite a lot, but worth mentioning for completeness as you may see |&gt; used elsewhere. The code is exactly the same.\n\nfilter(gapminder, year == 2002) |&gt;\n  arrange(continent, desc(gdpPercap)) |&gt;\n  select(-year)\n\n# A tibble: 71 √ó 5\n   country           continent lifeExp      pop gdpPercap\n   &lt;chr&gt;             &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Gabon             Africa       56.8  1299304    12522.\n 2 Botswana          Africa       46.6  1630347    11004.\n 3 Equatorial Guinea Africa       49.3   495627     7703.\n 4 Algeria           Africa       71.0 31287142     5288.\n 5 Egypt             Africa       69.8 73312559     4755.\n 6 Congo, Rep.       Africa       53.0  3328795     3484.\n 7 Angola            Africa       41.0 10866106     2773.\n 8 Cameroon          Africa       49.9 15929988     1934.\n 9 Djibouti          Africa       53.4   447416     1908.\n10 Cote d'Ivoire     Africa       46.8 16252726     1649.\n# ‚Ñπ 61 more rows"
  },
  {
    "objectID": "training/r_part2/index.html#why-use-ggplot2",
    "href": "training/r_part2/index.html#why-use-ggplot2",
    "title": "Introduction to R - Part 2",
    "section": "Why use ggplot2?",
    "text": "Why use ggplot2?\nThe structured syntax and high level of abstraction used by ggplot2 should allow for the user to concentrate on the visualisations instead of creating the underlying code.\nOn top of this central philosophy ggplot2 has:\n\nIncreased flexibility over many plotting systems.\nAn advanced theme system for professional/publication level graphics.\nLarge developer base ‚Äì Many libraries extending its flexibility.\nLarge user base ‚Äì Great documentation and active mailing list.\n\n\n\n\n\n\n\nTop tip\n\n\n\nIt is always useful to think about the message you want to convey and the appropriate plot before writing any R code. Resources like data-to-viz.com should help. Don‚Äôt be afraid to even sketch out the plot on paper or a whiteboard!\n\n\nWith some practice, ggplot2 makes it easier to go from the figure you are imagining in our head (or on paper) to a publication-ready image in R.\n\n\n\n\n\n\nAnother ‚Äúcheatsheet‚Äù\n\n\n\nAs with dplyr, we won‚Äôt have time to cover all details of ggplot2. This is however a useful cheatsheet that can be printed as a reference. The cheatsheet is also available through the RStudio Help menu."
  },
  {
    "objectID": "training/r_part2/index.html#basic-plot-types",
    "href": "training/r_part2/index.html#basic-plot-types",
    "title": "Introduction to R - Part 2",
    "section": "Basic plot types",
    "text": "Basic plot types\nA plot in ggplot2 is created with the following type of command. N.B. please dont try and run this code, it just an overall sketch of what our ggplot2 code will look like.\nggplot(data = &lt;DATA&gt;, mapping = aes(&lt;MAPPINGS&gt;)) +  &lt;GEOM_FUNCTION&gt;()\nSo we need to specify\n\nThe data to be used in graph\nMappings of data to the graph (aesthetic mapping)\nWhat type of graph we want to use (The geom to use).\n\nLets say that we want to explore the relationship between GDP and Life Expectancy. We might start with the hypothesis that richer countries have higher life expectancy. A sensible choice of plot would be a scatter plot with gdp on the x-axis and life expectancy on the y-axis.\nThe first stage is to specify our dataset using the data argument. ggplot2 is great, but not clever enough to know what kind of plot we might want. It just creates a blank canvas.\n\nlibrary(ggplot2)\nggplot(data = gapminder)\n\n\n\n\n\n\n\n\nFor the aesthetics, as a bare minimum we will map the gdpPercap and lifeExp to the x- and y-axis of the plot. Some progress is made; we at least get axes\n\nggplot(data = gapminder,aes(x=gdpPercap, y=lifeExp))\n\n\n\n\n\n\n\n\nThat created the axes, but we still need to define how to display our points on the plot. As we have continuous data for both the x- and y-axis, geom_point is a good choice.\n\nggplot(data = gapminder,aes(x=gdpPercap, y=lifeExp)) + geom_point()\n\n\n\n\n\n\n\n\nThe geom we use will depend on what kind of data we have (continuous, categorical etc)\n\ngeom_point() - Scatter plots\ngeom_line() - Line plots\ngeom_smooth() - Fitted line plots\ngeom_bar() - Bar plots\ngeom_boxplot() - Boxplots\ngeom_jitter() - Jitter to plots\ngeom_histogram() - Histogram plots\ngeom_density() - Density plots\ngeom_text() - Text to plots\ngeom_errorbar() - Errorbars to plots\ngeom_violin() - Violin plots\ngeom_tile() - for ‚Äúheatmap‚Äù-like plots\n\nBoxplots are commonly used to visualise the distributions of continuous data. We have to use a categorical variable on the x-axis such as continent or country (not advisable in this case as there are too many different values).\nThe order of the boxes along the x-axis is dictated by the order of categories in the factor; with the default for names being alphabetical order.\n\nggplot(gapminder, aes(x = continent, y=gdpPercap)) + geom_boxplot()\n\n\n\n\n\n\n\n\nA histogram is a common method for visualising a distribution of numeric values. Your data are split into a number of bins (which can be altered in the code) across the whole data range, and the number of observations in each bin is shown on the y-axis. Thus you can see where the majority of your data points are\n\nggplot(gapminder, aes(x = gdpPercap)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\nFor categorical data, e.g.¬†the country of continent columns in our case, a barplot will show the number of times each category is observed. The geom_bar will do the job of counting and plotting.\n\nggplot(gapminder, aes(x=continent)) + geom_bar()\n\n\n\n\n\n\n\n\nIf you have particular numeric values you want to display in a barplot you can use geom_col. To give an example we will first filter the data to a particular year and continent. The gdpPercap values for each country can then be plotted. In the below plot the axis labels will be messy and difficult to read. This is something that can be customised with some of the ggplot2 options we will explore later.\n\ngapminder2002 &lt;- filter(gapminder, year==2002,continent==\"Americas\")\n\n## Notice that we plot the variable we have just created and not gapminder\n## You could also do this in one step using the piping technique for earlier\n\nggplot(gapminder2002, aes(x=country,y=gdpPercap)) + geom_col()\n\n\n\n\n\n\n\n\nWhere appropriate, we can add multiple layers of geoms to the plot. For instance, a criticism of the boxplot is that it does not show all the data. We can rectify this by overlaying the individual points. This can give a representation of how many data points there are.\n\nggplot(gapminder, aes(x = continent, y=gdpPercap)) + geom_boxplot() + geom_point()\n\n\n\n\n\n\n\n\nHowever, the default x-coordinate is always the same for each category. Adding some random ‚Äúnoise‚Äù to the x-axis can help using geom_jitter.\n\nggplot(gapminder, aes(x = continent, y=gdpPercap)) + geom_boxplot() + geom_jitter(width=0.1)\n\n\n\n\n\n\n\n\n\n\n\n\nExercises\n\n\nThe violin plot is a popular alternative to the boxplot. Create a violin plot with geom_violin to visualise the differences in GDP between different continents.\nCreate a subset of the gapminder data frame containing just the rows for your country of birth\nHas there been an increase in life expectancy over time?\n\nvisualise the trend using a scatter plot (geom_point), line graph (geom_line) or smoothed line (geom_smooth).\n\nWhat happens when you modify the geom_boxplot example to compare the gdp distributions for different years?\n\nLook at the message ggplot2 prints above the plot and try to modify the code to give a separate boxplot for each year\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nggplot(gapminder, aes(x = continent, y = gdpPercap)) + geom_violin()\n\n## I choose United Kingdom here, but pick a different one if you like\n\nuk_data &lt;- filter(gapminder, country == \"United Kingdom\")\n\n## As a scatter plot\nggplot(uk_data, aes(x = year, y = lifeExp)) + geom_point()\n\n## As a line plot\nggplot(uk_data, aes(x = year, y = lifeExp)) + geom_line()\n\n## With a smoothed line\nggplot(uk_data, aes(x = year, y = lifeExp)) + geom_smooth()\n\n\n## more than one of the above\n## You can also fit a straight line (via a linear model) by changing method\n\nggplot(uk_data, aes(x = year, y = lifeExp)) + geom_point() + geom_smooth(method = \"lm\")\n\n## this exercise could also make use of the piping technique\n\nfilter(gapminder, country == \"United Kingdom\") %&gt;% \n  ggplot(aes(x = year, y = lifeExp)) + geom_point() + geom_smooth()\n\n# this is how we might expect the code to look like\nggplot(gapminder, aes(x = year, y = gdpPercap)) + geom_boxplot()\n\n# The previous output hints that you might want to group by year - otherwise it thinks that year is a numerical variable\n\n\nggplot(gapminder, aes(x = year, y = gdpPercap, group=year)) + geom_boxplot()\n\n# You may sometimes see this as a possible solution which fixes the year to be a categorical variable\nggplot(gapminder, aes(x = as.factor(year), y = gdpPercap)) + geom_boxplot()\n\n\n\n\n\n\n\nAs we have seen already, ggplot offers an interface to create many popular plot types. It is up to the user to decide what the best way to visualise the data."
  },
  {
    "objectID": "training/r_part2/index.html#customising-the-plot-appearance",
    "href": "training/r_part2/index.html#customising-the-plot-appearance",
    "title": "Introduction to R - Part 2",
    "section": "Customising the plot appearance",
    "text": "Customising the plot appearance\nOur plots are a bit dreary at the moment, but one way to add colour is to add a col argument to the geom_point function. The value can be any of the pre-defined colour names in R. These are displayed in this handy online reference. Red, Green, Blue of Hex values can also be given.\n\nggplot(gapminder, aes(x = gdpPercap, y=lifeExp)) + geom_point(col=\"red\")\n\n\n\n\n\n\n\n\n\n# Use the Hex codes from Farrow and Ball: https://convertingcolors.com/list/farrow-ball.html\n# (cook's blue)\n\nggplot(gapminder, aes(x = gdpPercap, y=lifeExp)) + geom_point(col=\"#6A90B4\")\n\n\n\n\n\n\n\n\nHowever, whilst looking nicer this doesn‚Äôt really tell us anything about the data. For example, what are the points to the far right? Do they belong to a particular country or continent? A powerful feature of ggplot2 is that colours are treated as aesthetics of the plot. In other words we can use a column in our dataset.\nLet‚Äôs say that we want points on our plot to be coloured according to continent. We add an extra argument to the definition of aesthetics to define the mapping. ggplot2 will even decide on colours and create a legend for us. Don‚Äôt worry if you don‚Äôt like the colours chosen, all of this can be customised.\n\nggplot(gapminder, aes(x = gdpPercap, y=lifeExp,col=continent)) + geom_point()\n\n\n\n\n\n\n\n\nIt will even choose a continuous or discrete colour scale based on the data type. We have already seen that ggplot2 is treating our year column as numerical data; which is probably not very useful for visualisation.\n\nggplot(gapminder, aes(x = gdpPercap, y=lifeExp,col=year)) + geom_point()\n\n\n\n\n\n\n\n\nWe can force ggplot2 to treat year as categorical data by using as.factor when creating the aesthetics.\n\nggplot(gapminder, aes(x = gdpPercap, y=lifeExp,col=as.factor(year))) + geom_point()\n\n\n\n\n\n\n\n\nWhen used in the construction of a boxplot, the col argument will change the colour of the lines. To change the colour of the boxes we have to use fill.\n\nggplot(gapminder, aes(x = continent, y=gdpPercap,fill=continent)) + geom_boxplot()"
  },
  {
    "objectID": "training/r_part2/index.html#help-with-dplyr-functions",
    "href": "training/r_part2/index.html#help-with-dplyr-functions",
    "title": "Introduction to R - Part 2",
    "section": "Help with dplyr functions",
    "text": "Help with dplyr functions\n\ndplyr cheatsheet. The ‚Äúcheatsheet‚Äù is also available through the RStudio Help menu. I personally tend to think of it as cheating to have such information to hand. There are far too many functions to remember all of them!\n\n:::\nBefore using any of these functions, we need to load the library:-\n\nlibrary(dplyr)\n\n\nselecting columns\nWe can access the columns of a data frame using the select function. This lets us have control over what is printed to the screen. Admitedly the dataset we are using here is rather small (being only six columns), but these useful functions really shine when faced with 10s or 100s of columns\n\nby name\nFirstly, we can select column by name, by adding bare column names (i.e.¬†not requiring quote marks around the name) after the name of the data frame, separated by a , .\n\nselect(gapminder, country, continent)\n\n# A tibble: 1,704 √ó 2\n   country     continent\n   &lt;chr&gt;       &lt;chr&gt;    \n 1 Afghanistan Asia     \n 2 Afghanistan Asia     \n 3 Afghanistan Asia     \n 4 Afghanistan Asia     \n 5 Afghanistan Asia     \n 6 Afghanistan Asia     \n 7 Afghanistan Asia     \n 8 Afghanistan Asia     \n 9 Afghanistan Asia     \n10 Afghanistan Asia     \n# ‚Ñπ 1,694 more rows\n\n\nNow lets imagine that we want to see all the columns apart from country. It would quickly become tedious, not to mention and prone to error, if we had to type every column name we wanted to keep by-hand.\nThankfully, we can also omit columns from the ouput by putting a minus (-) in front of the column name. Note that this is not the same as removing the column from the data permanently.\n\nselect(gapminder, -country)\n\n# A tibble: 1,704 √ó 5\n   continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Asia       1952    28.8  8425333      779.\n 2 Asia       1957    30.3  9240934      821.\n 3 Asia       1962    32.0 10267083      853.\n 4 Asia       1967    34.0 11537966      836.\n 5 Asia       1972    36.1 13079460      740.\n 6 Asia       1977    38.4 14880372      786.\n 7 Asia       1982    39.9 12881816      978.\n 8 Asia       1987    40.8 13867957      852.\n 9 Asia       1992    41.7 16317921      649.\n10 Asia       1997    41.8 22227415      635.\n# ‚Ñπ 1,694 more rows\n\n\n\n\n\n\n\n\nThe dplyr package has been carefully developed over the years with the needs of the data analyst in mind. Ideally we would rather be spending our time exploring and understanding data than writing reams of code. For this reason, you will often find a helpful function for a common task.\nIf you find yourself having to write lots of code to achieve a data manipulation task, the chances are the a convenient function already exists.\n\n\n\n\n\nrange of columns\nA range of columns can be selected by the : operator.\n\nselect(gapminder, lifeExp:gdpPercap)\n\n# A tibble: 1,704 √ó 3\n   lifeExp      pop gdpPercap\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1    28.8  8425333      779.\n 2    30.3  9240934      821.\n 3    32.0 10267083      853.\n 4    34.0 11537966      836.\n 5    36.1 13079460      740.\n 6    38.4 14880372      786.\n 7    39.9 12881816      978.\n 8    40.8 13867957      852.\n 9    41.7 16317921      649.\n10    41.8 22227415      635.\n# ‚Ñπ 1,694 more rows\n\n\n\n\nhelper functions\nThere are a number of helper functions can be employed if we are unsure about the exact name of the column.\n\nselect(gapminder, starts_with(\"co\"))\n\n# A tibble: 1,704 √ó 2\n   country     continent\n   &lt;chr&gt;       &lt;chr&gt;    \n 1 Afghanistan Asia     \n 2 Afghanistan Asia     \n 3 Afghanistan Asia     \n 4 Afghanistan Asia     \n 5 Afghanistan Asia     \n 6 Afghanistan Asia     \n 7 Afghanistan Asia     \n 8 Afghanistan Asia     \n 9 Afghanistan Asia     \n10 Afghanistan Asia     \n# ‚Ñπ 1,694 more rows\n\nselect(gapminder, contains(\"life\"))\n\n# A tibble: 1,704 √ó 1\n   lifeExp\n     &lt;dbl&gt;\n 1    28.8\n 2    30.3\n 3    32.0\n 4    34.0\n 5    36.1\n 6    38.4\n 7    39.9\n 8    40.8\n 9    41.7\n10    41.8\n# ‚Ñπ 1,694 more rows\n\n# selecting the last and penultimate columns\nselect(gapminder, last_col(1),last_col())\n\n# A tibble: 1,704 √ó 2\n        pop gdpPercap\n      &lt;dbl&gt;     &lt;dbl&gt;\n 1  8425333      779.\n 2  9240934      821.\n 3 10267083      853.\n 4 11537966      836.\n 5 13079460      740.\n 6 14880372      786.\n 7 12881816      978.\n 8 13867957      852.\n 9 16317921      649.\n10 22227415      635.\n# ‚Ñπ 1,694 more rows\n\n\nIt is also possible to use the column number in the selection.\n\nselect(gapminder, 4:6)\n\n# A tibble: 1,704 √ó 3\n   lifeExp      pop gdpPercap\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1    28.8  8425333      779.\n 2    30.3  9240934      821.\n 3    32.0 10267083      853.\n 4    34.0 11537966      836.\n 5    36.1 13079460      740.\n 6    38.4 14880372      786.\n 7    39.9 12881816      978.\n 8    40.8 13867957      852.\n 9    41.7 16317921      649.\n10    41.8 22227415      635.\n# ‚Ñπ 1,694 more rows\n\n\nThe select function can be used with just a single column name - in a similar manner to the $ operation we saw in Part 1. However, select always returns a data frame whereas $ gives a vector. Compare the output of the following code chunks\n\nselect(gapminder, pop)\n\n# A tibble: 1,704 √ó 1\n        pop\n      &lt;dbl&gt;\n 1  8425333\n 2  9240934\n 3 10267083\n 4 11537966\n 5 13079460\n 6 14880372\n 7 12881816\n 8 13867957\n 9 16317921\n10 22227415\n# ‚Ñπ 1,694 more rows\n\n\n\ngapminder$pop\n\nThe consequence of this is that you cannot use functions such as mean in combination with select\n\npops &lt;- select(gapminder, pop)\nmean(pops)\n\nIn the next session we will see how to calculate summary statistics on particular columns in our data. For now, a useful function is pull that will return the correct type of data required for a function such as mean.\n\npops &lt;- pull(gapminder,pop)\nmean(pops)\n\n[1] 29601212"
  },
  {
    "objectID": "training/r_part1/index.html#solutions",
    "href": "training/r_part1/index.html#solutions",
    "title": "Introduction to R - Part 1",
    "section": "Solutions",
    "text": "Solutions\n\n## The digits argument needs to be changed\nround(pi,digits = 3)\n\n[1] 3.142\n\n## Use the length.out argument\nseq(from = 2, to = 20, length.out = 5)\n\n[1]  2.0  6.5 11.0 15.5 20.0\n\n## Make sure you create a variable\n\nmy_numbers &lt;- rnorm(n = 1000, mean = 2, sd = 3)\n\nmax(my_numbers)\n\n[1] 11.39027\n\nmin(my_numbers)\n\n[1] -7.072349\n\nmean(my_numbers)\n\n[1] 2.080459\n\n\n:::\n\n\n\n\n\n\nAbout random numbers‚Ä¶\n\n\n\nSometimes we just want to create some numbers or data that we can play around with. However, most likely we will be concerned about the reproducibility of our R code. In circumstances where randomness is involved it is common to set a ‚Äúseed‚Äù which ensures the same random numbers are generated each time.\n\nset.seed(123)\nrnorm(10)\n\n [1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499\n [7]  0.46091621 -1.26506123 -0.68685285 -0.44566197"
  },
  {
    "objectID": "training/r_part1/index.html#coming-next",
    "href": "training/r_part1/index.html#coming-next",
    "title": "Introduction to R - Part 1",
    "section": "Coming next‚Ä¶",
    "text": "Coming next‚Ä¶\nIn Part 2 we will start to interrogate and visualise the data we have just imported\n\nChoosing which columns to show from the data\nChoosing what rows to keep in the data\nAdding / altering columns\nSorting the rows in our data\nIntroduction to plotting"
  },
  {
    "objectID": "training/r_part3/index.html#topics-covered",
    "href": "training/r_part3/index.html#topics-covered",
    "title": "Introduction to R - Part 3",
    "section": "",
    "text": "Customising ggplot2 plots\nSummarising data\nGroup-based summaries\nJoining data\nData Cleaning\n\nLets make sure we have read the gapminder data into R and have the relevant packages loaded.\n\n## Checks if the required file is present, and downloads if not\n\nif(!file.exists(\"raw_data/gapminder.csv\")) {\n  dir.create(\"raw_data/\",showWarnings = FALSE)\ndownload.file(\"https://raw.githubusercontent.com/markdunning/markdunning.github.com/refs/heads/master/files/training/r/raw_data/gapminder.csv\", destfile = \"raw_data/gapminder.csv\")\n}\n\nWe also discussed in the previous part(s) how to read the example dataset into R. We will also load the libraries needed.\n\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\ngapminder &lt;- read_csv(\"raw_data/gapminder.csv\")"
  },
  {
    "objectID": "training/r_part3/index.html#customising-a-plot",
    "href": "training/r_part3/index.html#customising-a-plot",
    "title": "Introduction to R - Part 3",
    "section": "Customising a plot",
    "text": "Customising a plot\nNow make a scatter plot of gdp versus life expectancy as we did in the previous session. One of the last topics we covered was how to add colour to a plot. This can make the plot more appealing, but also help with data interpretation. In this case, we can use different colours to indicate countries belonging to different continents. For example, we can see a cluster of Asia data points with unusually large GDP. At some point we might want to adjust the scale on the x-axis to make the trend between the two axes easier to visualise.\n\nggplot(gapminder, aes(x = gdpPercap, y=lifeExp,col=continent)) + geom_point()\n\n\n\n\n\n\n\n\nThe shape and size of points can also be mapped from the data. However, it is easy to get carried away!\n\nggplot(gapminder, aes(x = gdpPercap, y=lifeExp,shape=continent,size=pop)) + geom_point()\n\n\n\n\n\n\n\n\nScales and their legends have so far been handled using ggplot2 defaults. ggplot2 offers functionality to have finer control over scales and legends using the scale methods.\nScale methods are divided into functions by combinations of\n\nthe aesthetics they control.\nthe type of data mapped to scale.\n\nscale_aesthetic_type\nTry typing in scale_ then tab to autocomplete. This will provide some examples of the scale functions available in ggplot2.\nAlthough different scale functions accept some variety in their arguments, common arguments to scale functions include -\n\nname - The axis or legend title\nlimits - Minimum and maximum of the scale\nbreaks - Label/tick positions along an axis\nlabels - Label names at each break\nvalues - the set of aesthetic values to map data values\n\nWe can choose specific colour palettes, such as those provided by the RColorBrewer package. This package is included with R (so you don‚Äôt need to install it) and provides palettes for different types of scale (sequential, diverging, qualitative).\n\nlibrary(RColorBrewer)\ndisplay.brewer.all(colorblindFriendly = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen creating a plot, always check that the colour scheme is appropriate for people with various forms of colour-blindness\n\n\nWhen experimenting with colour palettes and labels, it is useful to save the plot as an object. This saves quite a bit of typing! Notice how nothing get shown on the screen.\n\np &lt;- ggplot(gapminder, aes(x = gdpPercap, y=lifeExp,col=continent)) + geom_point()\n\nRunning the line of code with just p now shows the plot on the screen\n\np \n\n\n\n\n\n\n\n\nBut we can also make modifications to the plot with the + symbol. Here, we change the colours to those defined as Set2 in RColorBrewer.\n\n## Here we pick 6 colours from the palette\np + scale_color_manual(values=brewer.pal(6,\"Set2\"))\n\n\n\n\n\n\n\n\nVarious labels can be modified using the labs function.\n\np + labs(x=\"Wealth\",y=\"Life Expectancy\",title=\"Relationship between Wealth and Life Expectancy\")\n\n\n\n\n\n\n\n\nWe can also modify the x- and y- limits of the plot so that any outliers are not shown. ggplot2 will give a warning that some points are excluded.\n\np + xlim(0,60000)\n\nWarning: Removed 5 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nSaving is supported by the ggsave function and automatically saves the last plot that was displayed in RStudio. A variety of file formats are supported (.png, .pdf, .tiff, etc) and the format used is determined from the extension given in the file argument. The height, width and resolution can also be configured. See the help on ggsave (?ggsave) for more information.\n\nggsave(file=\"my_ggplot.png\")\n\nSaving 7 x 5 in image\n\n\nWarning: Removed 5 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nMost aspects of the plot can be modified from the background colour to the grid sizes and font. Several pre-defined ‚Äúthemes‚Äù exist and we can modify the appearance of the whole plot using a theme_.. function.\n\np + theme_bw()\n\n\n\n\n\n\n\n\nMore themes are supported by the ggthemes package. You can make your plots look like the Economist, Wall Street Journal or Excel (but please don‚Äôt do this!)\n\n## this will check if ggthemes is already installed, and will only install if it is not found\n\nif(!require(\"ggthemes\")) install.packages(\"ggthemes\")\n\nLoading required package: ggthemes\n\nlibrary(ggthemes)\np + theme_excel()"
  },
  {
    "objectID": "training/r_part3/index.html#exercise",
    "href": "training/r_part3/index.html#exercise",
    "title": "Introduction to R - Part 3",
    "section": "Exercise",
    "text": "Exercise\n\nUse a boxplot to compare the life expectancy values of Australia and New Zealand. Use a Set2 palette from RColorBrewer to colour the boxplots and apply a ‚Äúminimal‚Äù theme to the plot.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ngapminder %&gt;% \n  filter(continent == \"Oceania\") %&gt;% \n  ggplot(aes(x = country, y = lifeExp,fill=country)) + geom_boxplot() + scale_fill_manual(values=brewer.pal(2,\"Set2\")) + theme_bw()\n\n\n\n\nAnother transformation that is useful in this case is to display the x-axis on a log\\(_10\\) scale. This compresses the values on the x-axis (reducing the impact of the high outliers) and makes trends easier to spot\n\np + scale_x_log10()\n\n\n\n\n\n\n\n\nIt now seems that lifeExp is increasing in a roughly linear fashion with the GDP (on a log\\(_10\\) scale).\n\n\n\n\n\n\nAbout the log transformation\n\n\n\n\n\nThe logarithm of 10 (log10) is the exponent to which the base 10 must be ‚Äúraised‚Äù to obtain the number 10. For example, log10(10) = 1, as 10 raised to the power of 1 equals 10.\n\nlog10(10)\n\n[1] 1\n\n10^1\n\n[1] 10\n\nlog10(100)\n\n[1] 2\n\n10^2\n\n[1] 100\n\n\nThis transformation helps in simplifying visualisation involving large numbers. The range of our gdpPercap values is extremely large. summary is a quick way to get various summary statistics from our data\n\n## we will use the $ notation for now\n\nsummary(gapminder$gdpPercap)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n   241.2   1202.1   3531.8   7215.3   9325.5 113523.1 \n\n\nAfter a log10 transformation the data are much more compressed.\n\nsummary(log10(gapminder$gdpPercap))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.382   3.080   3.548   3.543   3.970   5.055 \n\n\nThe largest value after the log\\(_10\\) transformation is around 5\n\n10^5.055\n\n[1] 113501.1"
  },
  {
    "objectID": "training/r_part3/index.html#facets",
    "href": "training/r_part3/index.html#facets",
    "title": "Introduction to R - Part 3",
    "section": "Facets",
    "text": "Facets\nOne very useful feature of ggplot2 is faceting. This allows you to produce plots for subsets and groupings in your data (aka ‚Äúfacets‚Äù). In the scatter plot above, it was quite difficult to determine if the relationship between gdp and life expectancy was the same for each continent. To overcome this, we would like a see a separate plot for each continent.\nIn we attempted such a task manually we might start off by plotting Africa\n\nafr_plot&lt;- gapminder %&gt;% \n  filter(continent == \"Africa\") %&gt;% \n  ggplot(aes(x = gdpPercap, y = lifeExp)) + geom_point() + scale_x_log10()\nafr_plot\n\n\n\n\n\n\n\n\nAnd then the same for Americas:-\n\namr_plot &lt;- gapminder %&gt;% \n  filter(continent == \"Americas\") %&gt;% \n  ggplot(aes(x = gdpPercap, y = lifeExp)) + geom_point() + scale_x_log10()\namr_plot\n\n\n\n\n\n\n\n\nAt some point we will have to stitch the plots together (which is possible, but we will cover this later) and make sure we have equivalent scales for all plots. In this setup we are manually specifying the name of the continent, which is prone to error. Again, we could use something like a for loop to make the plots for each continent. However, we aren‚Äôt covering such techniques in these materials as dplyr and ggplot2 don‚Äôt tend to require them.\nAs we said before, dplyr, and ggplot2 are built with the analyst in mind and have many useful features for automating some common tasks. To achieve the plot we want is surprisingly simple. To ‚Äúfacet‚Äù our data into multiple plots we can use the facet_wrap (1 variable) or facet_grid (2 variables) functions and specify the variable(s) we split by.\n\np + facet_wrap(~continent) + scale_x_log10() + xlab(\"GDP (log10)\") + ylab(\"Life Expectancy\")\n\n\n\n\n\n\n\n\nThe facet_grid function will create a grid-like plot with one variable on the x-axis and another on the y-axis.\n\np + facet_grid(continent~year)\n\n\n\n\n\n\n\n\nThe previous plot was a bit messy as it contained all combinations of year and continent. Let‚Äôs suppose we want our analysis to be a bit more focused and disregard countries in Oceania (as there are only 2 in our dataset) and maybe years between 1997 and 2002. However, we can only ‚Äúadd‚Äù more information from our plots and not take anything away. Therefore the suggested approach is to pre-filter and manipulate the data into the form you want for plotting.\nWeknow how to restrict the rows from the gapminder dataset using the filter function. Instead of filtering the data, creating a new data frame, and constructing the data frame from these new data we can use the%&gt;% operator to create the data frame ‚Äúon the fly‚Äù and pass directly to ggplot. Thus we don‚Äôt have to save a new data frame or alter the original data.\n\nfilter(gapminder, continent!=\"Oceania\", year %in% c(1997,2002,2007)) %&gt;% \n  ggplot(aes(x = gdpPercap, y=lifeExp,col=continent)) + geom_point() + facet_grid(continent~year)\n\n\n\n\n\n\n\n\nThere is lots more to cover on ggplot2 and quickly we can start to understand our data without too much in the way of coding. When it comes to reporting and justifying our findings we will need to produce some numerical summaries. We tackle this in the next section."
  },
  {
    "objectID": "training/r_part3/index.html#introducing-the-covid-19-data",
    "href": "training/r_part3/index.html#introducing-the-covid-19-data",
    "title": "Introduction to R - Part 3",
    "section": "Introducing the COVID-19 data",
    "text": "Introducing the COVID-19 data\nData for global COVID-19 cases are available online from CSSE at Johns Hopkins University on their github repository.\n\ngithub is an excellent way of making your code and analysis available for others to reuse and share. Private repositories with restricted access are also available. Here is a useful beginners guide.\n-Friendly github intro\n\nR is capable of downloading files to our own machine so we can analyse them. We need to know the URL (for the COVID data we can find this from github, or use the address below) and can specify what to call the file when it is downloaded.\n\ndownload.file(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\",destfile = \"raw_data/time_series_covid19_confirmed_global.csv\")\n\nWe can use the read_csv function as before to import the data and take a look. We can see the basic structure of the data is one row for each country / region and columns for cases on each day.\n\ncovid &lt;- read_csv(\"raw_data/time_series_covid19_confirmed_global.csv\")\n\nRows: 289 Columns: 1147\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr    (2): Province/State, Country/Region\ndbl (1145): Lat, Long, 1/22/20, 1/23/20, 1/24/20, 1/25/20, 1/26/20, 1/27/20,...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncovid\n\n# A tibble: 289 √ó 1,147\n   `Province/State`  `Country/Region`   Lat   Long `1/22/20` `1/23/20` `1/24/20`\n   &lt;chr&gt;             &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 &lt;NA&gt;              Afghanistan       33.9  67.7          0         0         0\n 2 &lt;NA&gt;              Albania           41.2  20.2          0         0         0\n 3 &lt;NA&gt;              Algeria           28.0   1.66         0         0         0\n 4 &lt;NA&gt;              Andorra           42.5   1.52         0         0         0\n 5 &lt;NA&gt;              Angola           -11.2  17.9          0         0         0\n 6 &lt;NA&gt;              Antarctica       -71.9  23.3          0         0         0\n 7 &lt;NA&gt;              Antigua and Bar‚Ä¶  17.1 -61.8          0         0         0\n 8 &lt;NA&gt;              Argentina        -38.4 -63.6          0         0         0\n 9 &lt;NA&gt;              Armenia           40.1  45.0          0         0         0\n10 Australian Capit‚Ä¶ Australia        -35.5 149.           0         0         0\n# ‚Ñπ 279 more rows\n# ‚Ñπ 1,140 more variables: `1/25/20` &lt;dbl&gt;, `1/26/20` &lt;dbl&gt;, `1/27/20` &lt;dbl&gt;,\n#   `1/28/20` &lt;dbl&gt;, `1/29/20` &lt;dbl&gt;, `1/30/20` &lt;dbl&gt;, `1/31/20` &lt;dbl&gt;,\n#   `2/1/20` &lt;dbl&gt;, `2/2/20` &lt;dbl&gt;, `2/3/20` &lt;dbl&gt;, `2/4/20` &lt;dbl&gt;,\n#   `2/5/20` &lt;dbl&gt;, `2/6/20` &lt;dbl&gt;, `2/7/20` &lt;dbl&gt;, `2/8/20` &lt;dbl&gt;,\n#   `2/9/20` &lt;dbl&gt;, `2/10/20` &lt;dbl&gt;, `2/11/20` &lt;dbl&gt;, `2/12/20` &lt;dbl&gt;,\n#   `2/13/20` &lt;dbl&gt;, `2/14/20` &lt;dbl&gt;, `2/15/20` &lt;dbl&gt;, `2/16/20` &lt;dbl&gt;, ‚Ä¶\n\n\nWe can potentially join these data to gapminder, and it would be beneficial to have one column name in common between both files. We can rename the Country/Region column of our new data frame to match gapminder.\n\ncovid &lt;- read_csv(\"raw_data/time_series_covid19_confirmed_global.csv\") %&gt;% \n  rename(country = `Country/Region`) \n\nRows: 289 Columns: 1147\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr    (2): Province/State, Country/Region\ndbl (1145): Lat, Long, 1/22/20, 1/23/20, 1/24/20, 1/25/20, 1/26/20, 1/27/20,...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncovid\n\n# A tibble: 289 √ó 1,147\n   `Province/State` country   Lat   Long `1/22/20` `1/23/20` `1/24/20` `1/25/20`\n   &lt;chr&gt;            &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 &lt;NA&gt;             Afghan‚Ä¶  33.9  67.7          0         0         0         0\n 2 &lt;NA&gt;             Albania  41.2  20.2          0         0         0         0\n 3 &lt;NA&gt;             Algeria  28.0   1.66         0         0         0         0\n 4 &lt;NA&gt;             Andorra  42.5   1.52         0         0         0         0\n 5 &lt;NA&gt;             Angola  -11.2  17.9          0         0         0         0\n 6 &lt;NA&gt;             Antarc‚Ä¶ -71.9  23.3          0         0         0         0\n 7 &lt;NA&gt;             Antigu‚Ä¶  17.1 -61.8          0         0         0         0\n 8 &lt;NA&gt;             Argent‚Ä¶ -38.4 -63.6          0         0         0         0\n 9 &lt;NA&gt;             Armenia  40.1  45.0          0         0         0         0\n10 Australian Capi‚Ä¶ Austra‚Ä¶ -35.5 149.           0         0         0         0\n# ‚Ñπ 279 more rows\n# ‚Ñπ 1,139 more variables: `1/26/20` &lt;dbl&gt;, `1/27/20` &lt;dbl&gt;, `1/28/20` &lt;dbl&gt;,\n#   `1/29/20` &lt;dbl&gt;, `1/30/20` &lt;dbl&gt;, `1/31/20` &lt;dbl&gt;, `2/1/20` &lt;dbl&gt;,\n#   `2/2/20` &lt;dbl&gt;, `2/3/20` &lt;dbl&gt;, `2/4/20` &lt;dbl&gt;, `2/5/20` &lt;dbl&gt;,\n#   `2/6/20` &lt;dbl&gt;, `2/7/20` &lt;dbl&gt;, `2/8/20` &lt;dbl&gt;, `2/9/20` &lt;dbl&gt;,\n#   `2/10/20` &lt;dbl&gt;, `2/11/20` &lt;dbl&gt;, `2/12/20` &lt;dbl&gt;, `2/13/20` &lt;dbl&gt;,\n#   `2/14/20` &lt;dbl&gt;, `2/15/20` &lt;dbl&gt;, `2/16/20` &lt;dbl&gt;, `2/17/20` &lt;dbl&gt;, ‚Ä¶\n\n\nMuch of the analysis of this dataset has looked at trends over time (e.g.¬†increasing /decreasing case numbers, comparing trajectories). As we know by now, the ggplot2 package allows us to map columns (variables) in our dataset to aspects of the plot.\nIn other words, we would expect to create plots by writing code such as:-\nggplot(covid, aes(x = Date, y =...)) + ...\nUnfortunately such plots are not possible with the data in it‚Äôs current format. Counts for each date are containing in a different column. What we require is a column to indicate the date, and the corresponding count in the next column. Such data arrangements are known as long data; whereas we have wide data. Fortunately we can convert between the two using the tidyr package (also part of tidyverse).\n\n## install tidyr if you don't already have it\ninstall.packages(\"tidyr\")\n\n\nFor more information on tidy data, and how to convert between long and wide data, see\nhttps://r4ds.had.co.nz/tidy-data.html\n\n\nlibrary(tidyr)\ncovid &lt;- read_csv(\"raw_data/time_series_covid19_confirmed_global.csv\") %&gt;% \n  rename(country = `Country/Region`) %&gt;% \n  pivot_longer(5:last_col(),names_to=\"Date\", values_to=\"Cases\")\n\nRows: 289 Columns: 1147\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr    (2): Province/State, Country/Region\ndbl (1145): Lat, Long, 1/22/20, 1/23/20, 1/24/20, 1/25/20, 1/26/20, 1/27/20,...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncovid\n\n# A tibble: 330,327 √ó 6\n   `Province/State` country       Lat  Long Date    Cases\n   &lt;chr&gt;            &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1 &lt;NA&gt;             Afghanistan  33.9  67.7 1/22/20     0\n 2 &lt;NA&gt;             Afghanistan  33.9  67.7 1/23/20     0\n 3 &lt;NA&gt;             Afghanistan  33.9  67.7 1/24/20     0\n 4 &lt;NA&gt;             Afghanistan  33.9  67.7 1/25/20     0\n 5 &lt;NA&gt;             Afghanistan  33.9  67.7 1/26/20     0\n 6 &lt;NA&gt;             Afghanistan  33.9  67.7 1/27/20     0\n 7 &lt;NA&gt;             Afghanistan  33.9  67.7 1/28/20     0\n 8 &lt;NA&gt;             Afghanistan  33.9  67.7 1/29/20     0\n 9 &lt;NA&gt;             Afghanistan  33.9  67.7 1/30/20     0\n10 &lt;NA&gt;             Afghanistan  33.9  67.7 1/31/20     0\n# ‚Ñπ 330,317 more rows\n\n\nAnother point to note is that the dates are not in an internationally recognised format, which could cause a problem for some visualisations that rely on date order. We can fix by explicitly converting to YYYY-MM-DD format.\n\nFor more ways of dealing with dates in R see the lubridate package.\n\n\ncovid &lt;- read_csv(\"raw_data/time_series_covid19_confirmed_global.csv\") %&gt;% \n  rename(country = `Country/Region`) %&gt;% \n  pivot_longer(5:last_col(),names_to=\"Date\", values_to=\"Cases\") %&gt;% \n    mutate(Date=as.Date(Date,\"%m/%d/%y\"))\n\nRows: 289 Columns: 1147\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr    (2): Province/State, Country/Region\ndbl (1145): Lat, Long, 1/22/20, 1/23/20, 1/24/20, 1/25/20, 1/26/20, 1/27/20,...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncovid\n\n# A tibble: 330,327 √ó 6\n   `Province/State` country       Lat  Long Date       Cases\n   &lt;chr&gt;            &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;\n 1 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-22     0\n 2 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-23     0\n 3 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-24     0\n 4 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-25     0\n 5 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-26     0\n 6 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-27     0\n 7 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-28     0\n 8 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-29     0\n 9 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-30     0\n10 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-31     0\n# ‚Ñπ 330,317 more rows\n\n\nAnother useful modification is to make sure only one row exists for each country. If we look at the data for some countries (e.g.¬†China and UK) there are different entries for provinces and oversees territories.\n\n## the count function tabulates the number of observations in a particular column\n\nfilter(covid, country == \"China\") %&gt;% \n  count(`Province/State`)\n\n# A tibble: 34 √ó 2\n   `Province/State`     n\n   &lt;chr&gt;            &lt;int&gt;\n 1 Anhui             1143\n 2 Beijing           1143\n 3 Chongqing         1143\n 4 Fujian            1143\n 5 Gansu             1143\n 6 Guangdong         1143\n 7 Guangxi           1143\n 8 Guizhou           1143\n 9 Hainan            1143\n10 Hebei             1143\n# ‚Ñπ 24 more rows\n\n\nSo we can change the Cases to be the sum of all cases for that country on a particular day. We can do this using the group_by and summarise functions from above\n\ncovid &lt;- read_csv(\"raw_data/time_series_covid19_confirmed_global.csv\") %&gt;% \n  rename(country = `Country/Region`) %&gt;% \n  pivot_longer(5:last_col(),names_to=\"Date\", values_to=\"Cases\") %&gt;% \n  mutate(Date=as.Date(Date,\"%m/%d/%y\")) %&gt;% \n  group_by(country,Date) %&gt;% \n  summarise(Cases = sum(Cases))\n\nRows: 289 Columns: 1147\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr    (2): Province/State, Country/Region\ndbl (1145): Lat, Long, 1/22/20, 1/23/20, 1/24/20, 1/25/20, 1/26/20, 1/27/20,...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n`summarise()` has grouped output by 'country'. You can override using the `.groups` argument.\n\ncovid\n\n# A tibble: 229,743 √ó 3\n# Groups:   country [201]\n   country     Date       Cases\n   &lt;chr&gt;       &lt;date&gt;     &lt;dbl&gt;\n 1 Afghanistan 2020-01-22     0\n 2 Afghanistan 2020-01-23     0\n 3 Afghanistan 2020-01-24     0\n 4 Afghanistan 2020-01-25     0\n 5 Afghanistan 2020-01-26     0\n 6 Afghanistan 2020-01-27     0\n 7 Afghanistan 2020-01-28     0\n 8 Afghanistan 2020-01-29     0\n 9 Afghanistan 2020-01-30     0\n10 Afghanistan 2020-01-31     0\n# ‚Ñπ 229,733 more rows\n\n\n\nExercise\n\nWhat plots and summaries can you make from these data?\n\nPlotting the number of cases over time for certain countries\nWhich country in each continent currently has the highest number of cases?\nNormalise the number of cases for population size (using 2007 population figures as a population estimate)?\n\ne.g.¬†cases per 100,000\n\nWhich European countries have the highest number of cases per 100,000 population\n\ne.g.¬†https://www.statista.com/statistics/1110187/coronavirus-incidence-europe-by-country/"
  },
  {
    "objectID": "training/r_part2/index.html#bonus-exercise",
    "href": "training/r_part2/index.html#bonus-exercise",
    "title": "Introduction to R - Part 2",
    "section": "Bonus Exercise",
    "text": "Bonus Exercise\nThese are a bit more challenging, but please feel free to have a go\n\nUsing the filter function, find all countries that start with the letter Z\n\nHint: You can find the first letter of each country using the substr function. The mutate function can then be used to add a new column to the data.\n\nUse geom_tile to create a heatmap visualising life expectancy over time for European countries. You will need to work out what aesthetics to specify for a geom_tile plot\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nPart 1\n\n## month.name is a built-in vector of the months of the year\nmonth.name\n\n\n## substr can be used to extract substrings from a character vector betwen a start and end position\n## e.g. print the first three letters of each moth\n\n\nsubstr(month.name, 1, 3)\n\n# Using mutate, add an extra column; the first letter of each country name. \n\ngapminder2 &lt;- mutate(gapminder, FirstLetter = substr(country, 1,1))\n\n# Now filter using the new FirstLetter column\n\ngapminder3 &lt;- filter(gapminder2, FirstLetter == \"Z\")\ngapminder3\n\n\n## Get the European countries\nfilter(gapminder, continent == \"Europe\") %&gt;% \n## make heatmap. See the fill aesthetic to be life expectancy\nggplot(aes(x=year,y=country,fill=lifeExp)) + geom_tile()"
  },
  {
    "objectID": "training/r_part2/index.html#wrap-up",
    "href": "training/r_part2/index.html#wrap-up",
    "title": "Introduction to R - Part 2",
    "section": "Wrap-up",
    "text": "Wrap-up\nWe have covered a lot about manipulating and visualising data, but have only just scratched the surface. In the next part we will conclude with\n\nChoosing colour palettes\n(Some ways to) customise our plots\nAutomatically plot different subsets / categories in our data using ‚Äúfaceting‚Äù\nProducing summary statistics from our data, and for different subsets / categories\nJoining two data frames\n(Briefly) how to clean ‚Äúmessy‚Äù data"
  },
  {
    "objectID": "training/r_part3/index.html#data-cleaning-a-covid-19-data-example",
    "href": "training/r_part3/index.html#data-cleaning-a-covid-19-data-example",
    "title": "Introduction to R - Part 3",
    "section": "Data Cleaning: A COVID-19 data example",
    "text": "Data Cleaning: A COVID-19 data example\nData for global COVID-19 cases are available online from CSSE at Johns Hopkins University on their github repository.\n\n\n\n\n\n\nNote\n\n\n\ngithub is an excellent way of making your code and analysis available for others to reuse and share. Private repositories with restricted access are also available. Here is a useful beginners guide.\n-Friendly github intro\n\n\nR is capable of downloading files to our own machine so we can analyse them. We need to know the URL (for the COVID data we can find this from github, or use the address below) and can specify what to call the file when it is downloaded.\n\nif(!file.exists(\"raw_data/time_series_covid19_confirmed_global.csv\")){\n  download.file(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\",destfile = \"raw_data/time_series_covid19_confirmed_global.csv\")\n}\n\nWe can use the read_csv function as before to import the data and take a look. We can see the basic structure of the data is one row for each country / region and columns for cases on each day.\n\ncovid &lt;- read_csv(\"raw_data/time_series_covid19_confirmed_global.csv\")\n\nRows: 289 Columns: 1147\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr    (2): Province/State, Country/Region\ndbl (1145): Lat, Long, 1/22/20, 1/23/20, 1/24/20, 1/25/20, 1/26/20, 1/27/20,...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncovid\n\n# A tibble: 289 √ó 1,147\n   `Province/State`  `Country/Region`   Lat   Long `1/22/20` `1/23/20` `1/24/20`\n   &lt;chr&gt;             &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 &lt;NA&gt;              Afghanistan       33.9  67.7          0         0         0\n 2 &lt;NA&gt;              Albania           41.2  20.2          0         0         0\n 3 &lt;NA&gt;              Algeria           28.0   1.66         0         0         0\n 4 &lt;NA&gt;              Andorra           42.5   1.52         0         0         0\n 5 &lt;NA&gt;              Angola           -11.2  17.9          0         0         0\n 6 &lt;NA&gt;              Antarctica       -71.9  23.3          0         0         0\n 7 &lt;NA&gt;              Antigua and Bar‚Ä¶  17.1 -61.8          0         0         0\n 8 &lt;NA&gt;              Argentina        -38.4 -63.6          0         0         0\n 9 &lt;NA&gt;              Armenia           40.1  45.0          0         0         0\n10 Australian Capit‚Ä¶ Australia        -35.5 149.           0         0         0\n# ‚Ñπ 279 more rows\n# ‚Ñπ 1,140 more variables: `1/25/20` &lt;dbl&gt;, `1/26/20` &lt;dbl&gt;, `1/27/20` &lt;dbl&gt;,\n#   `1/28/20` &lt;dbl&gt;, `1/29/20` &lt;dbl&gt;, `1/30/20` &lt;dbl&gt;, `1/31/20` &lt;dbl&gt;,\n#   `2/1/20` &lt;dbl&gt;, `2/2/20` &lt;dbl&gt;, `2/3/20` &lt;dbl&gt;, `2/4/20` &lt;dbl&gt;,\n#   `2/5/20` &lt;dbl&gt;, `2/6/20` &lt;dbl&gt;, `2/7/20` &lt;dbl&gt;, `2/8/20` &lt;dbl&gt;,\n#   `2/9/20` &lt;dbl&gt;, `2/10/20` &lt;dbl&gt;, `2/11/20` &lt;dbl&gt;, `2/12/20` &lt;dbl&gt;,\n#   `2/13/20` &lt;dbl&gt;, `2/14/20` &lt;dbl&gt;, `2/15/20` &lt;dbl&gt;, `2/16/20` &lt;dbl&gt;, ‚Ä¶\n\n\nMuch of the analysis of this dataset has looked at trends over time (e.g.¬†increasing /decreasing case numbers, comparing trajectories). As we know by now, the ggplot2 package allows us to map columns (variables) in our dataset to aspects of the plot.\nIn other words, we would expect to create plots by writing code such as:-\nggplot(covid, aes(x = Date, y =...)) + ...\nUnfortunately such plots are not possible with the data in it‚Äôs current format. Counts for each date are containing in a different column. What we require is a column to indicate the date, and the corresponding count in the next column. Such data arrangements are known as long data; whereas we have wide data. Fortunately we can convert between the two using the tidyr package (also part of tidyverse).\n\n## install tidyr if you don't already have it\ninstall.packages(\"tidyr\")\n\n\n\n\n\n\n\nAbout ‚Äútidy data‚Äù\n\n\n\nFor more information on tidy data, and how to convert between long and wide data, see\nhttps://r4ds.had.co.nz/tidy-data.html\n\n\nFor convenience we will also rename the column containing country names\n\n## set the show_col_types argument to FALSE to suppress message about column types\n\nlibrary(tidyr)\ncovid &lt;- read_csv(\"raw_data/time_series_covid19_confirmed_global.csv\",show_col_types = FALSE) %&gt;% \n    rename(country = `Country/Region`) %&gt;% \n  pivot_longer(5:last_col(),names_to=\"Date\", values_to=\"Cases\")\ncovid\n\n# A tibble: 330,327 √ó 6\n   `Province/State` country       Lat  Long Date    Cases\n   &lt;chr&gt;            &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1 &lt;NA&gt;             Afghanistan  33.9  67.7 1/22/20     0\n 2 &lt;NA&gt;             Afghanistan  33.9  67.7 1/23/20     0\n 3 &lt;NA&gt;             Afghanistan  33.9  67.7 1/24/20     0\n 4 &lt;NA&gt;             Afghanistan  33.9  67.7 1/25/20     0\n 5 &lt;NA&gt;             Afghanistan  33.9  67.7 1/26/20     0\n 6 &lt;NA&gt;             Afghanistan  33.9  67.7 1/27/20     0\n 7 &lt;NA&gt;             Afghanistan  33.9  67.7 1/28/20     0\n 8 &lt;NA&gt;             Afghanistan  33.9  67.7 1/29/20     0\n 9 &lt;NA&gt;             Afghanistan  33.9  67.7 1/30/20     0\n10 &lt;NA&gt;             Afghanistan  33.9  67.7 1/31/20     0\n# ‚Ñπ 330,317 more rows\n\n\nThe number of rows and columns has changed dramatically, but this is a much more usable form for dplyr and ggplot2.\nAnother point to note is that the dates are not in an internationally recognised format, which could cause a problem for some visualisations that rely on date order.\n\nWe can fix by explicitly converting to YYYY-MM-DD format. The as.Date function can be used to convert an existing column into standardised dates. It needs to know how the months, days and years are being specified which might look a bit obtuse. The specification needed for these data is %m/%d/%y. This means months(%m) separated by a / followed by a day (%d) followed by another / followed by the year represented by two digits (%y). Other conversions are possible including if you have dates with month names (Jan, Feb‚Ä¶) or four digit years. See the link below for more information.\n\n\n\n\n\n\nDealing with dates\n\n\n\nSee this website for more about representing and converting dates in R.\n\nhttps://www.statology.org/r-date-format/\n\nFor more ways of dealing with dates in R see the lubridate package which can handle tasks such as calculating intervals between dates and much more\n\n\n\ncovid &lt;- read_csv(\"raw_data/time_series_covid19_confirmed_global.csv\",show_col_types = FALSE) %&gt;% \n    rename(country = `Country/Region`) %&gt;% \n  pivot_longer(5:last_col(),names_to=\"Date\", values_to=\"Cases\") %&gt;% \n  mutate(Date=as.Date(Date,\"%m/%d/%y\"))\ncovid\n\n# A tibble: 330,327 √ó 6\n   `Province/State` country       Lat  Long Date       Cases\n   &lt;chr&gt;            &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;\n 1 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-22     0\n 2 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-23     0\n 3 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-24     0\n 4 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-25     0\n 5 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-26     0\n 6 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-27     0\n 7 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-28     0\n 8 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-29     0\n 9 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-30     0\n10 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-31     0\n# ‚Ñπ 330,317 more rows\n\n\nAnother useful modification is to make sure only one row exists for each country. If we look at the data for some countries (e.g.¬†China and UK) there are different entries for provinces and oversees territories. So we can change the Cases to be the sum of all cases for that country on a particular day. We can do this using the group_by and summarise functions from above.\n\ncovid &lt;- read_csv(\"raw_data/time_series_covid19_confirmed_global.csv\", show_col_types = FALSE) %&gt;% \n  rename(country = `Country/Region`) %&gt;% \n  pivot_longer(5:last_col(),names_to=\"Date\", values_to=\"Cases\") %&gt;% \n  mutate(Date=as.Date(Date,\"%m/%d/%y\")) %&gt;% \n  group_by(country,Date) %&gt;% \n  summarise(Cases = sum(Cases))\n\n`summarise()` has grouped output by 'country'. You can override using the\n`.groups` argument.\n\ncovid\n\n# A tibble: 229,743 √ó 3\n# Groups:   country [201]\n   country     Date       Cases\n   &lt;chr&gt;       &lt;date&gt;     &lt;dbl&gt;\n 1 Afghanistan 2020-01-22     0\n 2 Afghanistan 2020-01-23     0\n 3 Afghanistan 2020-01-24     0\n 4 Afghanistan 2020-01-25     0\n 5 Afghanistan 2020-01-26     0\n 6 Afghanistan 2020-01-27     0\n 7 Afghanistan 2020-01-28     0\n 8 Afghanistan 2020-01-29     0\n 9 Afghanistan 2020-01-30     0\n10 Afghanistan 2020-01-31     0\n# ‚Ñπ 229,733 more rows\n\n\n\ncovid\n\n# A tibble: 229,743 √ó 3\n# Groups:   country [201]\n   country     Date       Cases\n   &lt;chr&gt;       &lt;date&gt;     &lt;dbl&gt;\n 1 Afghanistan 2020-01-22     0\n 2 Afghanistan 2020-01-23     0\n 3 Afghanistan 2020-01-24     0\n 4 Afghanistan 2020-01-25     0\n 5 Afghanistan 2020-01-26     0\n 6 Afghanistan 2020-01-27     0\n 7 Afghanistan 2020-01-28     0\n 8 Afghanistan 2020-01-29     0\n 9 Afghanistan 2020-01-30     0\n10 Afghanistan 2020-01-31     0\n# ‚Ñπ 229,733 more rows\n\n\nSince we previously renamed the country column in covid and both data frames now have a column called country we can use a left_join.\n\nleft_join(gapminder, covid)\n\nJoining with `by = join_by(country)`\n\n\nWarning in left_join(gapminder, covid): Detected an unexpected many-to-many relationship between `x` and `y`.\n‚Ñπ Row 1 of `x` matches multiple rows in `y`.\n‚Ñπ Row 1 of `y` matches multiple rows in `x`.\n‚Ñπ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n# A tibble: 1,755,816 √ó 8\n   country     continent  year lifeExp     pop gdpPercap Date       Cases\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8 8425333      779. 2020-01-22     0\n 2 Afghanistan Asia       1952    28.8 8425333      779. 2020-01-23     0\n 3 Afghanistan Asia       1952    28.8 8425333      779. 2020-01-24     0\n 4 Afghanistan Asia       1952    28.8 8425333      779. 2020-01-25     0\n 5 Afghanistan Asia       1952    28.8 8425333      779. 2020-01-26     0\n 6 Afghanistan Asia       1952    28.8 8425333      779. 2020-01-27     0\n 7 Afghanistan Asia       1952    28.8 8425333      779. 2020-01-28     0\n 8 Afghanistan Asia       1952    28.8 8425333      779. 2020-01-29     0\n 9 Afghanistan Asia       1952    28.8 8425333      779. 2020-01-30     0\n10 Afghanistan Asia       1952    28.8 8425333      779. 2020-01-31     0\n# ‚Ñπ 1,755,806 more rows\n\n\n\nIf we hadn‚Äôt used rename previously, the joining code would look like this\n\nleft_join(gapminder, covid, by = c(\"county\" = \"Country/Region\"))\n\n\nFurthermore, we might also want to just use the 2007 rows from gapminder.\n\nfilter(gapminder, year == 2007) %&gt;% \n  left_join(covid)\n\nJoining with `by = join_by(country)`\n\n\n# A tibble: 146,318 √ó 8\n   country     continent  year lifeExp      pop gdpPercap Date       Cases\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       2007    43.8 31889923      975. 2020-01-22     0\n 2 Afghanistan Asia       2007    43.8 31889923      975. 2020-01-23     0\n 3 Afghanistan Asia       2007    43.8 31889923      975. 2020-01-24     0\n 4 Afghanistan Asia       2007    43.8 31889923      975. 2020-01-25     0\n 5 Afghanistan Asia       2007    43.8 31889923      975. 2020-01-26     0\n 6 Afghanistan Asia       2007    43.8 31889923      975. 2020-01-27     0\n 7 Afghanistan Asia       2007    43.8 31889923      975. 2020-01-28     0\n 8 Afghanistan Asia       2007    43.8 31889923      975. 2020-01-29     0\n 9 Afghanistan Asia       2007    43.8 31889923      975. 2020-01-30     0\n10 Afghanistan Asia       2007    43.8 31889923      975. 2020-01-31     0\n# ‚Ñπ 146,308 more rows\n\n\n\nExercise\n\nWhat plots and summaries can you make from these data?\n\nPlotting the number of cases over time for certain countries\nWhich country in each continent currently has the highest number of cases?\nNormalise the number of cases for population size (using 2007 population figures as a population estimate)?\n\ne.g.¬†cases per 100,000\n\nWhich European countries have the highest number of cases per 100,000 population\n\ne.g.¬†https://www.statista.com/statistics/1110187/coronavirus-incidence-europe-by-country/\n\n\n\n\n\n\n\n\n\nSome solutions\n\n\n\n\n\nCompare trajectories of different countries. The %in% operator is an alternative to or (|) to find a country name that can have a number of possibilities.\n\ncovid &lt;- read_csv(\"raw_data/time_series_covid19_confirmed_global.csv\") %&gt;% \n  rename(country = `Country/Region`) %&gt;% \n  pivot_longer(5:last_col(),names_to=\"Date\", values_to=\"Cases\") %&gt;% \n  mutate(Date=as.Date(Date,\"%m/%d/%y\")) %&gt;% \n  group_by(country,Date) %&gt;% \n  summarise(Cases = sum(Cases))\n\nRows: 289 Columns: 1147\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr    (2): Province/State, Country/Region\ndbl (1145): Lat, Long, 1/22/20, 1/23/20, 1/24/20, 1/25/20, 1/26/20, 1/27/20,...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n`summarise()` has grouped output by 'country'. You can override using the `.groups` argument.\n\nfilter(covid, country %in% c(\"United Kingdom\",\"France\",\"Spain\")) %&gt;%\n  ggplot(aes(x = Date, y = Cases,col=country)) + geom_line()\n\n\n\n\n\n\n\n\nTo explore the european data on a particular date, we first filter gapminder appropriately\n\nfilter(gapminder, year == 2007,continent==\"Europe\") %&gt;%\n  left_join(covid) %&gt;% \n  filter(Date == \"2023-01-13\") %&gt;% \n  mutate(Cases = round(Cases / (pop / 1e5)))\n\nJoining with `by = join_by(country)`\n\n\n# A tibble: 28 √ó 8\n   country             continent  year lifeExp    pop gdpPercap Date       Cases\n   &lt;chr&gt;               &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;\n 1 Albania             Europe     2007    76.4 3.60e6     5937. 2023-01-13  9277\n 2 Austria             Europe     2007    79.8 8.20e6    36126. 2023-01-13 69987\n 3 Belgium             Europe     2007    79.4 1.04e7    33693. 2023-01-13 45093\n 4 Bosnia and Herzego‚Ä¶ Europe     2007    74.9 4.55e6     7446. 2023-01-13  8813\n 5 Bulgaria            Europe     2007    73.0 7.32e6    10681. 2023-01-13 17672\n 6 Croatia             Europe     2007    75.7 4.49e6    14619. 2023-01-13 28181\n 7 Denmark             Europe     2007    78.3 5.47e6    35278. 2023-01-13 62970\n 8 Finland             Europe     2007    79.3 5.24e6    33207. 2023-01-13 27654\n 9 France              Europe     2007    80.7 6.11e7    30470. 2023-01-13 64906\n10 Germany             Europe     2007    79.4 8.24e7    32170. 2023-01-13 45637\n# ‚Ñπ 18 more rows\n\n\nTry to make example similar to https://www.statista.com/statistics/1110187/coronavirus-incidence-europe-by-country/. We‚Äôre not using up-to-date figures for population so there will be some differences. The two data frames can be joined because they have a column name in common (country). Most of the country names that appear in gapminder also appear in the covid dataset, so not much data is lost in the join.\nSince we are interested in European countries for this dataset, we pre-filter the gapminder data. Also, we only need the population values from 2007.\n\nfilter(gapminder, year == 2007,continent==\"Europe\") %&gt;% \n  left_join(covid) %&gt;% \n  filter(Date == \"2022-01-06\") %&gt;% \n  mutate(Cases = round(Cases / (pop / 1e5))) %&gt;% \n  ggplot(aes(x = Cases, y = country)) + geom_col()\n\nJoining with `by = join_by(country)`\n\n\n\n\n\n\n\n\n\nThe default ordering for the bars is alphabetical, which is probably not a natural choice. The forcats package, which is part of tidyverse allows factors in a data frame to be re-ordered and re-labeled.\n\nforcats package\n\nIn particular, we can use the fct_reorder function to reorder the country names according to the number of cases. This can be done inside the ggplot2 function itself.\n\n## this will install the package if it is not already installed\nif(!require(forcats)) install.packages(forcats)\n\nLoading required package: forcats\n\nfilter(gapminder, year == 2007,continent==\"Europe\") %&gt;% \n  left_join(covid) %&gt;% \n  filter(Date == \"2023-01-13\") %&gt;% \n  mutate(Cases = round(Cases / (pop / 1e5))) %&gt;% \n  ggplot(aes(x = Cases, y = forcats::fct_reorder(country,Cases))) + geom_col()\n\nJoining with `by = join_by(country)`\n\n\n\n\n\n\n\n\n\nA heatmap of number of cases over time (similar to that reported by the BBC) can be achieved using a geom_tile\n\n### Get the 2007 gapminder data to avoid repeating data\nfilter(gapminder, year ==  2007, continent==\"Europe\") %&gt;% \n  left_join(covid) %&gt;% \n  filter(!is.na(Cases)) %&gt;% ## remove countries with missing values%&gt;% \n  mutate(Cases = round(Cases / (pop / 1e5))) %&gt;% \n  ggplot(aes(x = Date, y = country,fill=Cases)) + geom_tile() + scale_fill_viridis_c()\n\nJoining with `by = join_by(country)`"
  },
  {
    "objectID": "posts/2025_10_16_check_data/index.html",
    "href": "posts/2025_10_16_check_data/index.html",
    "title": "Always check your input data",
    "section": "",
    "text": "Why you always check your input data before starting"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "You will find some comprehensive training materials on this site, and here I will add some shorter musings on other aspects of data analysis and Bioinformatics that I have found useful\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidymodels for omics data: Part 3a\n\n14 min\n\nMachine Learning with glmnet and random forests in R using many features (genes)\n\n\n\nNov 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidymodels for omics data: Part 3b\n\n10 min\n\nMachine Learning with KNN and SVM from RNA-seq\n\n\n\nNov 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidymodels for omics data: Part 2\n\n23 min\n\nA proof of concept machine learning tasks to classify Estrogen Receptor status of TCGA breast cancers using logistic regression and decision trees\n\n\n\nNov 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidymodels for omics data: Part 1\n\n19 min\n\nDownloading and exploring TCGA (breast cancer) expression data from GEO so we can proceed to use machine learning later. We‚Äôll also make some exploratory plots such as‚Ä¶\n\n\n\nNov 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeaching an old dog new tricks; tidy analysis of RNA-seq\n\n13 min\n\nCan I retrain myself to use tidy methods for bulk rna-seq analysis?\n\n\n\nNov 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nReading 10X Spatial Data from GEO into R\n\n7 min\n\n\n\n\n\nOct 23, 2025\n\n\n\n\n\n\n\n\n\n\n\nAlways check your input data\n\n6 min\n\n\n\n\n\nOct 16, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025_10_16_check_data/index.html#overview-of-the-code",
    "href": "posts/2025_10_16_check_data/index.html#overview-of-the-code",
    "title": "Always check your input data",
    "section": "Overview of the code",
    "text": "Overview of the code\n\n\nWarning in dir.create(\"raw_data\"): 'raw_data' already exists\n\n\nThe code chunk below takes a time series dataset of covid cases worldwide and applies some essential data cleaning transformations. These ensure that the data are in a ‚Äútidy‚Äù format expected by ggplot2 and convert the dates into an international standards. Furthermore, some countries are represented by multiple regions and for simplicity we add these case numbers together.\n\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(tidyr)\n\ncovid &lt;- read_csv(\"raw_data/time_series_covid19_confirmed_global.csv\") %&gt;% \n  rename(country = `Country/Region`) %&gt;% \n  pivot_longer(5:last_col(),names_to=\"Date\", values_to=\"Cases\") %&gt;% \n  mutate(Date=as.Date(Date,\"%m/%d/%y\")) %&gt;% \n  group_by(country,Date) %&gt;% \n  summarise(Cases = sum(Cases))\n\nRows: 289 Columns: 1147\n\n\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr    (2): Province/State, Country/Region\ndbl (1145): Lat, Long, 1/22/20, 1/23/20, 1/24/20, 1/25/20, 1/26/20, 1/27/20,...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n`summarise()` has grouped output by 'country'. You can override using the `.groups` argument.\n\nhead(covid)\n\n# A tibble: 6 √ó 3\n# Groups:   country [1]\n  country     Date       Cases\n  &lt;chr&gt;       &lt;date&gt;     &lt;dbl&gt;\n1 Afghanistan 2020-01-22     0\n2 Afghanistan 2020-01-23     0\n3 Afghanistan 2020-01-24     0\n4 Afghanistan 2020-01-25     0\n5 Afghanistan 2020-01-26     0\n6 Afghanistan 2020-01-27     0\n\n\nThe plot is now a standard application of the ggplot function\n\nfilter(covid, country %in% c(\"United Kingdom\",\"France\",\"Spain\")) %&gt;%\n  ggplot(aes(x = Date, y = Cases,col=country)) + geom_line()\n\n\n\n\n\n\n\n\nBut the plot is now looking as expected - with United Kingdom showing high numbers of cases. So what could have happened to produce the top of the page? I neglected to explain that the source data come from a github page are were downloaded as part of my code. In the training materials this was intended to show a workflow that started from data located at a remote source. The code below first creates a raw_data folder (without complaining if such a folder already exists - showWarnings=FALSE) and then checks via file.exists if time_series_covid19_confirmed_global.csv is already present. If not, the code will download from github.\n\ndir.create(\"raw_data\", showWarnings = FALSE)\nif(!file.exists(\"raw_data/time_series_covid19_confirmed_global.csv\")){\n  download.file(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\",destfile = \"raw_data/time_series_covid19_confirmed_global.csv\")\n}\n\nWhat should happen, and missing from the workshop materials, is to perform some basic checks on the dimensions of the data once imported into R and stored as a tibble. ü§¶\n\ncovid &lt;- read_csv(\"raw_data/time_series_covid19_confirmed_global.csv\")\n\nRows: 289 Columns: 1147\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr    (2): Province/State, Country/Region\ndbl (1145): Lat, Long, 1/22/20, 1/23/20, 1/24/20, 1/25/20, 1/26/20, 1/27/20,...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nChecking the dimensions will print the number of rows and columns\n\ndim(covid)\n\n[1]  289 1147\n\n\nThe head function is a classic function for printing the first six rows (adjustable using the n argument)\n\nhead(covid)\n\n# A tibble: 6 √ó 1,147\n  `Province/State` `Country/Region`   Lat  Long `1/22/20` `1/23/20` `1/24/20`\n  &lt;chr&gt;            &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 &lt;NA&gt;             Afghanistan       33.9 67.7          0         0         0\n2 &lt;NA&gt;             Albania           41.2 20.2          0         0         0\n3 &lt;NA&gt;             Algeria           28.0  1.66         0         0         0\n4 &lt;NA&gt;             Andorra           42.5  1.52         0         0         0\n5 &lt;NA&gt;             Angola           -11.2 17.9          0         0         0\n6 &lt;NA&gt;             Antarctica       -71.9 23.3          0         0         0\n# ‚Ñπ 1,140 more variables: `1/25/20` &lt;dbl&gt;, `1/26/20` &lt;dbl&gt;, `1/27/20` &lt;dbl&gt;,\n#   `1/28/20` &lt;dbl&gt;, `1/29/20` &lt;dbl&gt;, `1/30/20` &lt;dbl&gt;, `1/31/20` &lt;dbl&gt;,\n#   `2/1/20` &lt;dbl&gt;, `2/2/20` &lt;dbl&gt;, `2/3/20` &lt;dbl&gt;, `2/4/20` &lt;dbl&gt;,\n#   `2/5/20` &lt;dbl&gt;, `2/6/20` &lt;dbl&gt;, `2/7/20` &lt;dbl&gt;, `2/8/20` &lt;dbl&gt;,\n#   `2/9/20` &lt;dbl&gt;, `2/10/20` &lt;dbl&gt;, `2/11/20` &lt;dbl&gt;, `2/12/20` &lt;dbl&gt;,\n#   `2/13/20` &lt;dbl&gt;, `2/14/20` &lt;dbl&gt;, `2/15/20` &lt;dbl&gt;, `2/16/20` &lt;dbl&gt;,\n#   `2/17/20` &lt;dbl&gt;, `2/18/20` &lt;dbl&gt;, `2/19/20` &lt;dbl&gt;, `2/20/20` &lt;dbl&gt;, ‚Ä¶\n\n\nSimilarly, tail will show the last rows in the data\n\ntail(covid)\n\n# A tibble: 6 √ó 1,147\n  `Province/State` `Country/Region`      Lat  Long `1/22/20` `1/23/20` `1/24/20`\n  &lt;chr&gt;            &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 &lt;NA&gt;             Vietnam              14.1 108.          0         2         2\n2 &lt;NA&gt;             West Bank and Gaza   32.0  35.2         0         0         0\n3 &lt;NA&gt;             Winter Olympics 20‚Ä¶  39.9 116.          0         0         0\n4 &lt;NA&gt;             Yemen                15.6  48.5         0         0         0\n5 &lt;NA&gt;             Zambia              -13.1  27.8         0         0         0\n6 &lt;NA&gt;             Zimbabwe            -19.0  29.2         0         0         0\n# ‚Ñπ 1,140 more variables: `1/25/20` &lt;dbl&gt;, `1/26/20` &lt;dbl&gt;, `1/27/20` &lt;dbl&gt;,\n#   `1/28/20` &lt;dbl&gt;, `1/29/20` &lt;dbl&gt;, `1/30/20` &lt;dbl&gt;, `1/31/20` &lt;dbl&gt;,\n#   `2/1/20` &lt;dbl&gt;, `2/2/20` &lt;dbl&gt;, `2/3/20` &lt;dbl&gt;, `2/4/20` &lt;dbl&gt;,\n#   `2/5/20` &lt;dbl&gt;, `2/6/20` &lt;dbl&gt;, `2/7/20` &lt;dbl&gt;, `2/8/20` &lt;dbl&gt;,\n#   `2/9/20` &lt;dbl&gt;, `2/10/20` &lt;dbl&gt;, `2/11/20` &lt;dbl&gt;, `2/12/20` &lt;dbl&gt;,\n#   `2/13/20` &lt;dbl&gt;, `2/14/20` &lt;dbl&gt;, `2/15/20` &lt;dbl&gt;, `2/16/20` &lt;dbl&gt;,\n#   `2/17/20` &lt;dbl&gt;, `2/18/20` &lt;dbl&gt;, `2/19/20` &lt;dbl&gt;, `2/20/20` &lt;dbl&gt;, ‚Ä¶\n\n\nI saved a copy of the csv file that was downloaded during the same session that created the erroneous covid line plot.\n\ncovid_bad &lt;- read_csv(\"time_series_covid19_confirmed_global_BAD.csv\")\n\nRows: 270 Columns: 1147\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr    (2): Province/State, Country/Region\ndbl (1145): Lat, Long, 1/22/20, 1/23/20, 1/24/20, 1/25/20, 1/26/20, 1/27/20,...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe first rows of the tibble look to be the same\n\nhead(covid)\n\n# A tibble: 6 √ó 1,147\n  `Province/State` `Country/Region`   Lat  Long `1/22/20` `1/23/20` `1/24/20`\n  &lt;chr&gt;            &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 &lt;NA&gt;             Afghanistan       33.9 67.7          0         0         0\n2 &lt;NA&gt;             Albania           41.2 20.2          0         0         0\n3 &lt;NA&gt;             Algeria           28.0  1.66         0         0         0\n4 &lt;NA&gt;             Andorra           42.5  1.52         0         0         0\n5 &lt;NA&gt;             Angola           -11.2 17.9          0         0         0\n6 &lt;NA&gt;             Antarctica       -71.9 23.3          0         0         0\n# ‚Ñπ 1,140 more variables: `1/25/20` &lt;dbl&gt;, `1/26/20` &lt;dbl&gt;, `1/27/20` &lt;dbl&gt;,\n#   `1/28/20` &lt;dbl&gt;, `1/29/20` &lt;dbl&gt;, `1/30/20` &lt;dbl&gt;, `1/31/20` &lt;dbl&gt;,\n#   `2/1/20` &lt;dbl&gt;, `2/2/20` &lt;dbl&gt;, `2/3/20` &lt;dbl&gt;, `2/4/20` &lt;dbl&gt;,\n#   `2/5/20` &lt;dbl&gt;, `2/6/20` &lt;dbl&gt;, `2/7/20` &lt;dbl&gt;, `2/8/20` &lt;dbl&gt;,\n#   `2/9/20` &lt;dbl&gt;, `2/10/20` &lt;dbl&gt;, `2/11/20` &lt;dbl&gt;, `2/12/20` &lt;dbl&gt;,\n#   `2/13/20` &lt;dbl&gt;, `2/14/20` &lt;dbl&gt;, `2/15/20` &lt;dbl&gt;, `2/16/20` &lt;dbl&gt;,\n#   `2/17/20` &lt;dbl&gt;, `2/18/20` &lt;dbl&gt;, `2/19/20` &lt;dbl&gt;, `2/20/20` &lt;dbl&gt;, ‚Ä¶\n\n\nBut clearly there are fewer rows\n\ndim(covid_bad)\n\n[1]  270 1147\n\n\nAnd the last rows of the tibble are not the same as the complete dataset.\n\ntail(covid_bad)\n\n# A tibble: 6 √ó 1,147\n  `Province/State`   `Country/Region`   Lat   Long `1/22/20` `1/23/20` `1/24/20`\n  &lt;chr&gt;              &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Anguilla           United Kingdom    18.2 -63.1          0         0         0\n2 Bermuda            United Kingdom    32.3 -64.8          0         0         0\n3 British Virgin Is‚Ä¶ United Kingdom    18.4 -64.6          0         0         0\n4 Cayman Islands     United Kingdom    19.3 -81.3          0         0         0\n5 Channel Islands    United Kingdom    49.4  -2.36         0         0         0\n6 Falkland Islands ‚Ä¶ United Kingdom   -51.8 -59.5          0         0         0\n# ‚Ñπ 1,140 more variables: `1/25/20` &lt;dbl&gt;, `1/26/20` &lt;dbl&gt;, `1/27/20` &lt;dbl&gt;,\n#   `1/28/20` &lt;dbl&gt;, `1/29/20` &lt;dbl&gt;, `1/30/20` &lt;dbl&gt;, `1/31/20` &lt;dbl&gt;,\n#   `2/1/20` &lt;dbl&gt;, `2/2/20` &lt;dbl&gt;, `2/3/20` &lt;dbl&gt;, `2/4/20` &lt;dbl&gt;,\n#   `2/5/20` &lt;dbl&gt;, `2/6/20` &lt;dbl&gt;, `2/7/20` &lt;dbl&gt;, `2/8/20` &lt;dbl&gt;,\n#   `2/9/20` &lt;dbl&gt;, `2/10/20` &lt;dbl&gt;, `2/11/20` &lt;dbl&gt;, `2/12/20` &lt;dbl&gt;,\n#   `2/13/20` &lt;dbl&gt;, `2/14/20` &lt;dbl&gt;, `2/15/20` &lt;dbl&gt;, `2/16/20` &lt;dbl&gt;,\n#   `2/17/20` &lt;dbl&gt;, `2/18/20` &lt;dbl&gt;, `2/19/20` &lt;dbl&gt;, `2/20/20` &lt;dbl&gt;, ‚Ä¶\n\n\nWhat is incredibly unlucky in my case was that not all of the United Kingdom rows are present in the shorter dataset.\n\nfilter(covid, `Country/Region` == \"United Kingdom\")\n\n# A tibble: 15 √ó 1,147\n   `Province/State`          `Country/Region`    Lat    Long `1/22/20` `1/23/20`\n   &lt;chr&gt;                     &lt;chr&gt;             &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 Anguilla                  United Kingdom    18.2   -63.1          0         0\n 2 Bermuda                   United Kingdom    32.3   -64.8          0         0\n 3 British Virgin Islands    United Kingdom    18.4   -64.6          0         0\n 4 Cayman Islands            United Kingdom    19.3   -81.3          0         0\n 5 Channel Islands           United Kingdom    49.4    -2.36         0         0\n 6 Falkland Islands (Malvin‚Ä¶ United Kingdom   -51.8   -59.5          0         0\n 7 Gibraltar                 United Kingdom    36.1    -5.35         0         0\n 8 Guernsey                  United Kingdom    49.4    -2.59         0         0\n 9 Isle of Man               United Kingdom    54.2    -4.55         0         0\n10 Jersey                    United Kingdom    49.2    -2.14         0         0\n11 Montserrat                United Kingdom    16.7   -62.2          0         0\n12 Pitcairn Islands          United Kingdom   -24.4  -128.           0         0\n13 Saint Helena, Ascension ‚Ä¶ United Kingdom    -7.95  -14.4          0         0\n14 Turks and Caicos Islands  United Kingdom    21.7   -71.8          0         0\n15 &lt;NA&gt;                      United Kingdom    55.4    -3.44         0         0\n# ‚Ñπ 1,141 more variables: `1/24/20` &lt;dbl&gt;, `1/25/20` &lt;dbl&gt;, `1/26/20` &lt;dbl&gt;,\n#   `1/27/20` &lt;dbl&gt;, `1/28/20` &lt;dbl&gt;, `1/29/20` &lt;dbl&gt;, `1/30/20` &lt;dbl&gt;,\n#   `1/31/20` &lt;dbl&gt;, `2/1/20` &lt;dbl&gt;, `2/2/20` &lt;dbl&gt;, `2/3/20` &lt;dbl&gt;,\n#   `2/4/20` &lt;dbl&gt;, `2/5/20` &lt;dbl&gt;, `2/6/20` &lt;dbl&gt;, `2/7/20` &lt;dbl&gt;,\n#   `2/8/20` &lt;dbl&gt;, `2/9/20` &lt;dbl&gt;, `2/10/20` &lt;dbl&gt;, `2/11/20` &lt;dbl&gt;,\n#   `2/12/20` &lt;dbl&gt;, `2/13/20` &lt;dbl&gt;, `2/14/20` &lt;dbl&gt;, `2/15/20` &lt;dbl&gt;,\n#   `2/16/20` &lt;dbl&gt;, `2/17/20` &lt;dbl&gt;, `2/18/20` &lt;dbl&gt;, `2/19/20` &lt;dbl&gt;, ‚Ä¶\n\n\nHere are the rows for United Kingdom in the truncated data\n\nfilter(covid_bad, `Country/Region` == \"United Kingdom\")\n\n# A tibble: 6 √ó 1,147\n  `Province/State`   `Country/Region`   Lat   Long `1/22/20` `1/23/20` `1/24/20`\n  &lt;chr&gt;              &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Anguilla           United Kingdom    18.2 -63.1          0         0         0\n2 Bermuda            United Kingdom    32.3 -64.8          0         0         0\n3 British Virgin Is‚Ä¶ United Kingdom    18.4 -64.6          0         0         0\n4 Cayman Islands     United Kingdom    19.3 -81.3          0         0         0\n5 Channel Islands    United Kingdom    49.4  -2.36         0         0         0\n6 Falkland Islands ‚Ä¶ United Kingdom   -51.8 -59.5          0         0         0\n# ‚Ñπ 1,140 more variables: `1/25/20` &lt;dbl&gt;, `1/26/20` &lt;dbl&gt;, `1/27/20` &lt;dbl&gt;,\n#   `1/28/20` &lt;dbl&gt;, `1/29/20` &lt;dbl&gt;, `1/30/20` &lt;dbl&gt;, `1/31/20` &lt;dbl&gt;,\n#   `2/1/20` &lt;dbl&gt;, `2/2/20` &lt;dbl&gt;, `2/3/20` &lt;dbl&gt;, `2/4/20` &lt;dbl&gt;,\n#   `2/5/20` &lt;dbl&gt;, `2/6/20` &lt;dbl&gt;, `2/7/20` &lt;dbl&gt;, `2/8/20` &lt;dbl&gt;,\n#   `2/9/20` &lt;dbl&gt;, `2/10/20` &lt;dbl&gt;, `2/11/20` &lt;dbl&gt;, `2/12/20` &lt;dbl&gt;,\n#   `2/13/20` &lt;dbl&gt;, `2/14/20` &lt;dbl&gt;, `2/15/20` &lt;dbl&gt;, `2/16/20` &lt;dbl&gt;,\n#   `2/17/20` &lt;dbl&gt;, `2/18/20` &lt;dbl&gt;, `2/19/20` &lt;dbl&gt;, `2/20/20` &lt;dbl&gt;, ‚Ä¶\n\n\nWhat‚Äôs worse is that the row containing the covid cases for mainland Uk are only present in the full dataset. This is where most of the cases occur.\n\nfilter(covid, `Country/Region` == \"United Kingdom\", is.na(`Province/State`))\n\n# A tibble: 1 √ó 1,147\n  `Province/State` `Country/Region`   Lat  Long `1/22/20` `1/23/20` `1/24/20`\n  &lt;chr&gt;            &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 &lt;NA&gt;             United Kingdom    55.4 -3.44         0         0         0\n# ‚Ñπ 1,140 more variables: `1/25/20` &lt;dbl&gt;, `1/26/20` &lt;dbl&gt;, `1/27/20` &lt;dbl&gt;,\n#   `1/28/20` &lt;dbl&gt;, `1/29/20` &lt;dbl&gt;, `1/30/20` &lt;dbl&gt;, `1/31/20` &lt;dbl&gt;,\n#   `2/1/20` &lt;dbl&gt;, `2/2/20` &lt;dbl&gt;, `2/3/20` &lt;dbl&gt;, `2/4/20` &lt;dbl&gt;,\n#   `2/5/20` &lt;dbl&gt;, `2/6/20` &lt;dbl&gt;, `2/7/20` &lt;dbl&gt;, `2/8/20` &lt;dbl&gt;,\n#   `2/9/20` &lt;dbl&gt;, `2/10/20` &lt;dbl&gt;, `2/11/20` &lt;dbl&gt;, `2/12/20` &lt;dbl&gt;,\n#   `2/13/20` &lt;dbl&gt;, `2/14/20` &lt;dbl&gt;, `2/15/20` &lt;dbl&gt;, `2/16/20` &lt;dbl&gt;,\n#   `2/17/20` &lt;dbl&gt;, `2/18/20` &lt;dbl&gt;, `2/19/20` &lt;dbl&gt;, `2/20/20` &lt;dbl&gt;, ‚Ä¶\n\n\n\nfilter(covid_bad, `Country/Region` == \"United Kingdom\", is.na(`Province/State`))\n\n# A tibble: 0 √ó 1,147\n# ‚Ñπ 1,147 variables: Province/State &lt;chr&gt;, Country/Region &lt;chr&gt;, Lat &lt;dbl&gt;,\n#   Long &lt;dbl&gt;, 1/22/20 &lt;dbl&gt;, 1/23/20 &lt;dbl&gt;, 1/24/20 &lt;dbl&gt;, 1/25/20 &lt;dbl&gt;,\n#   1/26/20 &lt;dbl&gt;, 1/27/20 &lt;dbl&gt;, 1/28/20 &lt;dbl&gt;, 1/29/20 &lt;dbl&gt;, 1/30/20 &lt;dbl&gt;,\n#   1/31/20 &lt;dbl&gt;, 2/1/20 &lt;dbl&gt;, 2/2/20 &lt;dbl&gt;, 2/3/20 &lt;dbl&gt;, 2/4/20 &lt;dbl&gt;,\n#   2/5/20 &lt;dbl&gt;, 2/6/20 &lt;dbl&gt;, 2/7/20 &lt;dbl&gt;, 2/8/20 &lt;dbl&gt;, 2/9/20 &lt;dbl&gt;,\n#   2/10/20 &lt;dbl&gt;, 2/11/20 &lt;dbl&gt;, 2/12/20 &lt;dbl&gt;, 2/13/20 &lt;dbl&gt;, 2/14/20 &lt;dbl&gt;,\n#   2/15/20 &lt;dbl&gt;, 2/16/20 &lt;dbl&gt;, 2/17/20 &lt;dbl&gt;, 2/18/20 &lt;dbl&gt;, ‚Ä¶\n\n\nWe can filter the data for United Kingdom and use select to show the last column. The helper function last_col() is incredibly useful for this as we don‚Äôt need to know the name of the column. Using summarise we can add all the values in the column. This is the cumulative number of casees at the last date in the dataset. Dividing by one million (1e6) makes the numbers a bit easier to read.\n\nfilter(covid, `Country/Region` == \"United Kingdom\") %&gt;% \n  select(last_col()) %&gt;% \n  summarise(sum(.) / 1e6)\n\n# A tibble: 1 √ó 1\n  `sum(.)/1e+06`\n           &lt;dbl&gt;\n1           24.7\n\n\nFor the truncated dataset, the number of counts is substantially lower as we are missing the mainland Uk data. So it is not surprisingly that the trend lines for the Uk did not look correct.\n\nfilter(covid_bad, `Country/Region` == \"United Kingdom\") %&gt;% \n  select(last_col()) %&gt;% \n  summarise(sum(.))\n\n# A tibble: 1 √ó 1\n  `sum(.)`\n     &lt;dbl&gt;\n1    63439\n\n\nIn this particular instance I think my poor internet connection on the train wi-fi must have caused me to not download the complete file. Since no errors were produced I might not have detected the problem if I wasn‚Äôt trying to visualise data for United Kingdom which happened to be located towards the bottom of the file.\nChecking the dimensions of the tibble help to diagnose the problem, but there are couple of other techniques. R has an in-built function to check the size of a file.\n\nfile.size(\"raw_data/time_series_covid19_confirmed_global.csv\")\n\n[1] 1820194\n\n\n\nfile.size(\"time_series_covid19_confirmed_global_BAD.csv\")\n\n[1] 1714255\n\n\nA bit more rigourous is to calculate a checksum. In short, this is a calculated value that represents the exact contents of a file. If the file changes ‚Äî even by a single byte ‚Äî the checksum changes as well. That‚Äôs why it‚Äôs often described as a ‚Äúdigital fingerprint‚Äù for data integrity. This can be done in R by first loading the tools library and using the md5sum function.\n\n# Load the tools package\nlibrary(tools)\n\n# Compute MD5 checksum for a file\nfile_path &lt;- \"path/to/your/file.txt\"\nchecksum &lt;- md5sum(\"raw_data/time_series_covid19_confirmed_global.csv\")\nchecksum\n\nraw_data/time_series_covid19_confirmed_global.csv \n               \"095dfec62d4981a32630bed5094296f7\" \n\n\n\nchecksum_bad &lt;- md5sum(\"time_series_covid19_confirmed_global_BAD.csv\")\nchecksum_bad\n\ntime_series_covid19_confirmed_global_BAD.csv \n          \"179af9d326d1c9451c49e5d9821c85e0\" \n\n\nIn practice if someone is sending data you large, especially if the file is large, they will also send a file containing checksums for you to check data integrity. The following would print TRUE if both files had exactly the same contents.\n\nchecksum == checksum_bad\n\nraw_data/time_series_covid19_confirmed_global.csv \n                                            FALSE"
  },
  {
    "objectID": "training/bulk-rnaseq_1/index.html#pre-amble",
    "href": "training/bulk-rnaseq_1/index.html#pre-amble",
    "title": "Introduction to RNA-Seq - Part 1",
    "section": "",
    "text": "High-throughput sequencing is now established as a standard technique for many functional genomics studies; allowing the researcher to compare and contrast the transcriptomes of many individuals to obtain biological insight. A high-volume of data are generated from these experimental techniques and thus require robust and reproducible tools to be employed in the analysis.\nIn this workshop, you will be learning how to analyse RNA-seq count data, using R. This will include reading the data into R, quality control and performing differential expression analysis and gene set testing, with a focus on the well-respected DESEq2 analysis workflow. You will learn how to generate common plots for analysis and visualisation of gene expression data, such as boxplots and heatmaps.\n\n\n\n\n\n\nNote\n\n\n\nWe will be discussing Bulk RNA-seq only, although some of the methods and techniques will be applicable to single-cell RNA-seq. I am planning some materials on single-cell RNA-seq in the future. In the meantime, the homepage for Seurat (a popular R package for single-cell analysis) has lots of useful tutorials.\n\nSeurat homepage"
  },
  {
    "objectID": "training/bulk-rnaseq_1/index.html#setup",
    "href": "training/bulk-rnaseq_1/index.html#setup",
    "title": "Introduction to RNA-Seq - Part 1",
    "section": "Setup",
    "text": "Setup\nYou will need to install some R packages before you start, which I usually do this at RStudio‚Äôs console. See the below screenshot.\n\nRather than typing the location of the install script I suggest copying from here:-\n\nsource(\"https://raw.githubusercontent.com/markdunning/markdunning.github.com/refs/heads/master/files/training/bulk_rnaseq/install_bioc_packages.R\")\n\nIt may take a few minutes, but you will only have to do this once for a specific version of R. To check that everything worked, now copy and paste the following command. It should print messages to the screen to say that all the packages were installed\n\nsource(\"https://raw.githubusercontent.com/markdunning/markdunning.github.com/refs/heads/master/files/training/bulk_rnaseq/check_packages.R\")\n\nI also recommend creating a new project to work through the tutorial. You can do this via the file menu in Rstudio and it will ask you to choose a directory on your hard drive that you want the project to be located in. Briefly, using ‚Äúprojects‚Äù is a convenient way of keeping all the input data, R code, and outputs for a particular analysis together in the same place.\nFile -&gt; New Project -&gt; New Directory\n\nIn the screenshot, I am using a directory (which doesn‚Äôt exist at this point) called bulkrnaseq_tutorial in c:\\work\\personal_development. RStudio should now refresh and your working directory will be the location that you specified. Now we will use some code to download the example data. Again, I suggest you copy from below rather than typing manually.\n\n## Create two folders for the meta data and counts\ndir.create(\"meta_data\", showWarnings = FALSE)\ndir.create(\"raw_counts\",showWarnings = FALSE)\n\n\n## Download the raw data files\ndownload.file(\"https://raw.githubusercontent.com/markdunning/markdunning.github.com/refs/heads/master/files/training/bulk_rnaseq/meta_data/sampleInfo.csv\", destfile = \"meta_data/sampleInfo.csv\")\ndownload.file(\"https://raw.githubusercontent.com/markdunning/markdunning.github.com/refs/heads/master/files/training/bulk_rnaseq/raw_counts/raw_counts_matrix.tsv\", destfile = \"raw_counts/raw_counts_matrix.tsv\")\n\nHopefully your screen should look a bit like this:-\n\nFinally, create a new ‚ÄúQuarto Document‚Äù from the File menu\nFile -&gt; New File -&gt; Quarto Document\n\nClicking the ‚ÄúCreate Empty Document‚Äù button on the pop-up that appears will create a blank quarto document that you can use to type the code from this tutorial, and any comments you may wish to make along the way. The Title and Author boxes can be filled with anything you like. They are used when you want to create a report document from your code. A new panel should appear in RStudio which is your bare bones analysis. The title and author lines should correspond to the text you entered (if any) when creating the document\nR code can be added to this document by clicking the ‚ÄúInsert Code Chunk‚Äù toolbar option\n\nThis will give you space to type or paste R code from the tutorial. In the below screenshot, R code can be entered between lines 8 and 10. Pressing ENTER between these lines will allow more code to be written. Pressing CTRL + ENTER causes the code to be run.\n\nLines outside of a code chunk can be used to write any explanations or interpretations of the plots, stats that you produce along the way.\nAt this point you are working on an untitled document that is not saved to disk. Go through the menu File -&gt; Save and choose a file name"
  },
  {
    "objectID": "training/bulk-rnaseq_1/index.html#quick-start",
    "href": "training/bulk-rnaseq_1/index.html#quick-start",
    "title": "Introduction to RNA-Seq - Part 1",
    "section": "Quick Start",
    "text": "Quick Start\nI will go through the setup in quite a bit of detail. If you are already fairly confident with R, you can probably skim this and proceed to the start of the pre-processing section.\n\nCreate a new RStudio project that you want to work in\nCreate folders called meta_data and raw_counts\nDownload the meta data and raw counts\n\nmeta data\nraw counts\n\nPlace the sampleInfo.csv and raw_counts_matrix.tsv files into meta_data and raw_counts folders respectively\nInstall some R packages using this command\n\n\nsource(\"https://raw.githubusercontent.com/markdunning/markdunning.github.com/refs/heads/master/files/training/bulk_rnaseq/install_bioc_packages.R\")"
  },
  {
    "objectID": "training/bulk-rnaseq_1/index.html#rna-seq-processing",
    "href": "training/bulk-rnaseq_1/index.html#rna-seq-processing",
    "title": "Introduction to RNA-Seq - Part 1",
    "section": "RNA-seq processing",
    "text": "RNA-seq processing\nIn this short section, we will briefly describe the pre-processing of RNA-seq data which is not typically done in R. If you are only interested in using R, you may skip to the next section.\nThere are many steps involved in analysing an RNA-Seq experiment.\n\n(Workflow image from Harvard Bioinformatics Core)\nAnalysing an RNAseq experiment begins with sequencing reads. These are large (typically several Gb) that contain information on the sequences that have been generated for each biological sample; one fastq (or pair of fastqs) for each sample. Each set of four lines describe one sequence (called a ‚Äúread‚Äù).\nA typical RNA-seq experiment will have 10 - 30 million reads in a fastq file, with each read about 100 bases long. e.g.\n@D0UW5ACXX120511:8:1204:6261:40047/1\nAATGTTTATGTTCTTAAATTTTAGTTGTATATGTGAATCTTTGTAGTTTTTGCTAAAATACTAAGTAATTTATATAAAAGTGAGTTAAGAGATTTTTCTGA\n+\nCCCFFFFFHHHHHJJJJJIJJJJJIJJHIIJIJIJJIJJJIJJHIIHIJJJJJJBEGIHIJICGIDICFGIJJJIIJJGJ&gt;F&gt;GAGCGEEHEHHEEFFFD&gt;\nAs the fastq files are large, we tend to analyse them using command-line software and a computing cluster. The traditional workflow for RNA-seq compares the sequences to a reference genome to see which genomic region each read matches the best.\n\nAgain, this requires more memory than a typical laptop or desktop machine so is performed on a remote computer with large memory. The resulting file is called a bam and records the best genomic match for each read. However, as we are interested in gene expression we want to relate these mappings to the positions of genes.\nA variety of different counting methods can determine how many reads overlap each known gene region. These are know as the raw counts and are the kind of data we will start with today.\n\nRecent tools for RNA-seq analysis (e.g.¬†salmon, kallisto) do not require the time-consuming step of whole-genome alignment to be performed, and can therefore produce gene-level counts in a much faster time frame. They not require the creation of large bam files, which is useful if constrained by file space (e.g.¬†if using Galaxy).\nMy strong recommendation would be to use the nextflow workflow system in conjunction with the nf.core pipeline to align and quantify your RNA-seq reads. My former colleague Dr.¬†Lewis Quayle has a really good write-up of using nf.core on his pages\n\nhttps://www.lewisdoesdata.com/2024/05/01/bulk-rnaseq-end-to-end-part-1.html\n\n\n\n\n\n\n\nImportant\n\n\n\nUnless you are doing something extremely novel and bespoke I don‚Äôt believe it would be worth writing your own pipeline for processing RNA-seq, or any other sequencing data, from scratch rather than using what is available in nf.core."
  },
  {
    "objectID": "posts/2025_10_20_cellxgene/index.html",
    "href": "posts/2025_10_20_cellxgene/index.html",
    "title": "Exploring Single-cell RNA-seq",
    "section": "",
    "text": "How to grab public 10X single-cell RNA-seq and visualise in R\n\n\nUse the Bioconductor package cellxgenedp to download the dataset in Seurat format. More information about this package can be found here\n\nhttps://www.bioconductor.org/packages/release/bioc/vignettes/cellxgenedp/inst/doc/a_using_cellxgenedp.html\n\nIn particular the vignette demonstrates how to search for particular datasets of interest. Since we know the dataset we want, we can use the dataset id to download directly.\n\nlibrary(cellxgenedp)\ndb &lt;- db()\nlocal_file &lt;- files(db) |&gt;\nfilter(\n        dataset_id == \"9fddb063-056d-4202-8b8a-4b0ee531d3ce\",\n        filetype == \"RDS\"\n    ) |&gt;\n    files_download(dry.run = FALSE)\n\nfile.copy(local_file,\".\")\n\nOnce downloaded, we can read the local file into R.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(Seurat)\nbrca &lt;- readRDS(\"cbbb607f-578b-47ba-858b-407bb8be917f.rds\")\n#brca &lt;- NormalizeData(brca)\n\nAs a quick check we can take a look at the UMAP coloured by different subtypes - which should agree with the figures generated from the same dataset on single-cell atlas.\n\nDimPlot(brca,reduction = \"umap\",group.by = \"celltype_major\")\n\n\nVlnPlot(brca, \n        features = \"ENSG00000089041\",\n        split.by = \"subtype\",group.by = \"celltype_major\")\nggsave(\"P2RX7_cell_type_violins.png\")\n\n\np1 &lt;- FeaturePlot(brca, \"ENSG00000089041\")\np2 &lt;- DimPlot(brca,reduction = \"umap\",group.by = \"celltype_major\")\n\ncowplot::plot_grid(p1,p2)\nggsave(\"P2RX7_UMAPs.png\",width=8,height=6)\n\n\nhypoxia_genes &lt;- readxl::read_xls(\"Buffa Hypoxic score genes.xls\",skip = 4) %&gt;% \n  pull(`HUGO Symbol`) \n\nlibrary(org.Hs.eg.db)\nhypoxia_ens &lt;- AnnotationDbi::select(org.Hs.eg.db,\n                              keys = hypoxia_genes,\n                              keytype = \"SYMBOL\",\n                              columns = \"ENSEMBL\") %&gt;% \n                              pull(ENSEMBL) %&gt;% na.omit\n\n\nlibrary(ggpubr)\nbrca &lt;- AddModuleScore(brca, features = list(hypoxia_ens),name=\"hypoxia_score\")\nbrca$HypoxiaGroup &lt;- ifelse(brca$hypoxia_score1 &gt; quantile(brca$hypoxia_score1, 0.75), \"High\", \"Low\") # Example: Top 25% as \"High\"\n\n## get expression values for P2RX7\ngene_expression &lt;- GetAssayData(brca, slot = \"data\", assay = \"RNA\")[\"ENSG00000089041\", ]\n\nbrca$P2RX7 &lt;- gene_expression\n\n\nbrca_ce &lt;- subset(brca, celltype_major == \"Cancer Epithelial\")\n  \nbrca_ce@meta.data %&gt;% \nggplot(aes(x= subtype, y = hypoxia_score1)) + geom_violin() + stat_compare_means() + ylab(\"Hypoxia Score\")\n\nggsave(\"hypoxia_score_violins.png\")\n\n\nanno &lt;- select(org.Hs.eg.db, keys = rownames(brca),\n               keytype = \"ENSEMBL\",\n               columns = c(\"SYMBOL\",\"GENENAME\"))\nhypoxia_markers &lt;- FindMarkers(brca_ce, \n                               ident.1 = \"Low\",\n                               ident.2 = \"High\",\n                               group.by = \"HypoxiaGroup\") %&gt;% \n  tibble::rownames_to_column(\"ENSEMBL\") %&gt;% left_join(anno)\n\n\nAn immediate thought is, although the myeloid cells have higher expression of P2RX7 (which is expected), is there a significant difference in the expression levels between the Normal epithelial cells and cancer epithelial cells in either of the subtypes - could this be evaluated?\n\n\nplot_data &lt;- brca@meta.data %&gt;%\n  filter(grepl(\"Epithelial\", celltype_major)) %&gt;%\n  group_by(subtype, celltype_major) %&gt;%\n  mutate(total_cells = n()) %&gt;% \n  mutate(non_zero_cells = sum(P2RX7 &gt; 0)) %&gt;%\n  dplyr::select(total_cells, non_zero_cells,celltype_major, subtype, P2RX7) %&gt;% \n  ungroup()\n\nplot_data %&gt;% \n  ggplot(aes(x = celltype_major, y = P2RX7)) + geom_violin() + geom_jitter(width=0.1) + stat_compare_means() + facet_wrap(~subtype) +\n  geom_text(\n    aes(label = paste(\"n =\", total_cells, \"(non zero counts:\", non_zero_cells, \")\")),\n    y = max(plot_data$P2RX7) * 0.9, # Adjust vertical position\n    size = 3,\n    vjust = 0\n  )"
  },
  {
    "objectID": "posts/2025_10_20_cellxgene/index.html#download-the-data-from-cellxgene",
    "href": "posts/2025_10_20_cellxgene/index.html#download-the-data-from-cellxgene",
    "title": "Exploring Single-cell RNA-seq",
    "section": "",
    "text": "Use the Bioconductor package cellxgenedp to download the dataset in Seurat format. More information about this package can be found here\n\nhttps://www.bioconductor.org/packages/release/bioc/vignettes/cellxgenedp/inst/doc/a_using_cellxgenedp.html\n\nIn particular the vignette demonstrates how to search for particular datasets of interest. Since we know the dataset we want, we can use the dataset id to download directly.\n\nlibrary(cellxgenedp)\ndb &lt;- db()\nlocal_file &lt;- files(db) |&gt;\nfilter(\n        dataset_id == \"9fddb063-056d-4202-8b8a-4b0ee531d3ce\",\n        filetype == \"RDS\"\n    ) |&gt;\n    files_download(dry.run = FALSE)\n\nfile.copy(local_file,\".\")\n\nOnce downloaded, we can read the local file into R.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(Seurat)\nbrca &lt;- readRDS(\"cbbb607f-578b-47ba-858b-407bb8be917f.rds\")\n#brca &lt;- NormalizeData(brca)\n\nAs a quick check we can take a look at the UMAP coloured by different subtypes - which should agree with the figures generated from the same dataset on single-cell atlas.\n\nDimPlot(brca,reduction = \"umap\",group.by = \"celltype_major\")\n\n\nVlnPlot(brca, \n        features = \"ENSG00000089041\",\n        split.by = \"subtype\",group.by = \"celltype_major\")\nggsave(\"P2RX7_cell_type_violins.png\")\n\n\np1 &lt;- FeaturePlot(brca, \"ENSG00000089041\")\np2 &lt;- DimPlot(brca,reduction = \"umap\",group.by = \"celltype_major\")\n\ncowplot::plot_grid(p1,p2)\nggsave(\"P2RX7_UMAPs.png\",width=8,height=6)\n\n\nhypoxia_genes &lt;- readxl::read_xls(\"Buffa Hypoxic score genes.xls\",skip = 4) %&gt;% \n  pull(`HUGO Symbol`) \n\nlibrary(org.Hs.eg.db)\nhypoxia_ens &lt;- AnnotationDbi::select(org.Hs.eg.db,\n                              keys = hypoxia_genes,\n                              keytype = \"SYMBOL\",\n                              columns = \"ENSEMBL\") %&gt;% \n                              pull(ENSEMBL) %&gt;% na.omit\n\n\nlibrary(ggpubr)\nbrca &lt;- AddModuleScore(brca, features = list(hypoxia_ens),name=\"hypoxia_score\")\nbrca$HypoxiaGroup &lt;- ifelse(brca$hypoxia_score1 &gt; quantile(brca$hypoxia_score1, 0.75), \"High\", \"Low\") # Example: Top 25% as \"High\"\n\n## get expression values for P2RX7\ngene_expression &lt;- GetAssayData(brca, slot = \"data\", assay = \"RNA\")[\"ENSG00000089041\", ]\n\nbrca$P2RX7 &lt;- gene_expression\n\n\nbrca_ce &lt;- subset(brca, celltype_major == \"Cancer Epithelial\")\n  \nbrca_ce@meta.data %&gt;% \nggplot(aes(x= subtype, y = hypoxia_score1)) + geom_violin() + stat_compare_means() + ylab(\"Hypoxia Score\")\n\nggsave(\"hypoxia_score_violins.png\")\n\n\nanno &lt;- select(org.Hs.eg.db, keys = rownames(brca),\n               keytype = \"ENSEMBL\",\n               columns = c(\"SYMBOL\",\"GENENAME\"))\nhypoxia_markers &lt;- FindMarkers(brca_ce, \n                               ident.1 = \"Low\",\n                               ident.2 = \"High\",\n                               group.by = \"HypoxiaGroup\") %&gt;% \n  tibble::rownames_to_column(\"ENSEMBL\") %&gt;% left_join(anno)\n\n\nAn immediate thought is, although the myeloid cells have higher expression of P2RX7 (which is expected), is there a significant difference in the expression levels between the Normal epithelial cells and cancer epithelial cells in either of the subtypes - could this be evaluated?\n\n\nplot_data &lt;- brca@meta.data %&gt;%\n  filter(grepl(\"Epithelial\", celltype_major)) %&gt;%\n  group_by(subtype, celltype_major) %&gt;%\n  mutate(total_cells = n()) %&gt;% \n  mutate(non_zero_cells = sum(P2RX7 &gt; 0)) %&gt;%\n  dplyr::select(total_cells, non_zero_cells,celltype_major, subtype, P2RX7) %&gt;% \n  ungroup()\n\nplot_data %&gt;% \n  ggplot(aes(x = celltype_major, y = P2RX7)) + geom_violin() + geom_jitter(width=0.1) + stat_compare_means() + facet_wrap(~subtype) +\n  geom_text(\n    aes(label = paste(\"n =\", total_cells, \"(non zero counts:\", non_zero_cells, \")\")),\n    y = max(plot_data$P2RX7) * 0.9, # Adjust vertical position\n    size = 3,\n    vjust = 0\n  )"
  },
  {
    "objectID": "training/bulk-rnaseq_1/index.html#introducing-the-example-dataset",
    "href": "training/bulk-rnaseq_1/index.html#introducing-the-example-dataset",
    "title": "Introduction to RNA-Seq - Part 1",
    "section": "Introducing the example dataset",
    "text": "Introducing the example dataset\nThe data for this tutorial comes from the paper, Induction of fibroblast senescence generates a non-fibrogenic myofibroblast phenotype that differentially impacts on cancer prognosis..\n\nCancer associated fibroblasts characterized by an myofibroblastic phenotype play a major role in the tumour microenvironment, being associated with poor prognosis. We found that this cell population is made in part by senescent fibroblasts in vivo. As senescent fibroblasts and myofibroblasts have been shown to share similar tumour promoting functions in vitro we compared the transcriptosomes of these two fibroblast types and performed RNA-seq of human foetal foreskin fibroblasts 2 (HFFF2) treated with 2ng/ml TGF-beta-1 to induce myofibroblast differentiation or 10Gy gamma irradiation to induce senescence. We isolated RNA 7 days upon this treatments changing the medium 3 days before the RNA extraction.\n\n\n\n\n\n\n\nNote\n\n\n\nA really useful resource for obtaining raw sequencing reads is sraexplorer. Given a GEO, ArrayExpress of SRA dataset name it will give you links to download the raw fastq files\n\nSRA Explorer\n\nAgain, Lewis‚Äô page has some commands for downloading these data in fastq form.\n\nDownloading the example data"
  },
  {
    "objectID": "training/bulk-rnaseq_1/index.html#about-metadata",
    "href": "training/bulk-rnaseq_1/index.html#about-metadata",
    "title": "Introduction to RNA-Seq - Part 1",
    "section": "About ‚Äúmetadata‚Äù",
    "text": "About ‚Äúmetadata‚Äù\nModern sequencing devices are truly remarkable machines capable of taking biological material and generating vast amounts of biological sequence. However, they know nothing about the biological context of the experiment taking place - nor do they need to. If you are a researcher requesting sequencing from a Core facility you will typically submit a set of test tubes (containing prepared sequencing libraries) along with a spreadsheet giving a name to each sample. These names can be as simple as ‚ÄúSample 1‚Äù, ‚ÄúSample 2‚Äù etc. When the data come back from sequencing, the raw data will retain the labels ‚ÄúSample 1‚Äù, ‚ÄúSample 2‚Äù. We refer to metadata as the data that describes the biological and technical characteristics of the samples we have sequenced\n. Examples of variables recorded in the metadata might include.\n\ntumour / normal status\ncell line\nage\ngender\ndate of collection\nlitter\n\nWe include the sample groups that we want to compare, and any potential confounding factors that we might need to address as part of our quality assessment. The metadata is stored in a spreadsheet and typically entered by-hand. When creating such data we should be mindful of some best-practice guidelines that will make our data easier to read into R.\n\n\n\n\n\n\nNote\n\n\n\nSee here for a round-up of common errors to be avoiding when creating spreadsheets\n\nData Carpentry lesson on spreadsheet errors\n\n\n\nThe sampleInfo.csv in the meta_data folder contains basic information about the samples that we will need for the analysis today. This includes the ID for the sample ID assigned by the researcher (Run), the experimental condition (condition), shorter name for the sample (Name), the replicate number (Replicate) and whether the sample is a control or treated sample (Treated).\n\nsampleInfo &lt;- read.csv(\"meta_data/sampleInfo.csv\")\nsampleInfo\n\n          Run condition  Name Replicate Treated\n1  1_CTR_BC_2       CTR CTR_1         1       N\n2  2_TGF_BC_4       TGF TGF_1         1       Y\n3   3_IR_BC_5        IR  IR_1         1       Y\n4  4_CTR_BC_6       CTR CTR_2         2       N\n5  5_TGF_BC_7       TGF TGF_2         2       Y\n6  6_IR_BC_12        IR  IR_2         2       Y\n7 7_CTR_BC_13       ctr CTR_3         3       N\n8 8_TGF_BC_14       tgf TGF_3         3       Y\n9  9_IR_BC_15        ir  IR_3         3       Y\n\n\nIf you want to know how to create such a data frame based on the column names alone (i.e.¬†if no csv file was provided), read the following note.\n\n\n\n\n\n\nInferring the sample information from the count column names\n\n\n\n\n\nIt is sometimes possible to create a meta data spreadsheet if none is available. This relies on the column names of the count matrix being named in a consistent fashion. In this example we can see that the biological group (CTR, TGF or IR) is encoded in the name and the column names contain a _ character. The stringr and tidyr packages include the functionality for dealing cleaning these data.\n\nstringr reference guide\n\n\ninstall.packages(\"stringr\")\ninstall.packages(\"tidyr\")\n\n\nlibrary(stringr)\nlibrary(tidyr)\nlibrary(dplyr)\n\n## Get the column names of counts - except the first column which is the gene name\ncols &lt;- colnames(counts)[-1]\ncols \n\n[1] \"1_CTR_BC_2\"  \"2_TGF_BC_4\"  \"3_IR_BC_5\"   \"4_CTR_BC_6\"  \"5_TGF_BC_7\" \n[6] \"6_IR_BC_12\"  \"7_CTR_BC_13\" \"8_TGF_BC_14\" \"9_IR_BC_15\" \n\n\nThe column names consists of various letters separated by a _. Some of these can be used to form the conditions and replicate information, and the separate function from tidyr is an efficient way of splitting the Run column into different components. The BC and number at the each of the name is not actually useful analysis, so we will remove in the next step. The remove = FALSE argument is used to keep the Run column in the data.\n\nmeta &lt;- data.frame(Run = cols) %&gt;% \n  separate(Run, into = c(\"Sample Number\", \"condition\", \"BC\", \"X\"),remove = FALSE)\nmeta\n\n          Run Sample Number condition BC  X\n1  1_CTR_BC_2             1       CTR BC  2\n2  2_TGF_BC_4             2       TGF BC  4\n3   3_IR_BC_5             3        IR BC  5\n4  4_CTR_BC_6             4       CTR BC  6\n5  5_TGF_BC_7             5       TGF BC  7\n6  6_IR_BC_12             6        IR BC 12\n7 7_CTR_BC_13             7       CTR BC 13\n8 8_TGF_BC_14             8       TGF BC 14\n9  9_IR_BC_15             9        IR BC 15\n\n\nNow we keep the columns we need, and add the replicate numbers using the rep function to repeat a sequence 1,2,3 three times. The shorter name is formed by pasting the condition and replicate number (using paste with a _ separator. Finally, Treated column is formed using an ifelse statement. This is used to test if a particular value of condition is equal to CTR or not. If it is, a value of N is used in the new Treated column. If not, the value of Y is used instead.\n\nmeta &lt;- select(meta, Run, condition) %&gt;% \n  mutate(Replicate = rep(c(1,2,3), 3)) %&gt;% \n  mutate(Name = paste(condition, Replicate, sep = \"_\")) %&gt;% \n  mutate(Treated = ifelse(condition == \"CTR\", \"N\", \"Y\"))\n\n\n\n\nAlthough we can use basic R commands to read the counts and sample information into R, it is much more common to use specialist software for further analysis. Packages such as DESeq2 (used in this tutorial), edgeR and limma have evolved other many years to offer an efficient way of storing and manipulating complex datasets.\n\nlibrary(DESeq2)\n\nThe DESeq2 allows data to be imported from various formats, and the option we will be using here is that of a counts matrix. The ‚Äúvignette‚Äù for DESeq2 is incredibly detailed, and offers code for importing data produced by different software\n\nDESeq2 Guide\n\nHowever, as we said the count matrix is the most common. It is important to note, as described in the vignette, that your counts must be raw and not subjected to any kind of normalisation or calibration. These steps are included in the standard workflow as we will see.\nThe code to import our data as given below. We also have to specify at this point a design for analysis. In the simplest terms this is essentially telling DESeq2 which column in our dataset to use when making comparisons between groups. DESeq2 will not actually do any differential expression unless we tell it to, so this is just a placeholder value for now. We can also introduce more complicated designs that might more than one column.\nWe would normally think of a ‚Äúcount matrix‚Äù as containing just numeric data. Our counts contains gene identifiers which might usually be a problem but DESeq2 is still able to import the data if we set tidy=TRUE.\n\ndds &lt;- DESeqDataSetFromMatrix(counts, \n                                  colData = sampleInfo, \n                                  design = ~condition, tidy = TRUE)\n\nWarning in DESeqDataSet(se, design = design, ignoreRank): some variables in\ndesign formula are characters, converting to factors\n\n\n\n\n\n\n\n\nWarning about ‚Äúconverting to factors‚Äù\n\n\n\nYou will probably get a Warning message including the phrase some variables in design formula are characters, converting to factors. This is perfectly fine and just means that DESeq2 has converted some columns in your sample information from characters into factors. i.e.¬†a column containing a categorical variable.\n\n\nPrinting the contents of dds to the screen gives some details of how the data are represented, and happily for us it does not print all the contents to the screen\n\ndds\n\nclass: DESeqDataSet \ndim: 57914 9 \nmetadata(1): version\nassays(1): counts\nrownames(57914): ENSG00000000003 ENSG00000000005 ... ENSG00000284747\n  ENSG00000284748\nrowData names(0):\ncolnames(9): 1_CTR_BC_2 2_TGF_BC_4 ... 8_TGF_BC_14 9_IR_BC_15\ncolData names(5): Run condition Name Replicate Treated\n\n\nThe object contains all the counts which can be retrieved using the counts function. The head function is used so that only the first six rows (genes) are shown.\nThe gene names that appear in the rows are clearly not very useful for us now, but these can be converted into something more manageable when we come to interpret our data.\n\nhead(counts(dds))\n\n                1_CTR_BC_2 2_TGF_BC_4 3_IR_BC_5 4_CTR_BC_6 5_TGF_BC_7\nENSG00000000003       1579       1547      1342       1704       1395\nENSG00000000005          0          0         0          0          0\nENSG00000000419       1774       1775      1866       1809       1921\nENSG00000000457        698        617       601        733        662\nENSG00000000460        246        309       116        224        255\nENSG00000000938         10          5        11          6          1\n                6_IR_BC_12 7_CTR_BC_13 8_TGF_BC_14 9_IR_BC_15\nENSG00000000003       1264        1556        1370       1269\nENSG00000000005          0           0           0          1\nENSG00000000419       1776        1797        1482       1980\nENSG00000000457        537         751         593        628\nENSG00000000460        108         245         289        127\nENSG00000000938          2          19           4          8\n\n\nWhereas colData will display the meta data that has been stored with the object.\n\ncolData(dds)\n\nDataFrame with 9 rows and 5 columns\n                    Run condition        Name Replicate     Treated\n            &lt;character&gt;  &lt;factor&gt; &lt;character&gt; &lt;integer&gt; &lt;character&gt;\n1_CTR_BC_2   1_CTR_BC_2       CTR       CTR_1         1           N\n2_TGF_BC_4   2_TGF_BC_4       TGF       TGF_1         1           Y\n3_IR_BC_5     3_IR_BC_5       IR         IR_1         1           Y\n4_CTR_BC_6   4_CTR_BC_6       CTR       CTR_2         2           N\n5_TGF_BC_7   5_TGF_BC_7       TGF       TGF_2         2           Y\n6_IR_BC_12   6_IR_BC_12       IR         IR_2         2           Y\n7_CTR_BC_13 7_CTR_BC_13       ctr       CTR_3         3           N\n8_TGF_BC_14 8_TGF_BC_14       tgf       TGF_3         3           Y\n9_IR_BC_15   9_IR_BC_15       ir         IR_3         3           Y\n\n\nIndividual columns from the metadata can also be accessed and printed using the $ notation. From the warning message that appeared when we created dds we saw that it already converted Treated into a factor\n\ndds$condition\n\n[1] CTR TGF IR  CTR TGF IR  ctr tgf ir \nLevels: ctr CTR ir IR tgf TGF\n\n\nWhereas Treated is still a character vector.\n\ndds$Treated\n\n[1] \"N\" \"Y\" \"Y\" \"N\" \"Y\" \"Y\" \"N\" \"Y\" \"Y\"\n\n\n\nVisualising library sizes\nWe can look at a few different plots to check that the data is good quality, and that the samples are behaving as we would expect. First, we can check how many reads we have for each sample in the DESeqDataSet. We will never get exactly the same total of total counts for each sample, but they should be roughly the same. Low total number of counts could be indicative of poor quality sample.\nThe counts themselves are accessed using the counts function; giving a matrix of counts. The sum of a particular column is therefore the total number of reads for that sample.\n\nsum(counts(dds)[,1])\n\n[1] 37966392\n\n\nA convenience function colSums exists for calculating the sum of each column in a matrix, returning a vector as a result.\n\ncolSums(counts(dds))\n\n 1_CTR_BC_2  2_TGF_BC_4   3_IR_BC_5  4_CTR_BC_6  5_TGF_BC_7  6_IR_BC_12 \n   37966392    42302453    33300002    39401879    37716366    32599748 \n7_CTR_BC_13 8_TGF_BC_14  9_IR_BC_15 \n   34273109    38522174    36478190 \n\n\n\n\nExercise\n\nUse an appropriate function from dplyr to add a column containing the number of reads for each sample to the sampleInfo data frame.\nProduce a bar plot to show the Millions of reads for each sample (see below)\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(ggplot2)\nmutate(sampleInfo, LibSize = colSums(assay(dds))/1e6) %&gt;% \n  ggplot(aes(x = Name, y = LibSize)) + geom_col(fill=\"steelblue\") + geom_hline(yintercept = 20,col=\"red\",lty=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising count distributions\nWe typically use a boxplot to visualise difference the distributions of the columns of a numeric data frame. Applying the boxplot function to the raw counts from our dataset reveals something about the nature of the data; the distributions are dominated by a few genes with very large counts. We are using the base boxplot function here for convenience because the count data are not in the long data format required by ggplot2.\n\nboxplot(counts(dds))\n\n\n\n\n\n\n\n\nWe can use the vst or rlog function from DESeq2to compensate for the effect of different library sizes and put the data on the log\\(_2\\) scale. The effect is to remove the dependence of the variance on the mean, particularly the high variance of the logarithm of count data when the mean is low. For more details see the DESeq2 vignette\n\n# Get log2 counts\nvsd &lt;- vst(dds)\n# Check distributions of samples using boxplots\nboxplot(assay(vsd), xlab=\"\", ylab=\"Log2 counts per million\",las=2,main=\"Normalised Distributions\")\n\nabline(h=median(assay(vsd)), col=\"blue\")\n\n\n\n\n\n\n\n\nWe can see that using vst has made the distributions more comparable, and indeed these transformed data will form the basis for most of our quality assessment. They are not however used for any differential analysis. DESeq2 has it‚Äôs own normalisation method"
  },
  {
    "objectID": "training/bulk-rnaseq_1/index.html#inferring-the-sample-information-from-the-count-column-names",
    "href": "training/bulk-rnaseq_1/index.html#inferring-the-sample-information-from-the-count-column-names",
    "title": "Introduction to RNA-Seq - Part 1",
    "section": "Inferring the sample information from the count column names",
    "text": "Inferring the sample information from the count column names\nIt is sometimes possible to create a meta data spreadsheet if none is available. This relies on the column names of the count matrix being named in a consistent fashion. In this example we can see that the biological group (CTR, TGF or IR) is encoded in the name and the column names contain a _ character. The stringr package includes many useful functions for dealing with strings in R. The columns have an X at the start, which is usually caused by R not being able to handle column names that start with a number\n\nstringr reference guide\n\n\ninstall.packages(\"stringr\")\n\n\nlibrary(stringr)\nlibrary(tidyr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n## Get the column names of counts - except the first column which is the gene name\ncols &lt;- colnames(counts)[-1]\ncols \n\n[1] \"X1_CTR_BC_2\"  \"X2_TGF_BC_4\"  \"X3_IR_BC_5\"   \"X4_CTR_BC_6\"  \"X5_TGF_BC_7\" \n[6] \"X6_IR_BC_12\"  \"X7_CTR_BC_13\" \"X8_TGF_BC_14\" \"X9_IR_BC_15\" \n\n\nFirst we can remove the unwanted X from the start of each column name\n\n## Remove the X from each name\n\ncols &lt;- str_remove_all(cols, \"X\")\ncols\n\n[1] \"1_CTR_BC_2\"  \"2_TGF_BC_4\"  \"3_IR_BC_5\"   \"4_CTR_BC_6\"  \"5_TGF_BC_7\" \n[6] \"6_IR_BC_12\"  \"7_CTR_BC_13\" \"8_TGF_BC_14\" \"9_IR_BC_15\" \n\n\nThe column names consists of various letters separated by a _. Some of these can be used to form the conditions and replicate information. The BC and number at the each of the name is not actually useful analysis, so we will remove in the next step.\n\nmeta &lt;- data.frame(Run = cols) %&gt;% \n  separate(Run, into = c(\"Sample Number\", \"condition\", \"BC\", \"X\"),remove = FALSE)\nmeta\n\n          Run Sample Number condition BC  X\n1  1_CTR_BC_2             1       CTR BC  2\n2  2_TGF_BC_4             2       TGF BC  4\n3   3_IR_BC_5             3        IR BC  5\n4  4_CTR_BC_6             4       CTR BC  6\n5  5_TGF_BC_7             5       TGF BC  7\n6  6_IR_BC_12             6        IR BC 12\n7 7_CTR_BC_13             7       CTR BC 13\n8 8_TGF_BC_14             8       TGF BC 14\n9  9_IR_BC_15             9        IR BC 15\n\n\nNow we keep the columns we need, and add the replicate numbers using the rep function to repeat a sequence 1,2,3 three times. The shorte\n\nmeta &lt;- select(meta, Run, condition) %&gt;% \n  mutate(Replicate = rep(c(1,2,3), 3)) %&gt;% \n  mutate(Name = paste(condition, Replicate, sep = \"_\")) %&gt;% \n  mutate(Treated = ifelse(condition == \"CTR\", \"N\", \"Y\"))\n\n:::"
  },
  {
    "objectID": "training/bulk-rnaseq_1/index.html#principal-components-analysis-pca",
    "href": "training/bulk-rnaseq_1/index.html#principal-components-analysis-pca",
    "title": "Introduction to RNA-Seq - Part 1",
    "section": "Principal components Analysis (PCA)",
    "text": "Principal components Analysis (PCA)\n\n\n\n\n\n\nNote\n\n\n\nSee here for a nice explanation of PCA\n\nhttps://www.youtube.com/watch?v=0Jp4gsfOLMs\n\n\n\nThe (Principal Components Analysis) PCA plot, shows the samples in the 2D plane spanned by their first two principal components. A PCA is an example of an unsupervised analysis, where we don‚Äôt need to specify the groups. Each point in the plot is a biological sample in your dataset, and crucially the points can be coloured and labeled according to your metadata. If your experiment is well-controlled and has worked well, what we hope to see is that the greatest sources of variation in the data correspond to the treatments/groups we are interested in. In other words, the points on the plot should separate according to biological conditions.\nIt is also an incredibly useful tool for quality control and checking for outliers, and DESeq2 has a convenient plotPCA function for making the PCA plot, which makes use of the ggplot2 graphics package.\n\nplotPCA(vsd,intgroup=\"Treated\")\n\nusing ntop=500 top features by variance\n\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\n‚Ñπ Please use tidy evaluation idioms with `aes()`.\n‚Ñπ See also `vignette(\"ggplot2-in-packages\")` for more information.\n‚Ñπ The deprecated feature was likely used in the DESeq2 package.\n  Please report the issue to the authors.\n\n\n\n\n\n\n\n\n\nThere is also an option to return the values used in the plot for further exploration and customisation. The ggplot2 package is a natural choice for plotting.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nplotPCA(vsd,intgroup=\"Treated\",returnData = TRUE) %&gt;% \n  ggplot(aes(x = PC1, y = PC2,col=group)) + geom_point(size=3)\n\nusing ntop=500 top features by variance\n\n\n\n\n\n\n\n\n\nThe PCA shown above is coloured according to the Treated status of each sample. An interpretation of this plot would be that samples separate cleanly based on Treated on the y-axis (PC2), with the non-treated samples (coloured red) having y coordinates around 10 and treated samples (coloured blue) having y coordinate around -5.\nHowever, the PCA method definition means that main source of variance cannot be explained by the Treated variable. Along the x-axis (PC1) we see three distinct groups. You would hope that these correspond to the condition, and we will assess this in the following exercise."
  },
  {
    "objectID": "training/bulk-rnaseq_1/index.html#exercise-1",
    "href": "training/bulk-rnaseq_1/index.html#exercise-1",
    "title": "Introduction to RNA-Seq - Part 1",
    "section": "Exercise",
    "text": "Exercise\n\n\nIs the plotPCA plot based on all genes in the dataset? How can we change how many genes are used for the PCA analysis? Does this significantly change the plot? (HINT: check the documentation for the plotPCA function.)\nVerify that the samples are separated based on the condition.\nWhat problems can you see with the metadata?\nCan you label the identify of each sample? Look for help on geom_textif you haven‚Äôt used it before\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe plotPCA function has an argument ntop which is used to specify how many genes are used to perform the PCA - so it is based on a limited set of genes in the data. The genes used are those with the highest variance.\nTo make a PCA based on condition we need to alter the intgroup argument\n\nplotPCA(vsd,intgroup=\"condition\",returnData = TRUE) %&gt;% \n  ggplot(aes(x = PC1, y = PC2,col=condition)) + geom_point()\n\nusing ntop=500 top features by variance\n\n\n\n\n\n\n\n\n\nThis almost shows what we expect, but there are far too many colours used in the plot. This is because of discrepancies in the notation used for condition.\nIn order for geom_text to work you need to include an aesthetic mapping label in the initial ggplot call.\n\nplotPCA(vsd,intgroup=\"condition\",returnData = TRUE) %&gt;% \n  ggplot(aes(x = PC1, y = PC2,col=condition, label=Name)) + geom_point() + geom_text()\n\nusing ntop=500 top features by variance\n\n\n\n\n\n\n\n\n\nHowever, the placement of the labels is sometimes not very satisfactory. Better placement can be achieved by using the ggrepel package. The geom_text_repel function can then be used instead of geom_text.\n\nif(!require(ggrepel)) install.packages(\"ggrepel\")\n\nLoading required package: ggrepel\n\nplotPCA(vsd,intgroup=\"condition\",returnData = TRUE) %&gt;% \n  ggplot(aes(x = PC1, y = PC2,col=condition, label=Name)) + geom_point() + ggrepel::geom_text_repel()\n\nusing ntop=500 top features by variance"
  },
  {
    "objectID": "training/bulk-rnaseq_1/index.html#note-about-batch-effects",
    "href": "training/bulk-rnaseq_1/index.html#note-about-batch-effects",
    "title": "Introduction to RNA-Seq - Part 1",
    "section": "Note about batch effects",
    "text": "Note about batch effects\nIn our unsupervised analysis we should see that the main source of variation is due to biological effects, and not technical variation such as when the libraries were sequenced. If we do observe high technical variation in our data, it is not a complete disaster provided that we have designed our experiment properly. In particular the sva Bioconductor package can correct for batch effects provided that representatives of the groups of interest appear in each batch. Alternatively, the batch or confounding factor may be incorporated into the differential expression analysis.\nBelow is an example of a PCA that might potentially worrying:-"
  },
  {
    "objectID": "training/bulk-rnaseq_1/index.html#correcting-the-sample-information",
    "href": "training/bulk-rnaseq_1/index.html#correcting-the-sample-information",
    "title": "Introduction to RNA-Seq - Part 1",
    "section": "Correcting the sample information",
    "text": "Correcting the sample information\nThe person creating the sample sheet has been inconsistent about the way that values of Treated have been entered into the metadata. Such errors can be annoying when labeling plots, but have more serious consequences when attempting to fit statistical models to the data.\nHere are a set of commands to create an updated sample sheet using the stringr package (part of tidyverse). We write a new file rather than over-writing the existing one.\n\nlibrary(stringr)\nlibrary(dplyr)\nsampleInfo %&gt;% \n  mutate(condition = str_to_upper(condition)) %&gt;% \n  readr::write_tsv(file=\"meta_data/sampleInfo_corrected.txt\")\n\n\nExercise\n\nRe-create the DESeqDataset object to include the corrected sample information\nRe-run the plotPCA function on the new data and verify that the sample groups now look correct\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe only actually need to change the code to read the meta data - the rest of the code is identical.\n\nsampleinfo_corrected &lt;- read.delim(\"meta_data/sampleInfo_corrected.txt\")\ndds &lt;- DESeqDataSetFromMatrix(counts, \n                                colData = sampleinfo_corrected,\n                                design = ~condition, tidy=TRUE)\n\nWarning in DESeqDataSet(se, design = design, ignoreRank): some variables in\ndesign formula are characters, converting to factors\n\nvsd &lt;- vst(dds)\n\nplotPCA(vsd, intgroup = \"condition\")\n\n\n\n\n\n\n\n\nNotice that the only that has changed in the PCA is the colouring of the points. As our counts are unchanged, the coordinates on the PCA remain the same.\n\n\n\nThe PCA produced when we used the corrected meta data gives us some confidence that our experiment has worked. Although it is tempting, we shouldn‚Äôt blindly correct errors in our meta data based on what we see on a PCA. Some detective work is required to make sure that the sample labeling is truly a mistake. Perhaps contact the lab personnel who processed the samples and review their lab notebook/LIMS records for the sample‚Äôs journey: RNA extraction -&gt; library preparation -&gt; index assignment.\n\n\n\n\n\n\n\n\n\nSample swaps can also happen during the indexing/barcoding step of library preparation. The sample might have physically clustered with the intended group, but the sequencing machine labeled it with the wrong index.\nPCA is a critical diagnostic and exploratory tool that informs and guides the inferential analysis, not just a simple visualization technique. However, it is a descriptive technique, not an inferential one. In other words, it cannot actually be used to prove anything about your data such as a biological hypothesis. This will come in the next section when we look at differential expression.\nAnyway, now that we are happy with the pre-processing we will save our DESeq object to disk so we can start from this point\n\ndir.create(\"Robjects\", showWarnings = FALSE)\nsaveRDS(dds, \"Robjects/dds.rds\")\n\n\n\nWhat about tSNE and UMAP? Aren‚Äôt these the modern methods for visualising RNA-seq\nSort of. These are methods recommended for single-cell RNA-seq which are much sparser (have many 0 counts), more complex, and contains distinct cell types that often have non-linear relationships. For bulk RNA-seq though good old PCA is perfectly adequate as an exploratory method for visualising variation in your data\nIf you wish to explore PCA in a bit more detail, expand the next section.\n\n\n\n\n\n\nGreater control over PCA\n\n\n\n\n\nThe plotPCA function is perfectly serviceable and gives a good overview of your data. However, sometimes you might want a bit more flexibility. Whilst not wishing to cover PCA in great depth we can go over the main steps.\nFirstly get the transformed VST values as a matrix\n\nvst_values &lt;- assay(vsd)\nhead(vst_values)\n\n                1_CTR_BC_2 2_TGF_BC_4 3_IR_BC_5 4_CTR_BC_6 5_TGF_BC_7\nENSG00000000003  10.611890  10.428009 10.617695  10.660566  10.445435\nENSG00000000005   5.107359   5.107359  5.107359   5.107359   5.107359\nENSG00000000419  10.773025  10.617265 11.075708  10.743334  10.887778\nENSG00000000457   9.509690   9.202704  9.532294   9.520157   9.444276\nENSG00000000460   8.219870   8.352648  7.590207   8.067853   8.264558\nENSG00000000938   5.855476   5.610337  5.957485   5.677762   5.346678\n                6_IR_BC_12 7_CTR_BC_13 8_TGF_BC_14 9_IR_BC_15\nENSG00000000003  10.582573   10.737816   10.404334  10.414804\nENSG00000000005   5.107359    5.107359    5.107359   5.355472\nENSG00000000419  11.054746   10.937878   10.512279  11.030753\nENSG00000000457   9.429903    9.746221    9.285840   9.469935\nENSG00000000460   7.553175    8.337201    8.396371   7.589312\nENSG00000000938   5.480466    6.181764    5.581574   5.803230\n\n\nWe now select the ‚Äúmost variable‚Äù features in the dataset by firstly using the rowVars function to calculate the variance of each row (‚Äúgene‚Äù) and ordering the result from largest to smallest. The result of this are row indices from the most variable to least variable. The ntop (e.g.¬†500) most variable genes can then be created by subsetting the row_var_order vector.\n\nntop &lt;- 500\n\nrow_var_order &lt;- rowVars(vst_values) %&gt;% order(decreasing = TRUE)\n\n\nrow_var_order[1]\n\n[1] 6715\n\n### Looks to be highly variable\nassay(vsd)[row_var_order[1],]\n\n 1_CTR_BC_2  2_TGF_BC_4   3_IR_BC_5  4_CTR_BC_6  5_TGF_BC_7  6_IR_BC_12 \n  10.527271   14.796726    9.922513   10.589045   14.928894   10.834244 \n7_CTR_BC_13 8_TGF_BC_14  9_IR_BC_15 \n  11.368250   14.375299   10.114217 \n\n## not variable at all\nassay(vsd)[row_var_order[length(row_var_order)],]\n\n 1_CTR_BC_2  2_TGF_BC_4   3_IR_BC_5  4_CTR_BC_6  5_TGF_BC_7  6_IR_BC_12 \n   5.107359    5.107359    5.107359    5.107359    5.107359    5.107359 \n7_CTR_BC_13 8_TGF_BC_14  9_IR_BC_15 \n   5.107359    5.107359    5.107359 \n\n## Get the 'ntop' most variable rows\nmost_var_rows &lt;- row_var_order[1:ntop]\n\nThe R function prcomp is used to perform the PCA but we need to ‚Äútranspose‚Äù the data (using the t function) otherwise the function will attempt to do PCA on the genes rather than our samples. Scaling is also recommended\n\npca_results &lt;- prcomp(t(vst_values[most_var_rows,]),scale = TRUE)\n\nnames(pca_results)\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n\nWithout digging too much into the meaning of the resulting object, the % of variance explained (as seen on the plotPCA output) is calculated using the formula pca_results$sdev^2 / sum(pca_results$sdev^2)*100. These can then be plotted on a bar plot and due to the nature of the PCA method the variance should decrease rapidly.\n\nhttps://scienceparkstudygroup.github.io/rna-seq-lesson/05-descriptive-plots/index.html\n\n\nvar_explained &lt;- pca_results$sdev^2 / sum(pca_results$sdev^2)*100\n\ndata.frame(PC = 1:9, var_explained) %&gt;% \n  ggplot(aes(y = var_explained, x =PC)) + geom_col(fill=\"steelblue\")\n\n\n\n\n\n\n\n\nThe values of the principal components can be found in the conveniently-named (!) x slot of the output. The dimensions of this matrix is 9 x 9 as we have 9 biological samples and hence 9 Principal Components\n\npca_results$x\n\n                    PC1       PC2        PC3       PC4        PC5         PC6\n1_CTR_BC_2   -4.6468927 10.989092 -6.3812748 -1.042373  0.1213624  1.65953033\n2_TGF_BC_4  -20.5235074 -8.365185  0.3891126 -1.496022  1.0603908  2.40505458\n3_IR_BC_5    22.0295945 -4.381020 -1.8413407 -1.325310 -8.0046029  4.10182976\n4_CTR_BC_6   -3.9922398 11.146824 -6.0463553 -3.144462  0.1002104 -4.40687268\n5_TGF_BC_7  -18.5278032 -7.460858  4.6149728 -0.903531 -4.6083436 -5.81192771\n6_IR_BC_12   20.0156744 -3.158754  5.2006785 -7.561357  5.5530510  0.03124602\n7_CTR_BC_13  -0.4709555 14.521663  9.9477207  4.780599 -0.3422440  1.77850878\n8_TGF_BC_14 -16.4022721 -6.462752 -2.9120981  3.155782  3.4799806  4.20899313\n9_IR_BC_15   22.5184017 -6.829011 -2.9714155  7.536674  2.6401953 -3.96636221\n                   PC7        PC8          PC9\n1_CTR_BC_2   5.4530609  4.0388908 1.151856e-14\n2_TGF_BC_4  -5.1314480  4.6771990 5.186823e-15\n3_IR_BC_5   -0.9596799 -1.0912456 1.425249e-14\n4_CTR_BC_6  -4.5084323 -3.0518958 1.183775e-14\n5_TGF_BC_7   3.4990387 -0.4737049 5.900662e-15\n6_IR_BC_12   1.4423047 -0.4688147 1.668110e-14\n7_CTR_BC_13 -1.1907042 -0.1826762 1.331574e-14\n8_TGF_BC_14  1.8721518 -5.2613527 7.879114e-15\n9_IR_BC_15  -0.4762919  1.8136002 1.364187e-14\n\n\nThese PC values can be combined with the (corrected) sample information and used to make a familiar plot\n\npca_plot &lt;- pca_results$x %&gt;% \n  bind_cols(sampleinfo_corrected) %&gt;% \n  ggplot(aes(x = PC1, y = PC2, col = condition)) + geom_point()\n\npca_plot\n\n\n\n\n\n\n\n\nThe percentage of variance explained can be added as a label on the x- and y- axes.\n\nxlabel &lt;- paste0(\"PC1(\", round(var_explained[1],1), \"%)\")\nxlabel\n\n[1] \"PC1(61.6%)\"\n\nylabel &lt;- paste0(\"PC2(\", round(var_explained[2],1), \"%)\")\nylabel\n\n[1] \"PC2(17.5%)\"\n\npca_plot + xlab(xlabel) + ylab(xlabel)\n\n\n\n\n\n\n\n\nIf your data are complex and involve multiple experimental factors and conditions you may wish to expand the PCA beyond the first two components. This may show more subtle relationships between your samples. Here we show PC1 and PC3.\n\npca_results$x %&gt;% \n  bind_cols(sampleinfo_corrected) %&gt;% \n  ggplot(aes(x = PC1, y = PC3, col = condition)) + geom_point()"
  },
  {
    "objectID": "training/bulk-rnaseq_1/index.html#solution-2",
    "href": "training/bulk-rnaseq_1/index.html#solution-2",
    "title": "Introduction to RNA-Seq - Part 1",
    "section": "Solution",
    "text": "Solution\n\nsampleinfo_corrected &lt;- read.delim(\"meta_data/sampleInfo_corrected.txt\")\ndds &lt;- DESeqDataSetFromMatrix(counts, \n                                colData = sampleinfo_corrected,\n                                design = ~condition, tidy=TRUE)\n\nWarning in DESeqDataSet(se, design = design, ignoreRank): some variables in\ndesign formula are characters, converting to factors\n\ndds\n\nclass: DESeqDataSet \ndim: 57914 9 \nmetadata(1): version\nassays(1): counts\nrownames(57914): ENSG00000000003 ENSG00000000005 ... ENSG00000284747\n  ENSG00000284748\nrowData names(0):\ncolnames(9): X1_CTR_BC_2 X2_TGF_BC_4 ... X8_TGF_BC_14 X9_IR_BC_15\ncolData names(5): Run condition Name Replicate Treated\n\nvsd &lt;- vst(dds)\n\nplotPCA(vsd, intgroup = \"condition\")\n\nusing ntop=500 top features by variance\n\n\n\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "training/bulk-rnaseq_2/index.html",
    "href": "training/bulk-rnaseq_2/index.html",
    "title": "Introduction to RNA-Seq - Part 2",
    "section": "",
    "text": "This section follows on from Part 1 where we saw how to import raw RNA-seq counts into DESeq2 and perform some quality assessment. Several packages are required, which can be downloaded with this code:-\n\nsource(\"https://raw.githubusercontent.com/markdunning/markdunning.github.com/refs/heads/master/files/training/bulk_rnaseq/install_bioc_packages.R\")\n\nThe following will also assume you have created a DESeq2 object in a folder called Robjects in your working directory. This can be downloaded with the following.\n\ndir.create(\"Robjects/\",showWarnings = FALSE)\ndownload.file(\"https://github.com/markdunning/markdunning.github.com/raw/refs/heads/master/files/training/bulk_rnaseq/dds.rds\",destfile = \"Robjects/dds.rds\")"
  },
  {
    "objectID": "training/bulk-rnaseq_2/index.html#quick-start",
    "href": "training/bulk-rnaseq_2/index.html#quick-start",
    "title": "Introduction to RNA-Seq - Part 2",
    "section": "",
    "text": "This section follows on from Part 1 where we saw how to import raw RNA-seq counts into DESeq2 and perform some quality assessment. Several packages are required, which can be downloaded with this code:-\n\nsource(\"https://raw.githubusercontent.com/markdunning/markdunning.github.com/refs/heads/master/files/training/bulk_rnaseq/install_bioc_packages.R\")\n\nThe following will also assume you have created a DESeq2 object in a folder called Robjects in your working directory. This can be downloaded with the following.\n\ndir.create(\"Robjects/\",showWarnings = FALSE)\ndownload.file(\"https://github.com/markdunning/markdunning.github.com/raw/refs/heads/master/files/training/bulk_rnaseq/dds.rds\",destfile = \"Robjects/dds.rds\")"
  },
  {
    "objectID": "training/bulk-rnaseq_2/index.html#recap-of-pre-processing",
    "href": "training/bulk-rnaseq_2/index.html#recap-of-pre-processing",
    "title": "Introduction to RNA-Seq - Part 2",
    "section": "Recap of pre-processing",
    "text": "Recap of pre-processing\nThe previous section walked-through the pre-processing and transformation of the count data. Here, for completeness, we list the minimal steps required to process the data prior to differential expression analysis.\nNote that although we spent some time looking at the quality of our data , these steps are not strictly necessary prior to performing differential expression so are not shown here for the sake of brevity. Remember, DESeq2 requires raw counts so the vst transformation is not shown as part of this basic protocol.\n\nlibrary(DESeq2)\n\ncount_file &lt;- \"raw_counts/raw_counts_matrix.tsv\"\ncounts &lt;- read.delim(count_file)\n\n## Step needed for this data to tidy the column names\ncolnames(counts)[-1] &lt;- stringr::str_remove_all(colnames(counts)[-1], \"X\")\n\nsampleinfo_corrected &lt;- read.delim(\"meta_data/sampleInfo_corrected.txt\")\ndds &lt;- DESeqDataSetFromMatrix(counts, \n                                colData = sampleinfo_corrected,\n                                design = ~condition, tidy=TRUE)\n\nsaveRDS(dds, file=\"Robjects/dds.rds\")\n\nWe will be using these raw counts throughout the workshop and transforming them using methods in the DESeq2 package. If you want to know about alternative methods for count normalisation they are covered on this page."
  },
  {
    "objectID": "training/bulk-rnaseq_2/index.html#exercise",
    "href": "training/bulk-rnaseq_2/index.html#exercise",
    "title": "Introduction to RNA-Seq - Part 2",
    "section": "Exercise",
    "text": "Exercise\n\nRe-run the analysis to find differentially-expressed genes between the IR treated samples and CTR\nWrite a csv file that contains results for the genes that have an adjusted p-value less than 0.05 and a log2 fold change more than 1, or less than -1 in the contrast of TGF vs CTRL.\nUse the plotCounts function to visually-inspect the most statistically-significant gene identified\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n## Like to run the `results` function without `tidy=TRUE` to check everything is working\nresults(de_condition, contrast = c(\"condition\", \"IR\", \"CTR\"))\n\nlog2 fold change (MLE): condition IR vs CTR \nWald test p-value: condition IR vs CTR \nDataFrame with 57914 rows and 6 columns\n                   baseMean log2FoldChange     lfcSE      stat      pvalue\n                  &lt;numeric&gt;      &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;   &lt;numeric&gt;\nENSG00000000003 1430.562846      -0.136193 0.0817196 -1.666593 9.55953e-02\nENSG00000000005    0.113566       1.143864 4.0804559  0.280328 7.79226e-01\nENSG00000000419 1790.537536       0.241934 0.1006649  2.403361 1.62451e-02\nENSG00000000457  640.692302      -0.128599 0.1140639 -1.127428 2.59561e-01\nENSG00000000460  206.179026      -0.850453 0.1675739 -5.075092 3.87308e-07\n...                     ...            ...       ...       ...         ...\nENSG00000284744    8.307038       0.843943  0.772014  1.093171    0.274319\nENSG00000284745    0.000000             NA        NA        NA          NA\nENSG00000284746    0.101097      -0.779681  4.080456 -0.191077    0.848465\nENSG00000284747   28.783710      -0.391963  0.456290 -0.859021    0.390329\nENSG00000284748    0.548323       2.942851  3.977407  0.739892    0.459366\n                       padj\n                  &lt;numeric&gt;\nENSG00000000003 2.18208e-01\nENSG00000000005          NA\nENSG00000000419 5.32619e-02\nENSG00000000457 4.47487e-01\nENSG00000000460 3.99611e-06\n...                     ...\nENSG00000284744          NA\nENSG00000284745          NA\nENSG00000284746          NA\nENSG00000284747    0.586086\nENSG00000284748          NA\n\nresults(de_condition, contrast = c(\"condition\", \"IR\", \"CTR\"), tidy = TRUE) %&gt;% \n  arrange(padj) %&gt;% \n  filter(padj &lt; 0.05, abs(log2FoldChange) &gt; 1) %&gt;% \n  readr::write_csv(\"de_analysis/condition_IR_vs_CTR_sig_genes.csv\")"
  },
  {
    "objectID": "training/bulk-rnaseq_2/index.html#exercise-1",
    "href": "training/bulk-rnaseq_2/index.html#exercise-1",
    "title": "Introduction to RNA-Seq - Part 2",
    "section": "Exercise",
    "text": "Exercise\n\nThe publication gives examples of COL1A1, COL1A2 and COL3A1 as genes that are up-regulated in TGF-treated samples vs controls (Figure 6C). Use your data to verify this by\n\nextracting their p-values\nplotting the counts for these genes\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe already have the differential expression statistics for the contrast TGF vs CTR and have just added gene names (SYMBOL) into the results_annotated. We can therefore filter our results to rows that match any of these genes of interest.\n\ngenes_of_interest &lt;- c(\"COL1A1\", \"COL1A2\", \"COL3A1\")\n\n## To make the output a bit cleaner I will select a few columns of interest. \nfilter(results_annotated, SYMBOL %in% genes_of_interest) %&gt;% \n  dplyr::select(log2FoldChange, padj, SYMBOL)\n\n  log2FoldChange         padj SYMBOL\n1      1.4077294 4.902831e-44 COL1A1\n2      0.4442566 6.458329e-08 COL1A2\n3      0.3552021 9.398496e-05 COL3A1\n\n\nIndeed they all are significant with a positive fold-change indicating higher expression in TGF-treated samples. Plotting the counts for genes requires a bit more thought, as the following does not work as we might hope\n\nplotCounts(dds, \"COL1A1\", intgroup = \"condition\")\n\n`Error in counts(dds, normalized = normalized, replaced = replaced)[gene,  : \n  subscript out of bounds`\n\nThe error is not particularly helpful, but it basically means that it cannot find counts for COL1A1 in the dds object. Although we have added gene names to the analysis, they only appear in our results table and the gene counts are still referred to by ensembl IDS.\n\nens_ids &lt;- filter(results_annotated, SYMBOL %in% genes_of_interest) %&gt;% \n  pull(row)\n\nens_ids\n\n[1] \"ENSG00000108821\" \"ENSG00000164692\" \"ENSG00000168542\"\n\n\nThis code works, but is a bit clunky. N.B. par(mfrow=c(1,3)) is the base R plotting way of making a panel with one row and three columns\n\npar(mfrow=c(1,3))\n\nplotCounts(dds, ens_ids[1], intgroup = \"condition\")\nplotCounts(dds, ens_ids[2], intgroup = \"condition\")\nplotCounts(dds, ens_ids[3], intgroup = \"condition\")\n\n\n\n\n\n\n\n\nTo do a nicer job we can use ggplot2 after some data manipulation. The first step is to get normalized counts for all genes after explicitly using the estimateSizeFactors. We don‚Äôt normally need to use this function as it is part of the DESeq workflow.\nThe results is a data frame with ensembl IDs in the rows.\n\ndds &lt;- estimateSizeFactors(dds)\nnorm_counts &lt;- counts(dds, normalized = TRUE)\nhead(norm_counts)\n\n                 1_CTR_BC_2  2_TGF_BC_4  3_IR_BC_5  4_CTR_BC_6   5_TGF_BC_7\nENSG00000000003 1496.753711 1309.582900 1503.06012 1550.428865 1326.3138781\nENSG00000000005    0.000000    0.000000    0.00000    0.000000    0.0000000\nENSG00000000419 1681.596633 1502.591885 2089.94798 1645.965855 1826.4150250\nENSG00000000457  661.642869  522.309405  673.12901  666.939177  629.4048655\nENSG00000000460  233.186455  261.577968  129.92174  203.812245  242.4444723\nENSG00000000938    9.479124    4.232653   12.32016    5.459257    0.9507626\n                 6_IR_BC_12 7_CTR_BC_13 8_TGF_BC_14  9_IR_BC_15\nENSG00000000003 1465.288919  1639.42512 1287.173576 1297.038521\nENSG00000000005    0.000000     0.00000    0.000000    1.022095\nENSG00000000419 2058.823670  1893.34636 1392.402365 2023.748047\nENSG00000000457  622.515941   791.26495  557.148855  641.875643\nENSG00000000460  125.198737   258.13570  271.527857  129.806062\nENSG00000000938    2.318495    20.01869    3.758171    8.176760\n\n\nWe can filter these counts by the ensembl IDs we identified for these genes; provided that the ensembl ID appears as a column name.\n\nnorm_counts %&gt;% \n  data.frame() %&gt;% \n  tibble:::rownames_to_column(\"row\") %&gt;% \n  filter(row %in% ens_ids)\n\n              row X1_CTR_BC_2 X2_TGF_BC_4 X3_IR_BC_5 X4_CTR_BC_6 X5_TGF_BC_7\n1 ENSG00000108821    250216.6    646241.5  158906.61   241050.74    530262.2\n2 ENSG00000164692    431598.7    556886.8  344485.25   399621.22    498829.0\n3 ENSG00000168542    100729.0    118220.5   46646.38    88590.09    109123.8\n  X6_IR_BC_12 X7_CTR_BC_13 X8_TGF_BC_14 X9_IR_BC_15\n1   159665.49    193788.27     641086.7   177323.26\n2   336490.15    354051.56     556982.6   353092.93\n3    47357.58     79414.13     116412.2    43879.56\n\n\nHowever, these are not in the tidy form that ggplot2 requires so we have to reshape the data using tidyr‚Äôs pivot_longer function. Turning the counts into a data frame introduced an ‚ÄúX‚Äù at the start of the column names, so we better remove that.\n\nnorm_counts %&gt;% \n  data.frame() %&gt;% \n  tibble:::rownames_to_column(\"row\") %&gt;% \n  filter(row %in% ens_ids) %&gt;% \n  tidyr::pivot_longer(-row, names_to = \"Run\", values_to = \"count\") %&gt;% \n  mutate(Run = stringr::str_remove_all(Run, \"X\"))\n\n# A tibble: 27 √ó 3\n   row             Run           count\n   &lt;chr&gt;           &lt;chr&gt;         &lt;dbl&gt;\n 1 ENSG00000108821 1_CTR_BC_2  250217.\n 2 ENSG00000108821 2_TGF_BC_4  646241.\n 3 ENSG00000108821 3_IR_BC_5   158907.\n 4 ENSG00000108821 4_CTR_BC_6  241051.\n 5 ENSG00000108821 5_TGF_BC_7  530262.\n 6 ENSG00000108821 6_IR_BC_12  159665.\n 7 ENSG00000108821 7_CTR_BC_13 193788.\n 8 ENSG00000108821 8_TGF_BC_14 641087.\n 9 ENSG00000108821 9_IR_BC_15  177323.\n10 ENSG00000164692 1_CTR_BC_2  431599.\n# ‚Ñπ 17 more rows\n\n\nNow we have the counts in a tidy form, but we don‚Äôt know the sample groupings. The groupings can be obtained from the colData\n\nsampleInfo &lt;- colData(dds) %&gt;% data.frame\n\nnorm_counts %&gt;% \n  data.frame() %&gt;% \n  tibble:::rownames_to_column(\"row\") %&gt;% \n  filter(row %in% ens_ids) %&gt;% \n  tidyr::pivot_longer(-row, names_to = \"Run\", values_to = \"count\") %&gt;% \n  mutate(Run = stringr::str_remove_all(Run, \"X\")) %&gt;% \n  left_join(sampleInfo, by = \"Run\")\n\n# A tibble: 27 √ó 8\n   row             Run        count condition Name  Replicate Treated sizeFactor\n   &lt;chr&gt;           &lt;chr&gt;      &lt;dbl&gt; &lt;fct&gt;     &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;\n 1 ENSG00000108821 1_CTR_BC‚Ä¶ 2.50e5 CTR       CTR_1         1 N            1.05 \n 2 ENSG00000108821 2_TGF_BC‚Ä¶ 6.46e5 TGF       TGF_1         1 Y            1.18 \n 3 ENSG00000108821 3_IR_BC_5 1.59e5 IR        IR_1          1 Y            0.893\n 4 ENSG00000108821 4_CTR_BC‚Ä¶ 2.41e5 CTR       CTR_2         2 N            1.10 \n 5 ENSG00000108821 5_TGF_BC‚Ä¶ 5.30e5 TGF       TGF_2         2 Y            1.05 \n 6 ENSG00000108821 6_IR_BC_‚Ä¶ 1.60e5 IR        IR_2          2 Y            0.863\n 7 ENSG00000108821 7_CTR_BC‚Ä¶ 1.94e5 CTR       CTR_3         3 N            0.949\n 8 ENSG00000108821 8_TGF_BC‚Ä¶ 6.41e5 TGF       TGF_3         3 Y            1.06 \n 9 ENSG00000108821 9_IR_BC_‚Ä¶ 1.77e5 IR        IR_3          3 Y            0.978\n10 ENSG00000164692 1_CTR_BC‚Ä¶ 4.32e5 CTR       CTR_1         1 N            1.05 \n# ‚Ñπ 17 more rows\n\n\nFinally, we can make the plot of count against condition\n\nlibrary(ggplot2)\nnorm_counts %&gt;% \n  data.frame() %&gt;% \n  tibble:::rownames_to_column(\"row\") %&gt;% \n  filter(row %in% ens_ids) %&gt;% \n  tidyr::pivot_longer(-row, names_to = \"Run\", values_to = \"count\") %&gt;% \n  mutate(Run = stringr::str_remove_all(Run, \"X\")) %&gt;% \n  left_join(sampleInfo, by = \"Run\") %&gt;% \n  ggplot(aes(x = condition, y = count, col = condition)) + geom_jitter(width=0.1) + facet_wrap(~row)\n\n\n\n\n\n\n\n\nBut what are the most recognisable names for these genes? We could use the results_annotated table that we already made and we could then facet using the SYMBOL which will print the gene names automatically.\n\nnorm_counts %&gt;% \n  data.frame() %&gt;% \n  tibble:::rownames_to_column(\"row\") %&gt;% \n  filter(row %in% ens_ids) %&gt;% \n  tidyr::pivot_longer(-row, names_to = \"Run\", values_to = \"count\") %&gt;% \n  mutate(Run = stringr::str_remove_all(Run, \"X\")) %&gt;% \n  left_join(sampleInfo, by = \"Run\") %&gt;% \n  left_join(results_annotated, by = \"row\") %&gt;% \n  ggplot(aes(x = condition, y = count, col = condition)) + geom_jitter(width=0.1) + facet_wrap(~SYMBOL)\n\n\n\n\n\n\n\n\nAdmittedly, that was quite a bit of effort and would have been simpler with a tidybulk approach from the start. If you were interested in lots of gene sets like genes_of_interest then it would we worth making the code into a function that could be run without having to repeat the code all the time."
  },
  {
    "objectID": "training/bulk-rnaseq_2/index.html#the-volcano-plot",
    "href": "training/bulk-rnaseq_2/index.html#the-volcano-plot",
    "title": "Introduction to RNA-Seq - Part 2",
    "section": "The volcano plot",
    "text": "The volcano plot\n\nlibrary(EnhancedVolcano)\n\nEnhancedVolcano(results_annotated, x = \"log2FoldChange\", y = \"padj\", lab = results_annotated$SYMBOL)\n\n\n\n\n\n\n\n\n\nExporting normalized counts\nThe DESeq workflow applies median of ratios normalization that accounts for differences in sequencing depth between samples. The user does not usually need to run this step. However, if you want a matrix of counts for some application outside of Bioconductor the values can be extracted from the dds object.\n\ndds &lt;- estimateSizeFactors(dds) \ncountMatrix &lt;-counts(dds, normalized=TRUE) \nhead(countMatrix)\n\n                 1_CTR_BC_2  2_TGF_BC_4  3_IR_BC_5  4_CTR_BC_6   5_TGF_BC_7\nENSG00000000003 1496.753711 1309.582900 1503.06012 1550.428865 1326.3138781\nENSG00000000005    0.000000    0.000000    0.00000    0.000000    0.0000000\nENSG00000000419 1681.596633 1502.591885 2089.94798 1645.965855 1826.4150250\nENSG00000000457  661.642869  522.309405  673.12901  666.939177  629.4048655\nENSG00000000460  233.186455  261.577968  129.92174  203.812245  242.4444723\nENSG00000000938    9.479124    4.232653   12.32016    5.459257    0.9507626\n                 6_IR_BC_12 7_CTR_BC_13 8_TGF_BC_14  9_IR_BC_15\nENSG00000000003 1465.288919  1639.42512 1287.173576 1297.038521\nENSG00000000005    0.000000     0.00000    0.000000    1.022095\nENSG00000000419 2058.823670  1893.34636 1392.402365 2023.748047\nENSG00000000457  622.515941   791.26495  557.148855  641.875643\nENSG00000000460  125.198737   258.13570  271.527857  129.806062\nENSG00000000938    2.318495    20.01869    3.758171    8.176760\n\nwrite.csv(countMatrix,file=\"normalized_counts.csv\")"
  },
  {
    "objectID": "posts/2025_10_23_10X_spatial_from_GEO/index.html",
    "href": "posts/2025_10_23_10X_spatial_from_GEO/index.html",
    "title": "Reading 10X Spatial Data from GEO into R",
    "section": "",
    "text": "Lots of spatial transcriptomics data is becoming available in R, but how do you load it into R for analysis? I will discuss an example of a dataset uploaded to Gene Expression Omnibus\n\n\nThe dataset I will be looking comes from Spatial single cell analysis of tumor microenvironment remodeling pattern in primary central nervous system lymphoma\nHere is an overview of the study.\n\nIf we are interested in tumour microenvironment we might want to explore the dataset for ourselves and test some hypotheses. The data are available in Gene Expression Onmibus (GEO), but unfortunately not in a form that we can immediately use.\n\nThe GEO entry for the dataset\n\n\n\n\nThe following packages will handle the download and re-organisation of the data\n\nif(!require(\"BiocManager\")) install.packages(\"BiocManager\")\nBiocManager::install(\"GEOquery\")\ninstall.packages(\"R.utils\")\n\nOur eventual aim will be to load the data into Seurat, so if you want to start looking at the data you can download this too.\n\ninstall.packages(\"Seurat\")\n\n\n\n\nThe GEOquery package has long been a favourite of mine for downloading data from GEO. However, don‚Äôt get too excited because it was developed during the days of microarrays and has limited functionality for sequencing datasets. The getGEO function will download some meta data about this dataset.\n\nlibrary(GEOquery)\ngeo &lt;- getGEO(\"GSE230207\")\ngeo\n\n$GSE230207_series_matrix.txt.gz\nExpressionSet (storageMode: lockedEnvironment)\nassayData: 0 features, 4 samples \n  element names: exprs \nprotocolData: none\nphenoData\n  sampleNames: GSM7192449 GSM7192450 GSM7192451 GSM7192453\n  varLabels: title geo_accession ... tissue:ch1 (43 total)\n  varMetadata: labelDescription\nfeatureData: none\nexperimentData: use 'experimentData(object)'\n  pubMedIds: 37120690 \nAnnotation: GPL18573 \n\n\nSince getGEO is also capable of downloading datasets that comprise multiple different technologies, the result is in the form a list. The particular dataset we want only have one type of data (spatial transcriptomics), so we subset the first item in the list. We can access the meta, or phenotypic data, using the pData function.\n\nmeta &lt;- pData(geo[[1]])\nmeta[,1:5]\n\n           title geo_accession                status submission_date\nGSM7192449   hot    GSM7192449 Public on May 01 2023     Apr 20 2023\nGSM7192450  cold    GSM7192450 Public on May 01 2023     Apr 20 2023\nGSM7192451   IME    GSM7192451 Public on May 01 2023     Apr 20 2023\nGSM7192453   IMS    GSM7192453 Public on May 01 2023     Apr 20 2023\n           last_update_date\nGSM7192449      May 08 2024\nGSM7192450      May 08 2024\nGSM7192451      May 08 2024\nGSM7192453      May 08 2024\n\n\nThe data we are interested have been uploaded as supplementary material, and conveniently GEOquery has a function for downloading these data. Depending on the speed of your network connection this may take a while. If the download fails, you might need to increase the timeout option in R.\n\n# You may need to un-comment the next line to increase the download timeout\n#options(timeout = 10000)\n\ngetGEOSuppFiles(\"GSE230207\")\n\nYou should now have a file called GSE230207_RAW.tar in a folder called GSE230207. This tar file is essentially a way of transferring a dataset consisting many separate files into a single download.\n\n\n\n\n\n\nNote\n\n\n\nThe name tar is short for Tape ARchive and harks back to the early days of computing.\nThe format and its associated command-line utility were originally developed for archiving files onto magnetic tape drives on Unix systems in the early days of computing.\nAlthough its original purpose was tape backup, the tar utility is now widely used on Unix-like systems (like Linux) as a general-purpose archive format (often called a tarball) to bundle multiple files and directories into a single file for distribution, backup, and transport, regardless of the storage medium.\n\n\n\nfile.exists(\"GSE230207/GSE230207_RAW.tar\")\n\n[1] TRUE\n\n\nR has a built-in function, untar for extracting all the files contained in the archive to a folder of our choosing (exdir). We‚Äôll just extract to the current working directory. After untar has finished we can list the contents of the working directory.\n\nuntar(\"GSE230207/GSE230207_RAW.tar\", exdir = \".\")\nlist.files(\"./\")\n\nYou‚Äôll notice that lots of files have appeared, and futhermore they are prefixed by the GEO IDs that we discovered using GEOquery (GSM7192449, GSM7192450, GSM7192451 and GSM7192452)\nWith list.files we can list everything that contains one of these IDs and notice that the files are named very predictably. This means for a given GEO ID we know what files to expect, which will be helpful to organise our files in a way that Seurat expects.\n\nlist.files(pattern = \"GSM7192449\")\n\n[1] \"GSM7192449_aligned_fiducials.jpg.gz\"    \n[2] \"GSM7192449_detected_tissue_image.jpg.gz\"\n[3] \"GSM7192449_hot.h5\"                      \n[4] \"GSM7192449_scalefactors_json.json.gz\"   \n[5] \"GSM7192449_tissue_hires_image.png.gz\"   \n[6] \"GSM7192449_tissue_lowres_image.png.gz\"  \n[7] \"GSM7192449_tissue_positions_list.csv.gz\"\n\n\n\nlist.files(pattern = \"GSM7192450\")\n\n[1] \"GSM7192450_aligned_fiducials.jpg.gz\"    \n[2] \"GSM7192450_cold.h5\"                     \n[3] \"GSM7192450_detected_tissue_image.jpg.gz\"\n[4] \"GSM7192450_scalefactors_json.json.gz\"   \n[5] \"GSM7192450_tissue_hires_image.png.gz\"   \n[6] \"GSM7192450_tissue_lowres_image.png.gz\"  \n[7] \"GSM7192450_tissue_positions_list.csv.gz\"\n\n\n\n\n\nOur eventual goal is to use a function called Load10X_Spatial in the Seurat package, which data to be organised and named in a very specific manner. For a sample called Sample_1 it should look like this:-\nSample_1\n‚îú‚îÄ‚îÄ filtered_feature_bc_matrix.h5/ \n‚îî‚îÄ‚îÄ spatial/                   \n    ‚îú‚îÄ‚îÄ tissue_hires_image.png\n    ‚îú‚îÄ‚îÄ tissue_lowres_image.png\n    ‚îú‚îÄ‚îÄ scalefactors_json.json\n    ‚îî‚îÄ‚îÄ tissue_positions_list.csv\n    ‚îî‚îÄ‚îÄ ... + other images files if required\n    \n\nIn other words, we need to create a separate folder for each of the four samples named according to the sample name. The ‚Äúh5‚Äù file should be placed here and all other image data should be in a folder named spatial. As the IDs created by GEO are completely arbitrary and not related to the underlying question we will rename to some relating to the sample groups. This labels for this are found in the title column of the meta data."
  },
  {
    "objectID": "posts/2025_10_23_10X_spatial_from_GEO/index.html#the-data",
    "href": "posts/2025_10_23_10X_spatial_from_GEO/index.html#the-data",
    "title": "Reading 10X Spatial Data from GEO into R",
    "section": "",
    "text": "The dataset I will be looking comes from Spatial single cell analysis of tumor microenvironment remodeling pattern in primary central nervous system lymphoma\nHere is an overview of the study.\n\nIf we are interested in tumour microenvironment we might want to explore the dataset for ourselves and test some hypotheses. The data are available in Gene Expression Onmibus (GEO), but unfortunately not in a form that we can immediately use.\n\nThe GEO entry for the dataset"
  },
  {
    "objectID": "posts/2025_10_23_10X_spatial_from_GEO/index.html#packages-required",
    "href": "posts/2025_10_23_10X_spatial_from_GEO/index.html#packages-required",
    "title": "Reading 10X Spatial Data from GEO into R",
    "section": "",
    "text": "The following packages will handle the download and re-organisation of the data\n\nif(!require(\"BiocManager\")) install.packages(\"BiocManager\")\nBiocManager::install(\"GEOquery\")\ninstall.packages(\"R.utils\")\n\nOur eventual aim will be to load the data into Seurat, so if you want to start looking at the data you can download this too.\n\ninstall.packages(\"Seurat\")"
  },
  {
    "objectID": "posts/2025_10_23_10X_spatial_from_GEO/index.html#downloading-using-geoquery",
    "href": "posts/2025_10_23_10X_spatial_from_GEO/index.html#downloading-using-geoquery",
    "title": "Reading 10X Spatial Data from GEO into R",
    "section": "",
    "text": "The GEOquery package has long been a favourite of mine for downloading data from GEO. However, don‚Äôt get too excited because it was developed during the days of microarrays and has limited functionality for sequencing datasets. The getGEO function will download some meta data about this dataset.\n\nlibrary(GEOquery)\ngeo &lt;- getGEO(\"GSE230207\")\ngeo\n\n$GSE230207_series_matrix.txt.gz\nExpressionSet (storageMode: lockedEnvironment)\nassayData: 0 features, 4 samples \n  element names: exprs \nprotocolData: none\nphenoData\n  sampleNames: GSM7192449 GSM7192450 GSM7192451 GSM7192453\n  varLabels: title geo_accession ... tissue:ch1 (43 total)\n  varMetadata: labelDescription\nfeatureData: none\nexperimentData: use 'experimentData(object)'\n  pubMedIds: 37120690 \nAnnotation: GPL18573 \n\n\nSince getGEO is also capable of downloading datasets that comprise multiple different technologies, the result is in the form a list. The particular dataset we want only have one type of data (spatial transcriptomics), so we subset the first item in the list. We can access the meta, or phenotypic data, using the pData function.\n\nmeta &lt;- pData(geo[[1]])\nmeta[,1:5]\n\n           title geo_accession                status submission_date\nGSM7192449   hot    GSM7192449 Public on May 01 2023     Apr 20 2023\nGSM7192450  cold    GSM7192450 Public on May 01 2023     Apr 20 2023\nGSM7192451   IME    GSM7192451 Public on May 01 2023     Apr 20 2023\nGSM7192453   IMS    GSM7192453 Public on May 01 2023     Apr 20 2023\n           last_update_date\nGSM7192449      May 08 2024\nGSM7192450      May 08 2024\nGSM7192451      May 08 2024\nGSM7192453      May 08 2024\n\n\nThe data we are interested have been uploaded as supplementary material, and conveniently GEOquery has a function for downloading these data. Depending on the speed of your network connection this may take a while. If the download fails, you might need to increase the timeout option in R.\n\n# You may need to un-comment the next line to increase the download timeout\n#options(timeout = 10000)\n\ngetGEOSuppFiles(\"GSE230207\")\n\nYou should now have a file called GSE230207_RAW.tar in a folder called GSE230207. This tar file is essentially a way of transferring a dataset consisting many separate files into a single download.\n\n\n\n\n\n\nNote\n\n\n\nThe name tar is short for Tape ARchive and harks back to the early days of computing.\nThe format and its associated command-line utility were originally developed for archiving files onto magnetic tape drives on Unix systems in the early days of computing.\nAlthough its original purpose was tape backup, the tar utility is now widely used on Unix-like systems (like Linux) as a general-purpose archive format (often called a tarball) to bundle multiple files and directories into a single file for distribution, backup, and transport, regardless of the storage medium.\n\n\n\nfile.exists(\"GSE230207/GSE230207_RAW.tar\")\n\n[1] TRUE\n\n\nR has a built-in function, untar for extracting all the files contained in the archive to a folder of our choosing (exdir). We‚Äôll just extract to the current working directory. After untar has finished we can list the contents of the working directory.\n\nuntar(\"GSE230207/GSE230207_RAW.tar\", exdir = \".\")\nlist.files(\"./\")\n\nYou‚Äôll notice that lots of files have appeared, and futhermore they are prefixed by the GEO IDs that we discovered using GEOquery (GSM7192449, GSM7192450, GSM7192451 and GSM7192452)\nWith list.files we can list everything that contains one of these IDs and notice that the files are named very predictably. This means for a given GEO ID we know what files to expect, which will be helpful to organise our files in a way that Seurat expects.\n\nlist.files(pattern = \"GSM7192449\")\n\n[1] \"GSM7192449_aligned_fiducials.jpg.gz\"    \n[2] \"GSM7192449_detected_tissue_image.jpg.gz\"\n[3] \"GSM7192449_hot.h5\"                      \n[4] \"GSM7192449_scalefactors_json.json.gz\"   \n[5] \"GSM7192449_tissue_hires_image.png.gz\"   \n[6] \"GSM7192449_tissue_lowres_image.png.gz\"  \n[7] \"GSM7192449_tissue_positions_list.csv.gz\"\n\n\n\nlist.files(pattern = \"GSM7192450\")\n\n[1] \"GSM7192450_aligned_fiducials.jpg.gz\"    \n[2] \"GSM7192450_cold.h5\"                     \n[3] \"GSM7192450_detected_tissue_image.jpg.gz\"\n[4] \"GSM7192450_scalefactors_json.json.gz\"   \n[5] \"GSM7192450_tissue_hires_image.png.gz\"   \n[6] \"GSM7192450_tissue_lowres_image.png.gz\"  \n[7] \"GSM7192450_tissue_positions_list.csv.gz\""
  },
  {
    "objectID": "posts/2025_10_23_10X_spatial_from_GEO/index.html#fun-fact",
    "href": "posts/2025_10_23_10X_spatial_from_GEO/index.html#fun-fact",
    "title": "Reading 10X Spatial Data from GEO into R",
    "section": "",
    "text": "The name tar is short for Tape ARchive and harks back to the early days of computing.\nThe format and its associated command-line utility were originally developed for archiving files onto magnetic tape drives on Unix systems in the early days of computing.\nAlthough its original purpose was tape backup, the tar utility is now widely used on Unix-like systems (like Linux) as a general-purpose archive format (often called a tarball) to bundle multiple files and directories into a single file for distribution, backup, and transport, regardless of the storage medium."
  },
  {
    "objectID": "posts/2025_10_23_10X_spatial_from_GEO/index.html#folder-structure-expected-by-seurat",
    "href": "posts/2025_10_23_10X_spatial_from_GEO/index.html#folder-structure-expected-by-seurat",
    "title": "Reading 10X Spatial Data from GEO into R",
    "section": "",
    "text": "Our eventual goal is to use a function called Load10X_Spatial in the Seurat package, which data to be organised and named in a very specific manner. For a sample called Sample_1 it should look like this:-\nSample_1\n‚îú‚îÄ‚îÄ filtered_feature_bc_matrix.h5/ \n‚îî‚îÄ‚îÄ spatial/                   \n    ‚îú‚îÄ‚îÄ tissue_hires_image.png\n    ‚îú‚îÄ‚îÄ tissue_lowres_image.png\n    ‚îú‚îÄ‚îÄ scalefactors_json.json\n    ‚îî‚îÄ‚îÄ tissue_positions_list.csv\n    ‚îî‚îÄ‚îÄ ... + other images files if required\n    \n\nIn other words, we need to create a separate folder for each of the four samples named according to the sample name. The ‚Äúh5‚Äù file should be placed here and all other image data should be in a folder named spatial. As the IDs created by GEO are completely arbitrary and not related to the underlying question we will rename to some relating to the sample groups. This labels for this are found in the title column of the meta data."
  },
  {
    "objectID": "training/bulk-rnaseq_2/index.html#exporting-normalized-counts",
    "href": "training/bulk-rnaseq_2/index.html#exporting-normalized-counts",
    "title": "Introduction to RNA-Seq - Part 2",
    "section": "Exporting normalized counts",
    "text": "Exporting normalized counts\nThe DESeq workflow applies median of ratios normalization that accounts for differences in sequencing depth between samples. The user does not usually need to run this step. However, if you want a matrix of counts for some application outside of Bioconductor the values can be extracted from the dds object.\n\ndds &lt;- estimateSizeFactors(dds) \ncountMatrix &lt;-counts(dds, normalized=TRUE) \nhead(countMatrix)\n\n                 1_CTR_BC_2  2_TGF_BC_4  3_IR_BC_5  4_CTR_BC_6   5_TGF_BC_7\nENSG00000000003 1496.753711 1309.582900 1503.06012 1550.428865 1326.3138781\nENSG00000000005    0.000000    0.000000    0.00000    0.000000    0.0000000\nENSG00000000419 1681.596633 1502.591885 2089.94798 1645.965855 1826.4150250\nENSG00000000457  661.642869  522.309405  673.12901  666.939177  629.4048655\nENSG00000000460  233.186455  261.577968  129.92174  203.812245  242.4444723\nENSG00000000938    9.479124    4.232653   12.32016    5.459257    0.9507626\n                 6_IR_BC_12 7_CTR_BC_13 8_TGF_BC_14  9_IR_BC_15\nENSG00000000003 1465.288919  1639.42512 1287.173576 1297.038521\nENSG00000000005    0.000000     0.00000    0.000000    1.022095\nENSG00000000419 2058.823670  1893.34636 1392.402365 2023.748047\nENSG00000000457  622.515941   791.26495  557.148855  641.875643\nENSG00000000460  125.198737   258.13570  271.527857  129.806062\nENSG00000000938    2.318495    20.01869    3.758171    8.176760\n\nwrite.csv(countMatrix,file=\"normalized_counts.csv\")\n\nLet‚Äôs wrap-up for now and continue our exploration of the differential expression in the next section. If you want to find out more about the DESeq workflow then read on."
  },
  {
    "objectID": "training/bulk-rnaseq_3/index.html",
    "href": "training/bulk-rnaseq_3/index.html",
    "title": "Introduction to RNA-Seq - Part 3",
    "section": "",
    "text": "This section follows on from Part 1 and Part 2 where we saw how to import raw RNA-seq counts into DESeq2, perform some quality assessment and then differential expression. Several packages are required, which can be downloaded with this code:-\n\nsource(\"https://raw.githubusercontent.com/markdunning/markdunning.github.com/refs/heads/master/files/training/bulk_rnaseq/install_bioc_packages.R\")\n\nThe following will also assume you have created a DESeq2 object in a folder called Robjects in your working directory. This can be downloaded with the following.\n\ndir.create(\"Robjects/\",showWarnings = FALSE)\ndownload.file(\"https://github.com/markdunning/markdunning.github.com/raw/refs/heads/master/files/training/bulk_rnaseq/dds.rds\",destfile = \"Robjects/dds.rds\")"
  },
  {
    "objectID": "training/bulk-rnaseq_3/index.html#quick-start",
    "href": "training/bulk-rnaseq_3/index.html#quick-start",
    "title": "Introduction to RNA-Seq - Part 3",
    "section": "",
    "text": "This section follows on from Part 1 and Part 2 where we saw how to import raw RNA-seq counts into DESeq2, perform some quality assessment and then differential expression. Several packages are required, which can be downloaded with this code:-\n\nsource(\"https://raw.githubusercontent.com/markdunning/markdunning.github.com/refs/heads/master/files/training/bulk_rnaseq/install_bioc_packages.R\")\n\nThe following will also assume you have created a DESeq2 object in a folder called Robjects in your working directory. This can be downloaded with the following.\n\ndir.create(\"Robjects/\",showWarnings = FALSE)\ndownload.file(\"https://github.com/markdunning/markdunning.github.com/raw/refs/heads/master/files/training/bulk_rnaseq/dds.rds\",destfile = \"Robjects/dds.rds\")"
  },
  {
    "objectID": "training/bulk-rnaseq_3/index.html#why-are-some-adjusted-p-values-na",
    "href": "training/bulk-rnaseq_3/index.html#why-are-some-adjusted-p-values-na",
    "title": "Introduction to RNA-Seq - Part 3",
    "section": "Why are some adjusted p-values ‚ÄúNA‚Äù?",
    "text": "Why are some adjusted p-values ‚ÄúNA‚Äù?\nIt turns out that DESeq2 has already done some processing on the results to exclude genes with low expression level across the dataset that it deems to be unreliable and unlikely to be differential-expressed. The genes it filters out (those that have NA adjusted p-value) tend to have a lower value of baseMean.\n\nresults_annotated %&gt;% \n  ggplot(aes(x = is.na(padj), y = baseMean)) + geom_boxplot() + scale_y_log10()\n\n\n\n\n\n\n\n\nThe core principle is that genes with very low expression (low average normalized counts) have little power to be called differentially expressed, even if their true \\(\\log_2\\) fold change is comparatively large. By testing all genes, you incur a multiple testing penalty for these low-power genes, which reduces the number of genes that can be called significant overall.\nThe procedure first uses the mean of normalized counts (baseMean) across all samples as a filtering mechanism. Genes with very low expression have high \\(p\\)-values due to low statistical power (high variance relative to the mean), regardless of their true fold change. DESeq2 determines an optimal cutoff for the baseMean that maximizes the number of genes called significant (padj &lt; \\(\\alpha\\), where \\(\\alpha\\) is usually \\(0.1\\) or \\(0.05\\)). This is done by testing a range of cutoffs and picking the one that yields the largest number of discoveries. Genes with mean counts below this optimal cutoff are removed (in practice their padj is set to NA). The remaining genes are then subjected to the Benjamini-Hochberg procedure. The benefit is a gain in statistical power. By reducing the total number of hypotheses tested, the multiple testing penalty is less severe, allowing more true positives (expressed genes with actual differential expression) to pass the adjusted \\(p\\)-value threshold.\nThe effect of filtering can be seen in the following histogram:-\n\nmutate(results_annotated, Filtered = is.na(padj)) %&gt;% \n  ggplot(aes(x = pvalue, fill = Filtered)) + geom_histogram(alpha = 0.4)\n\n\n\n\n\n\n\n\nThe filtered group (in blue) represents the low-count genes that were removed. The distribution is typically nearly uniform or even shows a slight peak towards the lower \\(p\\)-values. If these low-power genes were included, the overall flat null distribution needed for accurate FDR control would be slightly inflated near zero, leading to a harsher multiple testing penalty.\nThe non-Filtered Group (e.g., shown in red) represents the genes with sufficient expression to be tested. This is the set of \\(p\\)-values actually used for the Benjamini-Hochberg adjustment. This distribution should show a clear, desirable pattern: a strong peak near \\(p=0\\) (the true differentially expressed genes) and a flat, uniform distribution from \\(\\sim0.05\\) to \\(1\\) (the true null hypotheses).\nBy removing the ‚ÄúFiltered‚Äù genes, the adjusted \\(p\\)-values (padj) are calculated only on the well-behaved ‚ÄúNon-Filtered‚Äù set, resulting in more significant calls for the well-expressed genes.\nIf the details above seem complex, remember that DESeq2 will do these automatically without you having to think about them. If we think this behaviour isn‚Äôt desirable, or are curious to see the results without any adjustment, we can set the argument independentFiltering = FALSE in the results function."
  },
  {
    "objectID": "training/bulk-rnaseq_3/index.html#shrinking-the-log-fold-changes",
    "href": "training/bulk-rnaseq_3/index.html#shrinking-the-log-fold-changes",
    "title": "Introduction to RNA-Seq - Part 3",
    "section": "Shrinking the log fold-changes",
    "text": "Shrinking the log fold-changes\nDESeq2 provides an estimate, for each gene, of the difference in mean expression between our groups. However, in the same way that the raw pvalues can be improved by the adjustment described about, we can also get better estimates of the log fold-changes (LFC).\nWhen we made the MA-plot in the previous section we saw a fanning effect at lower expression levels. A technique called ‚Äúshrinkage‚Äù helps to address this as the ‚Äúshrunken‚Äù log fold-change is a better measure of biological magnitude for ranking genes.\nLog fold-change shrinkage, implemented in DESeq2 via the lfcShrink() function, uses a statistical technique called Bayesian shrinkage (or an Empirical Bayes approach) to address this.\nIt works by:\n\n‚ÄúBorrowing‚Äù Information: It assumes that most genes are not differentially expressed (i.e., most true LFCs are near zero).\n‚ÄúShrinking‚Äù: For genes with low counts (low confidence in the LFC), it shrinks their estimated LFC closer to zero.\nLeaving Alone: For genes with high counts (high confidence in the LFC), the shrinkage effect is minimal, leaving the raw LFC largely untouched.\n\nTo use lfcShrink we have to supply the initial run of the DESeq pipeline (saved as de_condition in our case) and pay careful attention to the number of the coefficient that we want to apply shrinkage to. Running the resultsNames function prints that names of the coefficients, so TGF_vs_CTR is numbered coefficient 3. Printing the results_final object allows us to check as name of the contrast is printed to the screen.\n\nresultsNames(de_condition)\n\n[1] \"Intercept\"            \"condition_IR_vs_CTR\"  \"condition_TGF_vs_CTR\"\n\nresults_final &lt;- lfcShrink(de_condition, coef = 3)\nresults_final\n\nlog2 fold change (MAP): condition TGF vs CTR \nWald test p-value: condition TGF vs CTR \nDataFrame with 57914 rows and 5 columns\n                   baseMean log2FoldChange     lfcSE     pvalue       padj\n                  &lt;numeric&gt;      &lt;numeric&gt; &lt;numeric&gt;  &lt;numeric&gt;  &lt;numeric&gt;\nENSG00000000003 1430.562846   -0.237256214 0.0803547 0.00164531 0.00906995\nENSG00000000005    0.113566   -0.000790077 0.2366069 0.98271709         NA\nENSG00000000419 1790.537536   -0.124931105 0.0952624 0.15063862 0.30807839\nENSG00000000457  640.692302   -0.272618872 0.1122208 0.00631933 0.02685529\nENSG00000000460  206.179026    0.115024336 0.1370301 0.30744046 0.50350287\n...                     ...            ...       ...        ...        ...\nENSG00000284744    8.307038    -0.01672418  0.227751   0.793565         NA\nENSG00000284745    0.000000             NA        NA         NA         NA\nENSG00000284746    0.101097    -0.00725327  0.236624   0.796900         NA\nENSG00000284747   28.783710    -0.06640128  0.219111   0.507490   0.689074\nENSG00000284748    0.548323     0.00647730  0.236572   0.830517         NA\n\n\nLet‚Äôs remind ourselves of the MA- plot of the raw differential expression results\n\nresults_raw &lt;- results(de_condition)\nplotMA(results_raw)\n\n\n\n\n\n\n\n\nLooking at the ‚Äúshrunken‚Äù results\n\nplotMA(results_final)\n\n\n\n\n\n\n\n\nThe shrinkage technique will only change the log\\(_2\\) fold-changes and not the adjusted or raw p-values. The purpose of shrinkage is to make the magnitude of the effect reliable for ranking genes, visualizing them in an MA plot, and filtering based on a minimum LFC threshold (e.g., Log2FoldChange &gt; 1). We can create a finalised results table by joining the annotation as before.\n\nresults_final &lt;- as.data.frame(results_final) %&gt;% \n  tibble::rownames_to_column(\"ENSEMBL\") %&gt;% \n  left_join(anno) %&gt;% \n  filter(!duplicated(ENSEMBL)) %&gt;% \n  arrange(padj)"
  },
  {
    "objectID": "training/bulk-rnaseq_3/index.html#threshold-based-gene-set-testing",
    "href": "training/bulk-rnaseq_3/index.html#threshold-based-gene-set-testing",
    "title": "Introduction to RNA-Seq - Part 3",
    "section": "Threshold-based Gene Set Testing",
    "text": "Threshold-based Gene Set Testing\nThreshold-based or Over-representation analysis is used once we have decided a list of DE genes that we are happy with. The analysis then looks for gene sets that are over-represented, or occur more than we would expect by chance.\nclusterProfiler is a Bioconductor package for pathways and downstream analysis and it‚Äôs main advantage is that it provides some nice visualisation methods.\nThe function for over-representation analysis on Gene Ontologies is enrichGO which requires the IDs of genes found to be DE (sigGenes) and the IDs of all genes in the dataset (universe). It uses the org.Hs.eg.db package to map between gene names and biological pathways.\n\n\n\n\n\n\nNote\n\n\n\nUnlike the DESeq workflow in the previous section, which reports all genes regardless of their significance, enrichGO only reports significant pathways based on the qvalueCutoff and pvalueCutoff arguments. If you want to see all the results regardless of significance you can set these values to 1.\nThe p.adjust and qvalue columns refers to two separate but related methods for multiple testing. p.adjust uses the the Benjamini-Hochberg (\\(\\text{BH}\\)) method (also called \\(\\text{FDR}\\) or \\(\\text{BH}\\)), which is a common and robust method for controlling the \\(\\text{FDR}\\). This is a step-up method that adjusts the raw \\(P\\)-value for each term. If you set a \\(\\text{BH}\\) cutoff of \\(0.05\\), you are stating that you expect at most 5% of your rejected null hypotheses (your significant terms) to be false discoveries (i.e., enriched by chance). Therefore the value in the \\(\\text{p.adjust}\\) column is the lowest \\(\\text{FDR}\\) you can tolerate while still declaring that specific term significant.\nThe qvalue column comes from a method developed by John Storey, often implemented via the qvalue R package, which is frequently used for high-throughput data. The \\(\\text{Q}\\)-value is an estimate of the minimum expected \\(\\text{FDR}\\) incurred when declaring a specific term significant. Unlike the \\(\\text{BH}\\) method, the \\(\\text{Q}\\)-value method estimates the proportion of true null hypotheses (\\(\\pi_0\\)) present in the data, leading to a potentially more powerful (less conservative) adjustment. If a term has a \\(\\text{Q}\\)-value of \\(0.01\\), it means that if you choose that value as your significance cutoff, you expect 1% of the terms declared significant to be false positives. Consequently the value in the \\(\\text{qvalue}\\) column is generally lower (more stringent) than the \\(\\text{p.adjust}\\) value because it uses the estimated \\(\\pi_0\\) to adjust the calculation.\n\n\n\nlibrary(clusterProfiler)\nuniverse &lt;- results_final %&gt;% pull(ENSEMBL)\nsigGenes &lt;- results_final %&gt;% \n  filter(padj &lt; 0.05) %&gt;% pull(ENSEMBL)\n\nenrich_go &lt;- enrichGO(\n  gene= sigGenes,\n  OrgDb = org.Hs.eg.db,\n  keyType = \"ENSEMBL\",\n  ont = \"BP\",\n  universe = universe,\n  qvalueCutoff = 0.05,\n  readable=TRUE\n)\n\nThe result of enrichGo can be turned into a data frame for easier interpretation.\n\nenrich_go %&gt;% data.frame %&gt;% \n  slice_head(n = 10) %&gt;% \n  select(-geneID)\n\n                   ID                                   Description GeneRatio\nGO:2001233 GO:2001233     regulation of apoptotic signaling pathway  164/4221\nGO:0045229 GO:0045229 external encapsulating structure organization  145/4221\nGO:0030198 GO:0030198             extracellular matrix organization  144/4221\nGO:0043062 GO:0043062          extracellular structure organization  144/4221\nGO:0036293 GO:0036293           response to decreased oxygen levels  140/4221\nGO:0001666 GO:0001666                           response to hypoxia  135/4221\nGO:0070482 GO:0070482                     response to oxygen levels  148/4221\nGO:0006979 GO:0006979                  response to oxidative stress  162/4221\nGO:0001503 GO:0001503                                  ossification  176/4221\nGO:0051338 GO:0051338            regulation of transferase activity  176/4221\n             BgRatio RichFactor FoldEnrichment   zScore       pvalue\nGO:2001233 398/18429  0.4120603       1.799066 8.784050 1.565104e-16\nGO:0045229 341/18429  0.4252199       1.856522 8.701630 3.876853e-16\nGO:0030198 339/18429  0.4247788       1.854596 8.656085 5.449599e-16\nGO:0043062 340/18429  0.4235294       1.849141 8.613749 7.346873e-16\nGO:0036293 328/18429  0.4268293       1.863548 8.601070 8.622993e-16\nGO:0001666 314/18429  0.4299363       1.877114 8.544404 1.403743e-15\nGO:0070482 356/18429  0.4157303       1.815090 8.464390 1.955033e-15\nGO:0006979 406/18429  0.3990148       1.742109 8.241337 7.442020e-15\nGO:0001503 458/18429  0.3842795       1.677775 8.005937 3.121776e-14\nGO:0051338 463/18429  0.3801296       1.659656 7.835427 9.807162e-14\n               p.adjust       qvalue Count\nGO:2001233 9.855460e-13 6.741479e-13   164\nGO:0045229 1.085980e-12 7.428482e-13   145\nGO:0030198 1.085980e-12 7.428482e-13   144\nGO:0043062 1.085980e-12 7.428482e-13   144\nGO:0036293 1.085980e-12 7.428482e-13   140\nGO:0001666 1.473228e-12 1.007740e-12   135\nGO:0070482 1.758692e-12 1.203007e-12   148\nGO:0006979 5.857800e-12 4.006940e-12   162\nGO:0001503 2.184203e-11 1.494071e-11   176\nGO:0051338 6.175570e-11 4.224306e-11   176\n\n\nThe output data frame is using the same approaches discussed above and the GeneRatio and BgRatio columns tell you for each row (pathway) how many genes in that pathway were found in the DE genes (GeneRatio), and in the background (BgRatio). The FoldEnrichment indicates how many more times each pathway is represented than you would expect. Somewhat reassuringly the ECM pathway is found among these most significant pathways. A dot plot can show us the most enriched pathways, their statistical significance, and the size of each.\n\nenrichplot::dotplot(enrich_go,showCategory=20)\n\n\n\n\n\n\n\n\nYou might notice that the names of some pathways, and the numbers of genes they contain are extremely similar. For example extracellular matrix organization and extracellular structure organization. This is because Gene Ontologies are not mutually exclusive and can comprise the same genes. The results can therefore give a false impression of how many biological functions are being represented amongst the results. Overlaps between gene sets can also be visualised using an ‚ÄúUpset plot‚Äù - an alternative to a venn diagram. The connected dots in the bottom panel show genes that are shared between different pathways, and the number of these genes is displayed in the bar chart.\n\nenrichplot::upsetplot(enrich_go)\n\n\n\n\n\n\n\n\nThe emapplot function is another way of identifying redundancy and clustering the enrichment results into broader biological themes. For instance, terms like ‚Äúcell proliferation,‚Äù ‚Äúcell division,‚Äù and ‚Äúmitotic nuclear division‚Äù would all cluster together, indicating they represent the single, large theme of ‚Äúcell cycle.‚Äù\nEach node in this plot represents a significantly enriched pathway or GO term. Whereas an edge (line) connects two nodes if their underlying gene sets overlap significantly (i.e., they share many common genes).\n\nenrich_go &lt;- enrichplot::pairwise_termsim(enrich_go)\nemapplot(enrich_go)\n\n\n\n\n\n\n\n\nA gene-concept network plot (cnetplot) depicts the linkages of genes and biological concepts (e.g.¬†GO terms or KEGG pathways) as a network. This plot can identify the core ‚Äúhub‚Äù genes (genes with high connectivity) that are common to multiple enriched pathways. These hub genes are often considered the most critical players driving the observed biological change.\n\ncnetplot(enrich_go)\n\n\n\n\n\n\n\n\nTheheatplot is another way of visualising pathway overlaps. In this plot, each column is a gene and a black box indicating if that gene features in a given pathway.\n\nheatplot(enrich_go, showCategory = 10)\n\n\n\n\n\n\n\n\nFinally, the treeplot function performs hierarchical clustering of enriched terms. It relies on the pairwise similarities of the enriched terms calculated by the pairwise_termsim() function, which by default usese the Jaccard‚Äôs similarity index (JC).\n\nenrich_go &lt;- enrichplot::pairwise_termsim(enrich_go)\nenrichplot::treeplot(enrich_go)\n\n\n\n\n\n\n\n\n\n‚ÄúSimplifying‚Äù the GO results\nAs noted, GO terms contain a lot of redundancy due to its hierarchical nature. We can however tackle this issue using the simplify function. In order to understand the impact we will first make a note of how many pathways are in the un-simplified output.\n\nenrich_go %&gt;% data.frame() %&gt;% nrow\n\n[1] 1486\n\n\n‚ÄúSimilarity‚Äù in this case refers to semantic similarity as implemented in the GOSemSim package. Semantic similarity, in the context of Gene Ontology (GO),cis a quantitative measure of how functionally related two GO terms are. It goes beyond simple gene sharing by assessing the terms‚Äô proximity within the GO hierarchy (the Directed Acyclic Graph or DAG). Two terms are highly similar if they share a common ancestor deep in the hierarchy, indicating a functional relationship, regardless of whether their gene lists overlap perfectly. This method ensures that terms covering the same core biological concept are grouped together.\nIf two or more GO terms are found to be highly similar (i.e., their similarity score meets or exceeds the defined cutoff), simplify() considers them redundant and removes one of them by for example picking the term with the most significant p-value.\n\n## This may take a while to run\n\nenrich_go_simplified &lt;- simplify(enrich_go)\nenrich_go_simplified %&gt;% data.frame %&gt;% slice_head(n = 10)\n\n                   ID                                  Description GeneRatio\nGO:2001233 GO:2001233    regulation of apoptotic signaling pathway  164/4221\nGO:0030198 GO:0030198            extracellular matrix organization  144/4221\nGO:0036293 GO:0036293          response to decreased oxygen levels  140/4221\nGO:0006979 GO:0006979                 response to oxidative stress  162/4221\nGO:0001503 GO:0001503                                 ossification  176/4221\nGO:0051338 GO:0051338           regulation of transferase activity  176/4221\nGO:0042327 GO:0042327       positive regulation of phosphorylation  168/4221\nGO:0036294 GO:0036294 cellular response to decreased oxygen levels   80/4221\nGO:0097193 GO:0097193        intrinsic apoptotic signaling pathway  135/4221\nGO:0032970 GO:0032970   regulation of actin filament-based process  153/4221\n             BgRatio RichFactor FoldEnrichment   zScore       pvalue\nGO:2001233 398/18429  0.4120603       1.799066 8.784050 1.565104e-16\nGO:0030198 339/18429  0.4247788       1.854596 8.656085 5.449599e-16\nGO:0036293 328/18429  0.4268293       1.863548 8.601070 8.622993e-16\nGO:0006979 406/18429  0.3990148       1.742109 8.241337 7.442020e-15\nGO:0001503 458/18429  0.3842795       1.677775 8.005937 3.121776e-14\nGO:0051338 463/18429  0.3801296       1.659656 7.835427 9.807162e-14\nGO:0042327 437/18429  0.3844394       1.678473 7.823726 1.157220e-13\nGO:0036294 162/18429  0.4938272       2.156063 8.055372 1.312428e-13\nGO:0097193 330/18429  0.4090909       1.786102 7.853963 1.461138e-13\nGO:0032970 390/18429  0.3923077       1.712826 7.755145 2.169650e-13\n               p.adjust       qvalue\nGO:2001233 9.855460e-13 6.741479e-13\nGO:0030198 1.085980e-12 7.428482e-13\nGO:0036293 1.085980e-12 7.428482e-13\nGO:0006979 5.857800e-12 4.006940e-12\nGO:0001503 2.184203e-11 1.494071e-11\nGO:0051338 6.175570e-11 4.224306e-11\nGO:0042327 6.624558e-11 4.531430e-11\nGO:0036294 6.886965e-11 4.710925e-11\nGO:0097193 7.077530e-11 4.841278e-11\nGO:0032970 9.758775e-11 6.675344e-11\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              geneID\nGO:2001233                                                                                      MMP2/CTSC/PTGS2/HGF/INHBA/EYA4/BDKRB2/NRG1/SFRP1/CLU/UNC5B/PPARG/NR4A2/CD44/TLR4/LGALS3/MAZ/PPP2R1B/ITGA6/NUPR1/PMAIP1/BMP4/LRRK2/FGF10/IL1B/DDIT3/TNFRSF12A/SERINC3/CAV1/TMBIM6/BDNF/BOK/CFLAR/IER3/TNFSF15/ATF4/CSF2/IGF1/DDX3X/FAS/SKIL/PML/ACSL5/G0S2/BBC3/KLF4/SGK3/IL1A/ICAM1/ARHGEF2/PYCR1/FGF2/NCK1/WNT16/RRM2B/CREB3L1/IVNS1ABP/NFE2L2/STRADB/ATF3/TNFSF10/HSPB1/FYN/GDNF/TRIM32/BAK1/CYLD/PTTG1IP/ACAA2/HERPUD1/SIAH2/VNN1/NOC2L/TMEM161A/VDAC2/DNAJA1/TP53/PIAS4/THBS1/TNFSF14/TAF6/HMOX1/BID/EIF2AK3/WNT5A/CTH/PTPRC/PCGF2/RBCK1/MARCHF7/HYOU1/CCAR2/RPS6KB1/MEIS3/RB1CC1/PLAUR/GCLM/SRPX/HIF1A/PAK2/MDM2/SERPINE1/TPT1/PTPN1/FADD/BCL2/URI1/TMEM14A/PPIF/USP15/TPD52L1/MYC/GHITM/EIF5A/ACKR3/TRAP1/USP47/ITPRIP/CXCL12/SYVN1/RELA/CTSH/BAX/ENO1/HMGB2/APP/PIK3CB/RPS3/NME5/SLC25A5/DDIAS/PYCARD/PPP1CA/NFATC4/KDM1A/TRAF2/BAD/TGFBR1/SLC25A6/MIF/NF1/MAPK9/MSX1/DEDD2/AKT1/BAG5/NOL3/PLEKHF1/SEPTIN4/FIS1/CD74/SOD1/SRC/SFRP2/NRP1/BCLAF1/PRELID1/RTKN2/JAK2/SCG2/LMNA/HDAC1/SNAI1/OPA1\nGO:0030198                                                                                                                  TGFBI/ELN/COL11A1/COL4A1/MMP2/ITGA8/TNFRSF11B/TGFBR3/COMP/RECK/CSGALNACT1/ADAMTS1/COL8A2/COL5A1/DPP4/COL1A1/COL4A2/SULF1/FBLN5/SMAD3/COL14A1/NID2/HMCN1/MMP11/GPM6B/MFAP4/COL4A6/LAMB1/SERPINH1/MMP1/FLRT2/NTN4/POSTN/COL16A1/PPARG/MMP14/ABI3BP/SLC39A8/ADAM10/MATN2/CYP1B1/PDGFRA/LOXL2/BMP2/RGCC/IL6/COL5A3/COL7A1/FMOD/PRDX4/ITGA2/PXDN/ADAMTS19/TNXB/CAV1/COL5A2/LAMA1/MMP3/CFLAR/ADAMTS10/ERO1A/ELF3/CTSK/LOXL4/ADAMTS6/COL4A5/COL24A1/P4HA3/FSCN1/COL1A2/PTX3/ADAMTSL1/GAS6/RUNX1/ADAMTS12/EMILIN1/COL27A1/ANTXR1/HAS2/FOXC1/THSD4/P3H4/SLC2A10/ERCC2/FOXC2/FBLN1/ADAMTS2/CREB3L1/ZNF469/COL10A1/FOXF2/COL3A1/TGFB1/B4GALT1/APBB2/CRTAP/ADAMTS9/TNFRSF1B/PAPLN/LRP1/HPSE2/AEBP1/ATXN1L/MELTF/HSD17B12/OLFML2B/MMP17/BCL3/SH3PXD2B/KAZALD1/DDR2/ECM2/BMP1/PHLDB1/POMT2/COL4A4/CCDC80/IER3IP1/CRISPLD2/COL11A2/ITGB3/NOTCH1/APP/FOXF1/RIC8A/ADAMTS7/AXIN2/ABL1/EXT1/CAV2/TGFBR1/MMP15/ITGB1/NF1/DPT/MYO1E/MAD2L2/COL12A1/SFRP2/ADAMTS13/WASHC1/APLP1/EXOC8/ADAMTS16\nGO:0036293                                                                                                                                                                                                                                              MMP2/TGFBR3/STC1/PTGS2/DPP4/INHBA/DDIT4/VCAM1/SMAD3/SFRP1/POSTN/PPARG/NR4A2/MMP14/LOC102724560/LOXL2/NPEPPS/BMP2/EPAS1/RGCC/RTN4/TFRC/PMAIP1/AJUBA/IRAK1/PTGIS/SLC11A2/VEGFA/SLC1A1/LONP1/ITGA2/CD24/BNIP3L/CAV1/TMBIM6/STC2/HIPK2/HK2/CFLAR/MYOCD/ERO1A/ATF4/KCNK2/CBS/PLAU/TEK/GATA6/PTK2B/PML/ADO/ROCK2/DDAH1/BBC3/KCNMA1/PLAT/CPEB4/CAPN2/PDK1/SLC29A1/IL1A/CAT/VASN/AQP3/MT-ND4/CITED2/MGARP/MT-ND5/SLC9A1/PGK1/ERCC2/FMN2/EPHA4/PPARA/NFE2L2/TGFB1/PRKAA1/FOSL2/LIMD1/CREBBP/VEGFB/ACAA2/SOD3/USP19/AK4/TP53/THBS1/WTIP/PTPRD/ABAT/EGLN1/HSF1/ZFP36L1/BNIP2/DNMT3A/SDHD/HYOU1/HSPG2/BNIP3/HMOX2/HSP90B1/MLST8/NOP53/HIF1A/DDR2/ADSL/MDM2/PTPN1/BCL2/MT-CYB/MYC/LIF/CXCL12/EGLN2/ADA/ENO1/NOTCH1/PIK3CB/CCNA2/MT-ND1/XRCC1/RPTOR/BAD/NF1/PPARD/AKT1/MT-CO1/NOL3/MAP3K7/MT-CO2/TIGAR/SIRT2/SRC/SRF/TLR2/PICK1/CPEB2/ACE/LMNA/PIN1/MT-ND2\nGO:0006979                                                                                                          MMP2/PTGS2/HGF/PDGFD/PTPRN/COL1A1/TRPA1/MGST1/TXNIP/TPM1/FBLN5/CD36/ARL6IP5/NR4A2/MMP14/MAP3K5/IDH1/RCAN1/GPX3/CYP1B1/SNCA/AREG/SESN3/PRDX1/PDGFRA/EPAS1/ADAM9/IL6/PLA2R1/NQO1/SLC11A2/LRRK2/PRDX4/ERMP1/SLC1A1/LONP1/PXDN/TXN/KLF2/APOD/PTGS1/SDC1/STC2/SESN2/MMP3/RBPMS/PAWR/ERO1A/PNPLA8/ATF4/PTK2B/PML/KAT2B/SPHK1/NET1/KDM6B/CYB5B/CAPN2/MET/IL1A/CAT/TREX1/PYCR1/PRR5L/OXR1/PRKAA2/AXL/MT-ND5/PRDX6/ERCC2/PRDX3/WNT16/FOSL1/RRM2B/NFE2L2/PRKAA1/BANF1/FYN/TRIM32/BAK1/ETV5/SIRPA/NCOA7/VNN1/SOD3/GSR/CCS/TMEM161A/CHUK/ERCC1/MT-ND3/APOE/TP53/MT-ND6/PRNP/MACROH2A1/AGAP3/MAPT/HMOX1/CD2AP/HSF1/HDAC6/NUDT1/ETFDH/SELENOP/PCGF2/PLK3/PPARGC1A/ATP13A2/BNIP3/ERCC6L2/HMOX2/FOXP1/GCLM/HIF1A/DDR2/PEX2/MDM2/BCL2/SCARA3/PPIF/TRAP1/ATP2A2/GPX4/RELA/ABCD1/ZNF580/APP/TRIM25/SELENOK/RPS3/GPX8/SLC8A1/PYROXD1/MT-ND1/PDK2/ABL1/PXN/EZH2/PLEKHA1/USP25/PEX13/XRCC1/CUL3/GCH1/MAPK9/TXNRD2/AKT1/MT-CO1/NAPRT/SIRT2/PDCD10/SOD1/SRC/PKD2/SETX/KEAP1/EIF2S1/FOXO4/JAK2/GPX7/PEX12\nGO:0001503            COL11A1/MMP2/EGR2/ITGA11/COMP/CSGALNACT1/STC1/PTGS2/HGF/COL1A1/VCAN/SMAD3/SEMA7A/CCN3/GPM6B/GLI2/FSTL3/SFRP1/FBN2/PPARG/MMP14/CTHRC1/LOC102724560/BAMBI/ANKH/VDR/GLI1/AREG/IL6ST/IGFBP5/XYLT1/BMP2/IL6/CCN2/MSX2/RFLNB/IGF2/TWIST2/BMP4/ALPL/EXT2/VEGFA/TWSG1/COL5A2/ASPN/PTPN11/JAG1/SRGN/SGMS2/CDH11/ATF4/CBS/CTSK/DCHS1/IGF1/PTN/ENPP1/TEK/PTK2B/COL1A2/IARS1/P2RX7/ADRB2/SMAD7/MEF2D/SUN1/BCAP29/PDLIM7/SCUBE3/RUNX1/ADAMTS12/RRAS2/ZMPSTE24/CAT/TOB1/SMO/SEMA4D/BPNT2/HSD17B4/FOXC1/FGF2/NAB2/ERCC2/FOXC2/RBPJ/TMT1A/OSR1/CREB3L1/PTK2/SMAD9/ECM1/TGFB1/FOSL2/LIMD1/HDAC7/TP53INP2/THRA/SBDS/OSR2/IGFBP2/H3-3B/GJA1/ALYREF/BCOR/SUCO/EPHA2/IFT80/GDF5/SBNO2/SNRNP200/ROGDI/LRP4/DKK1/OSTF1/CEBPD/DNAI3/CSF1/IL6R/SKI/EIF2AK3/CTNNBIP1/WNT5A/HIRA/CLEC3B/RASSF2/CLTC/MEN1/SMAD1/JUNB/HIF1A/KAZALD1/DDR2/BMP1/BCL2/JUND/SOX11/FBL/KREMEN1/TMCO1/NBR1/DNAJC13/ATRAID/DDX5/RXRB/PBX1/GLI3/ISG15/S1PR1/ATP2B1/NOTCH1/CCDC47/TENT5A/SLC8A1/ADAMTS7/AXIN2/EXT1/FBXL15/GIT1/PTGER4/SIX2/NF1/FASN/AKT1/RFLNA/SFRP2/NAB1/HNRNPC/PPP3CA/MRC2/MESD/MAPK14/RPS15/ADAR/SNAI1/TUFT1/ROR2\nGO:0051338 EREG/TENM1/IRAK3/NRG1/MMD/FBN1/TCIM/TRIB3/MAP3K5/TLR4/PDCD4/CCNG1/SNCA/CEMIP/RGCC/ADAM9/LYN/GPRC5B/TRIB2/PDGFA/NOX4/LRRK2/IL1B/TSPYL2/CD24/FLT1/GPRC5A/DUSP3/MYOCD/ARRDC4/TNFSF15/TAOK3/NEDD9/IGF1/DDX3X/PTK2B/ROBO1/KAT2B/UBE2S/BAG2/ADRB2/GAS6/UBE2C/PDGFRB/BTRC/EPHB2/AGTR1/SNX6/IRS2/CDC20/SOCS5/CDC14B/CAB39/FGF2/PLK1/PRDX3/TRAF4/STK38/DBI/SKP1/TFAP4/EPHA4/STRADB/PTK2/SYAP1/WARS1/LATS2/BANF1/MIDN/RALB/EEF1A2/LIMK1/HSPB1/PKMYT1/DVL2/CHP1/ADARB1/DUSP7/KSR1/CACUL1/PLAAT4/CHTF18/RAP2C/DUSP10/APOE/DNAJA1/SYNPO2/RGS2/HRAS/RPS2/THBS1/PABPN1/CEP43/USP44/MACROH2A1/TNFRSF10B/WNT5A/HEG1/SPINDOC/TINF2/CDKN2B/PTPRC/RASSF2/SASH1/MEN1/ERRFI1/HHEX/TAB1/DDR2/SPRED1/PAK2/PTPN1/PSRC1/SOCS4/GSKIP/AIDA/TSC2/TPD52L1/TAF7/LMO4/CDKN1A/STK11/PRKAG1/PRKAG2/AKT1S1/CCNT1/DUSP1/PRKRIP1/ITGB3/SERTAD1/PIH1D1/RPS3/PPM1F/HGS/IQGAP1/SLC8A1/RTRAF/GNAQ/ABL1/PYCARD/MST1/EZH2/VPS25/TRIB1/TPX2/XRCC1/TRAF2/CALM1/TRIM27/GHR/AKT1/BAG5/GSK3A/MAP3K7/RBL2/MAD2L2/TIGAR/MAP2K2/CORO1C/PDCD10/CD74/SRC/ITGB1BP1/RAP1A/LDB2/JAK2/ZFP36/ADAR/MAPRE3/CCNT2/CARD10/ZNF675/PILRB/ZNF16/FZR1/ETAA1\nGO:0042327                                                                                 FGF7/LEPR/EREG/ENPP2/CD36/FAM20A/TENM1/NRG1/MMD/FBN1/TCIM/TNFSF18/MAP3K5/TLR4/SNCA/AREG/C3/CEMIP/BMP2/RGCC/ADAM9/PLPP3/KITLG/ITGA6/IL6/TFRC/LYN/GPRC5B/NTRK2/PDGFA/BMP4/NOX4/VEGFA/LRRK2/FGF10/IL1B/CD24/FLT1/TXN/CAV1/PTPN11/SDCBP/TNFSF15/TAOK3/NEDD9/IGF1/DDX3X/TEK/GRB10/FAS/PTK2B/ROBO1/SPHK1/ROCK2/ADRB2/GAS6/TNIK/PDGFRB/PIK3R3/IL18/CREBL2/ARHGEF2/SEMA4D/CAB39/FGF2/KIT/PLK1/TRAF4/MYDGF/EPHA4/STRADB/PTK2/SYAP1/TGFB1/RALB/EEF1A2/NIBAN1/DVL2/DVL3/TNK2/VEGFB/IL11/DOCK7/ARL2BP/KCTD20/NPTN/LIMCH1/CACUL1/LRP4/CDK2AP1/TP53/RARRES2/LIMK2/HRAS/THBS1/YES1/PRNP/IL6R/EHD4/TNFRSF10B/TBK1/MUSK/WNT5A/HDAC6/BRAT1/PTPRC/RASSF2/FAXDC2/ITGA5/SASH1/MLST8/TAB1/PLAUR/DDR2/HAX1/PAK2/PTPN1/PSRC1/CSPG4/TPD52L1/LIF/BRAF/LMO4/CDKN1A/STK11/PRKAG1/PRKAG2/ITGB3/TNKS1BP1/DHX34/APP/FNIP2/PIH1D1/RPS3/EGF/IQGAP1/AXIN2/ABL1/EZH2/TPX2/TRAF2/CALM1/RPTOR/MIF/GHR/AKT1/MAP3K7/MAD2L2/TIGAR/MAP2K2/PDCD10/CSF3/CD74/SRC/ITGB1BP1/SFRP2/RAP1A/NRP1/MAVS/FZD7/JAK2/MAPRE3/PIN1/CARD10/PILRB/ZNF16/ETAA1/IL12A\nGO:0036294                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     STC1/PTGS2/INHBA/SFRP1/PPARG/LOC102724560/NPEPPS/EPAS1/RGCC/RTN4/PMAIP1/AJUBA/IRAK1/PTGIS/VEGFA/BNIP3L/TMBIM6/STC2/HIPK2/CFLAR/ERO1A/ATF4/KCNK2/CBS/GATA6/ADO/ROCK2/DDAH1/BBC3/CPEB4/PDK1/SLC29A1/VASN/AQP3/CITED2/MGARP/SLC9A1/PGK1/FMN2/EPHA4/NFE2L2/TGFB1/PRKAA1/ACAA2/USP19/AK4/TP53/PTPRD/EGLN1/ZFP36L1/DNMT3A/SDHD/HYOU1/BNIP3/MLST8/NOP53/HIF1A/DDR2/MDM2/PTPN1/BCL2/MYC/EGLN2/ENO1/NOTCH1/PIK3CB/CCNA2/RPTOR/BAD/PPARD/AKT1/NOL3/MAP3K7/TIGAR/SIRT2/SRC/PICK1/CPEB2/LMNA/PIN1\nGO:0097193                                                                                                                                                                                                                                                           MMP2/PTGS2/DDIT4/PERP/BDKRB2/CLU/TRIB3/ARL6IP5/CD44/MAP3K5/CYP1B1/NUPR1/PMAIP1/LRRK2/DDIT3/CD24/SERINC3/CAV1/TMBIM6/HIPK2/BOK/IER3/ERO1A/ATF4/DDX3X/SKIL/PML/BBC3/PDK1/ARHGEF2/PYCR1/HIC1/FGF2/NCK1/ERCC2/RRM2B/CREB3L1/IVNS1ABP/NFE2L2/HSPB1/FYN/EDA2R/TRIM32/BAK1/CYLD/PTTG1IP/HERPUD1/VNN1/EPHA2/NOC2L/TMEM161A/CUL5/VDAC2/TNFRSF1B/DNAJA1/TP53/PIAS4/HRAS/PHLDA3/TAF6/TNFRSF10B/BID/EIF2AK3/MARCHF7/HYOU1/BNIP3/CHAC1/CCAR2/BCL3/PLAUR/HIF1A/MDM2/TPT1/PTPN1/BCL2/URI1/PPIF/USP15/GSKIP/UACA/MYC/EIF5A/ACKR3/DDX5/TRAP1/USP47/PPP1R15A/CXCL12/CDKN1A/STK11/SYVN1/BAX/ENO1/E2F1/APP/FNIP2/PIK3CB/SELENOK/RPS3/PPM1F/NME5/DYRK2/PDK2/ABL1/DDIAS/PYCARD/NFATC4/KDM1A/TRAF2/BAD/CUL4A/CUL3/MIF/MSX1/AKT1/BAG5/NOL3/PLEKHF1/SEPTIN4/FIS1/CRIP1/PDCD10/CD74/SOD1/SRC/SFRP2/PPP2R5C/BCLAF1/RTKN2/JAK2/RPS27L/HDAC1/SNAI1/OPA1/CUL2\nGO:0032970                                                                                                                     ELN/STC1/TPM1/SMAD3/ACTA2/ARHGAP28/TENM1/GPM6B/SFRP1/LPAR1/DAAM2/KCNJ2/RND3/DSP/SEMA5A/PDGFRA/TMOD1/RGCC/RHOQ/EVL/CCN2/PDGFA/SLIT2/S100A10/ARHGAP6/CFL2/CDC42EP1/DLC1/CAV1/CD47/FHOD1/EPS8/KANK1/NEDD9/CAMK2D/CAPZA1/TACSTD2/MYH9/FSCN1/PTK2B/SSH2/LIMA1/MTPN/ROCK2/ADD3/GMFB/PDGFRB/MET/IL1A/CNN2/LMOD1/SLC9A1/IQGAP2/ARAP3/DSTN/NCK1/ARHGEF18/SPTBN5/FLNA/SYNPO2L/TGFB1/CAPZA2/LIMK1/FGF13/ARFGEF1/DVL2/DVL3/TWF2/LIMCH1/SDC4/ROCK1/PIK3CA/SCN5A/CCL26/SYNPO2/HRAS/FRMD6/PLEKHH2/DNAI3/RGS4/LRP1/CD2AP/PDE4B/MYO3B/KANK2/BRK1/WASF2/MKKS/PFN1/MLST8/SH3BP1/BIN1/HAX1/PAK2/SPTAN1/DAPK3/VASP/BRAF/ABITRAM/SLC4A2/PLEKHG2/KIRREL1/ATP2A2/CXCL12/ARF1/AMOT/S1PR1/WASF3/COTL1/ITGB3/CAPN10/RNH1/NAA20/CDC42EP5/PPM1F/CDC42EP3/SHANK1/DIXDC1/SUMO1/CYRIB/WASHC4/BAG4/ABL1/TTC8/PXN/PYCARD/RDX/TMOD3/PTGER4/ARPC3/TRIM27/TGFBR1/CACNA1C/CORO1A/CSF3/TAOK1/ARFIP1/ITGB1BP1/CDK10/RHOBTB2/NRP1/PICK1/WASHC1/TWF1/SVIL/CORO1B/ACTG1/ARAP1/ARF6/SHANK3/WHAMM/RHOBTB1/BAIAP2L1\n           Count\nGO:2001233   164\nGO:0030198   144\nGO:0036293   140\nGO:0006979   162\nGO:0001503   176\nGO:0051338   176\nGO:0042327   168\nGO:0036294    80\nGO:0097193   135\nGO:0032970   153\n\n\n\nenrich_go_simplified %&gt;% data.frame %&gt;% nrow()\n\n[1] 475\n\n\nCompare the number of rows (terms) in the original enrichment result to the simplified result. You should see a significant reduction in the total count of enriched terms, confirming that redundancy was successfully removed. We can also repeat some of the same plots as above\n\nemapplot(enrich_go_simplified)\n\n\n\n\n\n\n\n\n\ncnetplot(enrich_go_simplified)"
  },
  {
    "objectID": "training/bulk-rnaseq_3/index.html#analysis-with-clusterprofiler",
    "href": "training/bulk-rnaseq_3/index.html#analysis-with-clusterprofiler",
    "title": "Introduction to RNA-Seq - Part 3",
    "section": "Analysis with clusterProfiler",
    "text": "Analysis with clusterProfiler\nclusterProfiler is a Bioconductor package for over-representation analysis. It‚Äôs main advantage is that it provides some nice visualisation methods.\nThe main function is enrichGO which requires the IDs of genes found to be differentially-expressed (sigGenes) and the IDs of all genes in the dataset (universe). It uses the org.Hs.eg.db package to map between gene names and biological pathways.\n\nlibrary(clusterProfiler)\nuniverse &lt;- results_final %&gt;% pull(ENSEMBL)\nsigGenes &lt;- results_final %&gt;% \n  filter(padj &lt; 0.05) %&gt;% pull(ENSEMBL)\n\nenrich_go &lt;- enrichGO(\n  gene= sigGenes,\n  OrgDb = org.Hs.eg.db,\n  keyType = \"ENSEMBL\",\n  ont = \"BP\",\n  universe = universe,\n  qvalueCutoff = 0.05,\n  readable=TRUE\n)\n\nThe result of enrichGo can be turned into a data frame for easier interpretation.\n\nenrich_go %&gt;% data.frame %&gt;% \n  slice_head()\n\n                   ID                               Description GeneRatio\nGO:2001233 GO:2001233 regulation of apoptotic signaling pathway  164/4221\n             BgRatio RichFactor FoldEnrichment  zScore       pvalue    p.adjust\nGO:2001233 398/18429  0.4120603       1.799066 8.78405 1.565104e-16 9.85546e-13\n                 qvalue\nGO:2001233 6.741479e-13\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         geneID\nGO:2001233 MMP2/CTSC/PTGS2/HGF/INHBA/EYA4/BDKRB2/NRG1/SFRP1/CLU/UNC5B/PPARG/NR4A2/CD44/TLR4/LGALS3/MAZ/PPP2R1B/ITGA6/NUPR1/PMAIP1/BMP4/LRRK2/FGF10/IL1B/DDIT3/TNFRSF12A/SERINC3/CAV1/TMBIM6/BDNF/BOK/CFLAR/IER3/TNFSF15/ATF4/CSF2/IGF1/DDX3X/FAS/SKIL/PML/ACSL5/G0S2/BBC3/KLF4/SGK3/IL1A/ICAM1/ARHGEF2/PYCR1/FGF2/NCK1/WNT16/RRM2B/CREB3L1/IVNS1ABP/NFE2L2/STRADB/ATF3/TNFSF10/HSPB1/FYN/GDNF/TRIM32/BAK1/CYLD/PTTG1IP/ACAA2/HERPUD1/SIAH2/VNN1/NOC2L/TMEM161A/VDAC2/DNAJA1/TP53/PIAS4/THBS1/TNFSF14/TAF6/HMOX1/BID/EIF2AK3/WNT5A/CTH/PTPRC/PCGF2/RBCK1/MARCHF7/HYOU1/CCAR2/RPS6KB1/MEIS3/RB1CC1/PLAUR/GCLM/SRPX/HIF1A/PAK2/MDM2/SERPINE1/TPT1/PTPN1/FADD/BCL2/URI1/TMEM14A/PPIF/USP15/TPD52L1/MYC/GHITM/EIF5A/ACKR3/TRAP1/USP47/ITPRIP/CXCL12/SYVN1/RELA/CTSH/BAX/ENO1/HMGB2/APP/PIK3CB/RPS3/NME5/SLC25A5/DDIAS/PYCARD/PPP1CA/NFATC4/KDM1A/TRAF2/BAD/TGFBR1/SLC25A6/MIF/NF1/MAPK9/MSX1/DEDD2/AKT1/BAG5/NOL3/PLEKHF1/SEPTIN4/FIS1/CD74/SOD1/SRC/SFRP2/NRP1/BCLAF1/PRELID1/RTKN2/JAK2/SCG2/LMNA/HDAC1/SNAI1/OPA1\n           Count\nGO:2001233   164\n\n\nA dot plot can show us the most enriched pathways, and the size of each.\n\ndotplot(enrich_go,showCategory=20)\n\n\n\n\n\n\n\n\nRelationships between the identified categories can be found using emapplot.\n\nenrich_go &lt;- enrichplot::pairwise_termsim(enrich_go)\nemapplot(enrich_go)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n‚Ñπ The deprecated feature was likely used in the ggtangle package.\n  Please report the issue to the authors.\n\n\n\n\n\n\n\n\n\nOverlaps between gene sets can also be visualised using an ‚ÄúUpset plot‚Äù - an alternative to a venn diagram.\n\nenrichplot::upsetplot(enrich_go)\n\nWarning: `aes_()` was deprecated in ggplot2 3.0.0.\n‚Ñπ Please use tidy evaluation idioms with `aes()`\n‚Ñπ The deprecated feature was likely used in the enrichplot package.\n  Please report the issue at\n  &lt;https://github.com/GuangchuangYu/enrichplot/issues&gt;."
  },
  {
    "objectID": "training/bulk-rnaseq_3/index.html#gene-set-enrichment-analysis-gsea",
    "href": "training/bulk-rnaseq_3/index.html#gene-set-enrichment-analysis-gsea",
    "title": "Introduction to RNA-Seq - Part 3",
    "section": "Gene set enrichment analysis (GSEA)",
    "text": "Gene set enrichment analysis (GSEA)\nAn appealing feature of the GSEA method is that it does not require us to impose arbitrary cut-offs on the dataset to decide what is DE or not. Instead it takes a set of ranked statistics for all genes and looks for sets of genes that occur either at the top or bottom of the list. As GSEA uses the direction of a test result (either up- or down-regulated) the test statistics themselves must have a positive or negative sign. Although the p-values cannot therefore be used for this purpose, they can be transformed using the same equation used in the volcano plot (-10 x log\\(_{10}\\)) and multiplying by the ‚Äúsign‚Äù of the LFC (i.e.¬†+1 or -1).\nThe steps in producing the input required for GSEA are i) calculating and retrieving the ranking statistics ii) naming each one according to a chosen identifier (ENSEMBL or ENTREZID for example). The clusterProfiler package also includes an implementation of the GSEA algorithm (via the fgsea package), and the function works in much the same way as enrichGO from above.\n\nranked_genes &lt;- results_final %&gt;% \n  mutate(Score = sign(log2FoldChange)*-log10(padj)) %&gt;% \n  arrange(desc(Score)) %&gt;% \n  filter(!is.na(Score))\n  \ngeneList &lt;- pull(ranked_genes, Score)\nnames(geneList) &lt;- pull(ranked_genes, ENSEMBL)\n  \ngse_GO  &lt;- gseGO(geneList = geneList,\n        OrgDb = org.Hs.eg.db,\n        ont = \"BP\",keyType = \"ENSEMBL\")\n\nThe results may not necessarily be the same as the over-representation analysis, since GSEA is answering a slightly different question.\n\ngse_GO %&gt;% as.data.frame %&gt;% \n  slice_head(n = 10) %&gt;% \n  ## don't show the core enrichment to make a display a bit cleaner\n  select(-core_enrichment)\n\n                   ID                                          Description\nGO:1990266 GO:1990266                                 neutrophil migration\nGO:0032103 GO:0032103 positive regulation of response to external stimulus\nGO:1902622 GO:1902622                   regulation of neutrophil migration\nGO:0030593 GO:0030593                                neutrophil chemotaxis\nGO:0006066 GO:0006066                            alcohol metabolic process\nGO:0010562 GO:0010562  positive regulation of phosphorus metabolic process\nGO:0045937 GO:0045937   positive regulation of phosphate metabolic process\nGO:0006959 GO:0006959                              humoral immune response\nGO:0006956 GO:0006956                                complement activation\nGO:0006809 GO:0006809                    nitric oxide biosynthetic process\n           setSize enrichmentScore       NES       pvalue    p.adjust\nGO:1990266      67      -0.8447447 -1.854904 6.769307e-07 0.003719734\nGO:0032103     470      -0.6030969 -1.595928 3.442876e-06 0.009459301\nGO:1902622      36      -0.8936258 -1.806944 1.011179e-05 0.010266296\nGO:0030593      48      -0.8582870 -1.804620 7.208095e-06 0.010266296\nGO:0006066     265      -0.6470992 -1.630672 1.140449e-05 0.010266296\nGO:0010562     377      -0.6156663 -1.596903 1.307808e-05 0.010266296\nGO:0045937     377      -0.6156663 -1.596903 1.307808e-05 0.010266296\nGO:0006959     114      -0.7497180 -1.752118 2.433335e-05 0.016713970\nGO:0006956      25      -0.9207703 -1.764098 2.858334e-05 0.017451718\nGO:0006809      62      -0.8040712 -1.750708 4.250038e-05 0.021230871\n                qvalue rank                   leading_edge\nGO:1990266 0.003441658  721  tags=21%, list=4%, signal=20%\nGO:0032103 0.008752153 1823 tags=20%, list=10%, signal=18%\nGO:1902622 0.009498819  721  tags=22%, list=4%, signal=21%\nGO:0030593 0.009498819  323  tags=21%, list=2%, signal=21%\nGO:0006066 0.009498819 1365  tags=18%, list=8%, signal=17%\nGO:0010562 0.009498819 1432  tags=23%, list=8%, signal=21%\nGO:0045937 0.009498819 1432  tags=23%, list=8%, signal=21%\nGO:0006959 0.015464485 1411  tags=29%, list=8%, signal=27%\nGO:0006956 0.016147081  704  tags=56%, list=4%, signal=54%\nGO:0006809 0.019643715  655  tags=29%, list=4%, signal=28%\n\n\nAs GSEA uses all of the genes that were present in the dataset, it has also used all genes in a pathway to calculate the enrichment. However, to narrow-down the interpretation of a pathways the core enrichment or leading edge of a GSEA can be used. This refers to the subset of genes within an enriched pathway that primarily drives the enrichment signal. It represents the genes that are most responsible for the pathway receiving its high Enrichment Score and thus its significance.\n\n## show the core enrichment for the top pathway\ngse_GO %&gt;% as.data.frame %&gt;% \n  slice_head() %&gt;% \n  select(core_enrichment)\n\n                                                                                                                                                                                                                           core_enrichment\nGO:1990266 ENSG00000115008/ENSG00000002586/ENSG00000125538/ENSG00000145147/ENSG00000115020/ENSG00000169245/ENSG00000115310/ENSG00000131981/ENSG00000163735/ENSG00000124875/ENSG00000112378/ENSG00000111913/ENSG00000197635/ENSG00000115594\n\n\nAn overview of the results can be provided by a ‚Äúridge plot‚Äù. This allows comparison of the test statistics for each of the top enriched pathways.\n\nridgeplot(gse_GO)\n\n\n\n\n\n\n\n\nAn upset plot can still be produced, but this time the distribution of statistics for overlapping categories can be produced.\n\nenrichplot::upsetplot(gse_GO)\n\n\n\n\n\n\n\n\nGSEA introduces a new type of plot that summarizes the entire analysis for a single pathway and is central to interpreting its results. Shown below is the GSEA enrichment plot for the pathway with the most extreme enrichment score.\n\ntop_gsea &lt;- gse_GO %&gt;% as.data.frame %&gt;% \n  slice_max(abs(enrichmentScore)) %&gt;% \n  pull(ID)\n\ntop_gsea_ID &lt;- gse_GO %&gt;% as.data.frame %&gt;% \n  slice_max(abs(enrichmentScore)) %&gt;% \n  pull(Description)\n\nenrichplot::gseaplot2(gse_GO,geneSetID = top_gsea,title = top_gsea_ID)\n\n\n\n\n\n\n\n\nThe top panel shows the core evidence for enrichment. The line is a running sum statistic that measures the degree of enrichment. It starts at zero and increases every time a gene belonging to the pathway is encountered in the ranked list, whereas it slightly decreases when a gene not in the pathway is encountered. The peak of this curve is the maximum Enrichment Score (\\(\\text{ES}\\)). If the peak is on the left (positive), the pathway is up-regulated; if the peak is on the right (negative), the pathway is down-regulated. \\(\\text{GSEA}\\)\nThe black vertical lines beneath the curve are the pathway ‚Äúbarcode‚Äù. Each line represents a single gene from the pathway, placed at its exact ranked position. The clustering of these ticks‚Äîeither on the far left or far right‚Äîprovides the visual evidence for the calculated \\(\\text{ES}\\).\nThe Bottom Panel displays the value of the statistic used to rank all genes. This confirms that the genes on the far left of the plot have the most positive values (high \\(\\text{LFC}\\)) and those on the far right have the most negative values (low \\(\\text{LFC}\\)), providing context for the directional enrichment observed in the top panel.\nIn the example plot there is a high concentration of vertical black lines to the right of the plot - coinciding with the lowest point of the green line. This is where the majority of genes for this pathway can be found in the ranked gene list, and therefore we conclude there is a tendency for genes in this pathway to be DE."
  },
  {
    "objectID": "training/bulk-rnaseq_3/index.html#exercise-1",
    "href": "training/bulk-rnaseq_3/index.html#exercise-1",
    "title": "Introduction to RNA-Seq - Part 3",
    "section": "Exercise",
    "text": "Exercise\n\nIn addition to enriched GO terms, clusterProfiler can also find enriched KEGG terms using the enrichKEGG function. There are a couple of changes that are required from enrichGO\n\nENTREZID has to be used as the identifer type\nthe user must input an appropriate organism code. The code for humans is hsa.\n\nUse the enrichKEGG function to identify enriched KEGG terms in the analysis.\n(Optional) If you have time, use the gseKEGG to perform GSEA using KEGG terms.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n## KEGG tools require ENTREZ ID\n\nsigGenesEntrez &lt;- results_final %&gt;% \n  filter(padj &lt; 0.05) %&gt;% pull(ENTREZID)\n\nenrich_kegg &lt;- enrichKEGG(gene = sigGenesEntrez,\n                       organism = \"hsa\",)\nas.data.frame(enrich_kegg) %&gt;% slice_head(n = 10)\n\n                                     category                     subcategory\nhsa04820                   Cellular Processes                   Cell motility\nhsa05205                       Human Diseases                Cancer: overview\nhsa04510                   Cellular Processes Cellular community - eukaryotes\nhsa04151 Environmental Information Processing             Signal transduction\nhsa04933                       Human Diseases Endocrine and metabolic disease\nhsa05418                       Human Diseases          Cardiovascular disease\nhsa05010                       Human Diseases       Neurodegenerative disease\nhsa01521                       Human Diseases Drug resistance: antineoplastic\nhsa05417                       Human Diseases          Cardiovascular disease\nhsa05208                       Human Diseases                Cancer: overview\n               ID                                          Description\nhsa04820 hsa04820                         Cytoskeleton in muscle cells\nhsa05205 hsa05205                              Proteoglycans in cancer\nhsa04510 hsa04510                                       Focal adhesion\nhsa04151 hsa04151                           PI3K-Akt signaling pathway\nhsa04933 hsa04933 AGE-RAGE signaling pathway in diabetic complications\nhsa05418 hsa05418               Fluid shear stress and atherosclerosis\nhsa05010 hsa05010                                    Alzheimer disease\nhsa01521 hsa01521            EGFR tyrosine kinase inhibitor resistance\nhsa05417 hsa05417                            Lipid and atherosclerosis\nhsa05208 hsa05208    Chemical carcinogenesis - reactive oxygen species\n         GeneRatio  BgRatio RichFactor FoldEnrichment   zScore       pvalue\nhsa04820  104/2184 233/9497  0.4463519       1.940936 7.946716 1.285288e-13\nhsa05205   90/2184 204/9497  0.4411765       1.918431 7.246569 1.296939e-11\nhsa04510   89/2184 203/9497  0.4384236       1.906460 7.134185 2.554654e-11\nhsa04151  133/2184 362/9497  0.3674033       1.597632 6.335532 1.252586e-09\nhsa04933   51/2184 101/9497  0.5049505       2.195749 6.602037 1.306066e-09\nhsa05418   63/2184 142/9497  0.4436620       1.929239 6.096752 1.193027e-08\nhsa05010  138/2184 391/9497  0.3529412       1.534745 5.900922 1.228918e-08\nhsa01521   41/2184  80/9497  0.5125000       2.228577 6.030315 3.130383e-08\nhsa05417   85/2184 216/9497  0.3935185       1.711193 5.777844 3.891880e-08\nhsa05208   88/2184 227/9497  0.3876652       1.685740 5.714543 5.096923e-08\n             p.adjust       qvalue\nhsa04820 4.472802e-11 2.529988e-11\nhsa05205 2.256673e-09 1.276461e-09\nhsa04510 2.963398e-09 1.676211e-09\nhsa04151 9.090218e-08 5.141775e-08\nhsa04933 9.090218e-08 5.141775e-08\nhsa05418 6.109477e-07 3.455754e-07\nhsa05010 6.109477e-07 3.455754e-07\nhsa01521 1.361717e-06 7.702390e-07\nhsa05417 1.504860e-06 8.512065e-07\nhsa05208 1.773729e-06 1.003289e-06\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  geneID\nhsa04820                                                                                                                                                                    2006/1301/2335/1282/7058/8516/22801/1311/1289/1277/1284/633/7168/1462/22795/1634/1288/2200/1293/2273/2201/23500/84675/1832/7111/84665/91010/3655/3693/483/50509/1756/3673/3696/8572/1290/6382/1466/284217/84823/1837/829/1287/255631/4627/1278/83660/476/23353/9260/7414/3676/272/85301/8910/25802/58529/6443/6645/2192/56776/51332/4001/114793/1281/830/2275/6385/23345/171024/7057/4608/7791/2199/70/7170/2010/3678/3339/3679/6383/6709/375790/1286/7094/1302/2023/3690/64236/6444/51778/2027/5339/7169/29766/3688/1729/64423/25777/3691/7138/71/4000/4628\nhsa05205                                                                                                                                                                                                                                                                  2335/4313/3082/1277/7482/4660/1634/8325/7078/960/7099/27250/3693/3481/2535/117581/286/7422/3673/857/5781/6382/7472/817/967/5335/5328/3479/355/1278/3480/9475/4233/8503/22800/6608/6548/2247/51384/7023/5293/2316/5747/7040/10855/6385/6093/5290/7157/23365/3265/7057/6300/60495/7474/1514/6774/208/3678/2817/3339/6383/6198/5329/7074/3091/84309/4193/9138/4609/673/1655/1026/3690/5291/8826/5829/5962/858/5499/3688/6654/207/5605/6714/7097/6655/1432/8324/71\nhsa04510                                                                                                                                                                                                                                                                       2335/1282/7058/8516/22801/1311/3082/80310/3910/1277/1284/4660/3909/1288/3912/1293/5156/4638/3655/3693/5154/2317/7422/3673/3696/2321/7148/857/284217/56034/1287/3479/1278/3480/9475/83660/824/5159/7414/4233/3676/8503/3918/5293/2316/5747/2534/7423/399694/103910/6093/5290/3265/7057/7791/3725/10627/208/3678/9564/3679/5062/596/7408/673/1286/7094/3914/81/3690/5291/1950/5829/858/5499/572/3688/1729/5601/394/6654/207/331/6714/5906/3691/6655/2012/71\nhsa04151                             2335/1282/7058/8516/2252/22801/1311/3082/80310/2069/3910/1277/1284/1129/54541/3909/1288/3912/23566/1293/1902/5525/7099/9180/374/5106/5156/5519/1978/4254/3655/3569/3693/10971/3481/54331/4915/5154/3575/7422/2255/3673/3696/2321/7148/627/284217/6446/2791/468/56034/28227/1287/3479/7010/1278/3480/1019/3454/5159/23678/4233/3676/8503/3918/5563/2247/3815/2784/64764/90993/5293/5747/2790/5562/2668/7423/5585/3716/1969/1147/5290/7534/7157/5528/3265/55970/7057/1435/3570/1977/208/3678/7533/7184/3679/6198/64223/4193/596/7249/4609/2783/10161/3320/9586/1026/1286/3914/6794/5970/3667/3690/5291/7531/1950/29941/57521/572/3688/2690/6654/207/5934/5605/1440/2149/3691/7097/5527/5586/6655/3717\nhsa04933                                                                                                                                                                                                                                                                                                                                                                                                                                                                   2335/1282/4313/2152/1277/1284/7412/4088/1288/3569/50507/7422/3553/5335/1287/1278/1019/185/8503/3552/3383/5293/1281/7040/7423/5290/3265/6300/3725/23236/6774/208/5054/596/7048/1286/113026/5970/581/5291/6347/5332/7046/1729/5601/207/5331/1432/3717/5330/3576\nhsa05418                                                                                                                                                                                                                                                                                                                                                                                                             4313/3554/4257/7412/4217/5154/652/1728/7422/3553/7295/10365/857/6382/4205/2947/4258/5327/8503/8878/3552/3383/9181/5563/9446/5293/4780/5747/5562/6385/1147/5290/7157/51588/6300/3162/3725/1514/208/2817/7184/6383/596/3320/4259/5970/1843/3690/5291/2941/7341/6347/858/2946/801/5601/207/6885/2949/6714/9817/1432/71\nhsa05010 5743/7482/8325/4217/64116/102/6622/57142/3569/4311/2535/50507/83464/4891/3553/25825/1649/203068/7472/468/355/55062/10376/10381/824/8503/3552/22926/8660/4538/4540/51384/5293/1855/3028/1856/1857/10476/23621/4726/8408/56901/7417/1147/5290/4537/348/6391/4541/7381/64837/3265/22943/11047/1435/4729/4137/4713/4035/4707/637/23385/9451/7846/7474/23236/4539/10383/6392/208/4694/5289/9821/1452/7416/100506742/7384/8772/10105/4519/91252/673/5709/55334/488/1350/5970/3667/374291/89953/8883/4728/351/5291/5715/4709/8313/84617/522/4535/7979/292/2776/5332/7186/801/1020/572/293/3798/513/5601/5702/207/4512/5331/775/4513/514/3416/5605/498/5710/4514/4723/1965/5530/8324/5718/5330/515/3799/25800/4536/5714/29985/1351/5704\nhsa01521                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       3082/80310/3084/5156/1978/3569/5154/7422/56034/5335/3479/3480/2621/5159/4233/8503/558/2247/5293/3716/399694/5290/3265/3570/6774/1977/208/6198/596/673/581/5291/1950/572/4763/6654/207/5605/6714/6655/3717\nhsa05417                                                                                                                                                                                                                                                                                         7412/948/4312/5468/4217/7099/3569/4067/3654/7436/3553/1649/2919/4314/30001/468/817/5335/355/9475/8503/3383/3606/22926/142678/3306/5452/3309/5293/4780/5747/8743/4775/10010/1147/5290/7157/3265/6300/8795/29110/637/3725/9451/23236/6774/208/3308/7184/10454/3949/596/148022/9138/6257/3320/834/5970/929/581/23643/5291/6347/3665/29108/5332/7186/801/572/5601/207/5331/6885/51135/6714/5906/7097/1965/5530/1432/3717/5330/958/3576/3592\nhsa05208                                                                                                                                                                                                                                                                                 3082/4257/4217/1646/1545/1645/50507/1728/7422/5781/2947/4258/5337/8644/4233/8503/847/4538/4540/9446/5293/4780/5747/10476/4726/56901/7417/1147/5290/4537/6391/4541/7381/2052/3265/4729/6300/3162/4713/4707/3725/4539/6392/208/4694/3091/7416/7384/5770/10105/4519/673/1350/4259/5970/374291/4728/5291/2941/1950/4709/522/4535/292/25/2946/572/293/513/5601/6654/207/4512/4513/514/5605/2949/6647/6714/498/4514/4723/9817/6655/1432/515/4536/1351\n         Count\nhsa04820   104\nhsa05205    90\nhsa04510    89\nhsa04151   133\nhsa04933    51\nhsa05418    63\nhsa05010   138\nhsa01521    41\nhsa05417    85\nhsa05208    88\n\n\n\nranked_genes &lt;- results_final %&gt;% \n  mutate(Score = sign(log2FoldChange)*-log10(padj)) %&gt;% \n  arrange(desc(Score)) %&gt;% \n  filter(!is.na(Score)) %&gt;% \n  filter(!is.na(ENTREZID)) %&gt;% \n  filter(!duplicated(ENTREZID))\n  \ngeneList &lt;- pull(ranked_genes, Score)\nnames(geneList) &lt;- pull(ranked_genes, ENTREZID)\n  \ngse_GO  &lt;- gseKEGG(geneList = geneList,\n                   organism = \"hsa\")\n\ngse_GO %&gt;% data.frame() %&gt;% slice_head(n = 10)\n\n               ID                                                   Description\nhsa04610 hsa04610                           Complement and coagulation cascades\nhsa04142 hsa04142                                                      Lysosome\nhsa04061 hsa04061 Viral protein interaction with cytokine and cytokine receptor\nhsa00140 hsa00140                                  Steroid hormone biosynthesis\nhsa00590 hsa00590                                   Arachidonic acid metabolism\nhsa04820 hsa04820                                  Cytoskeleton in muscle cells\nhsa05204 hsa05204                         Chemical carcinogenesis - DNA adducts\nhsa04640 hsa04640                                    Hematopoietic cell lineage\n         setSize enrichmentScore       NES       pvalue     p.adjust\nhsa04610      42      -0.8847450 -1.863660 1.231655e-06 0.0004249211\nhsa04142     122      -0.7193317 -1.728881 5.642238e-05 0.0097328598\nhsa04061      45      -0.8092317 -1.733303 3.298341e-04 0.0250365638\nhsa00140      22      -0.8920840 -1.708880 2.177490e-04 0.0250365638\nhsa00590      31      -0.8461613 -1.701086 3.628488e-04 0.0250365638\nhsa04820     169       0.6681234  1.798939 5.897550e-04 0.0339109115\nhsa05204      29      -0.8392856 -1.664224 7.370988e-04 0.0363284415\nhsa04640      45      -0.7855749 -1.682633 1.111485e-03 0.0479327919\n               qvalue rank                  leading_edge\nhsa04610 0.0003746825 1081 tags=48%, list=7%, signal=44%\nhsa04142 0.0085821403 1373 tags=25%, list=9%, signal=23%\nhsa04061 0.0220764819 1348 tags=56%, list=9%, signal=51%\nhsa00140 0.0220764819  662 tags=41%, list=4%, signal=39%\nhsa00590 0.0220764819  662 tags=29%, list=4%, signal=28%\nhsa04820 0.0299016123  406 tags=20%, list=3%, signal=20%\nhsa05204 0.0320333168  814 tags=45%, list=5%, signal=42%\nhsa04640 0.0422656807  809 tags=38%, list=5%, signal=36%\n                                                                                                                                                                             core_enrichment\nhsa04610                                                                                        5270/716/10544/2159/5327/3075/710/5328/5627/1604/966/2/718/5648/1191/624/5055/7035/4179/2152\nhsa04142                        1213/1514/164/6609/3920/8763/135112/285362/138050/1200/8905/2760/1203/968/1513/967/9583/1508/950/2799/175/4891/2519/9741/2517/9516/3074/26503/3988/3423/1075\nhsa04061                                                    6347/3588/6387/57007/11009/9547/8795/8740/6357/3570/1435/10344/7133/8743/8794/3606/6354/2919/6355/3569/3627/3572/6374/6372/51554\nhsa00140                                                                                                                                         8644/6715/3290/3294/1645/1545/1646/412/9420\nhsa00590                                                                                                                                     8644/284273/22949/5742/6916/5740/9536/5321/5743\nhsa04820 2006/1301/2335/1282/7058/22801/1311/1289/1277/1284/633/7168/1462/2200/1293/2201/23500/1832/91010/3693/50509/1756/8572/1290/6382/1466/284217/84823/255631/4627/1278/83660/9260/85301\nhsa05204                                                                                                                       2946/2941/4259/9/2052/9446/4258/2947/3290/1545/1646/4257/5743\nhsa04640                                                                                               3815/3552/3676/1604/100133941/3673/966/3553/4311/7037/3569/3655/4254/960/928/948/3554"
  },
  {
    "objectID": "training/bulk-rnaseq_3/index.html#which-database-to-use",
    "href": "training/bulk-rnaseq_3/index.html#which-database-to-use",
    "title": "Introduction to RNA-Seq - Part 3",
    "section": "Which database to use?",
    "text": "Which database to use?\nWe have looked the two main types of test supported by clusterProfiler using a database of Gene Ontology (GO) terms as a reference. These were implemented in the enrichGO and gseGO functions respectively. The above exercise used KEGG instead. A nice feature of clusterProfiler is that you can test different databases for significant pathways with few changes to your code. Some of the different databases and the corresponding function to use are summarised below\n\nSummary of different databases that clusterProfiler can use\n\n\nDatabase\nFunction\nPrimary Focus\nPros\nCons\n\n\n\n\nGene Ontology (GO)\n\n`enrichGO`\n`gseGO`\n\nBiological Process (BP), Molecular Function (MF), Cellular Component (CC)\n\nBroadest Coverage: Covers nearly all known genes in most species.\nHierarchical: Allows for semantic similarity grouping (simplify()).\n\n\nHigh Redundancy: Similar terms often overlap significantly.\nFunctional Ambiguity: Terms can be very broad.\n\n\n\nKEGG (Kyoto Encyclopedia of Genes and Genomes)\n\n`enrichKEGG`\n`gseKEGG`\n\nMetabolic & Regulatory Pathways\n\nVisual Maps: Provides detailed, manually drawn pathway diagrams.\nFocus on Mechanism: Excellent for examining core signaling and metabolic processes\n\n\nLimited Coverage: Focused primarily on well-studied pathways.\nLicensing/Updates: Access can be restricted/less frequently updated in free versions.\n\n\n\nReactome\n\nenrichPathway\n`gsePathway`\n\nMolecular Events & Reactions\n\nMechanistic Detail: Highly curated, evidence-based sequence of molecular events (reactions)\nNon-Redundant Structure: Less overlap between main pathways than in GO.\n\n\nBias toward Human: Predominantly focused on human biology.\nPathway Fragmentation: High detail can make pathways appear fragmented.\n\n\n\nWikiPathways\n\nenrichWP\ngseWP\n\nCommunity-Driven, Open Content\n\nFlexible & Collaborative: Allows any researcher to create, edit, and contribute.\nRapid Inclusion: Quickly integrates novel and species-specific pathways.\n\n\nVariable Quality: Quality control is less stringent than curated databases.\nInconsistent Scope: Pathway scale and definition can be less uniform.\n\n\n\nDisease Ontology\n\nenrichDO\ngseDO\n\nStructured Vocabulary for Human Diseases\n\nHierarchical Structure: DAG allows for GSEA and semantic clustering.\nStandardized: Provides a single, clear terminology for classifying diseases.\n\n\nGene Association Reliability: Associations are aggregated from multiple sources, meaning reliability can vary.\nRedundancy: Hierarchical terms mean high-level parent terms can obscure specific results.\n\n\n\n\nUltimately, you will have to decide which database(s) are more appropriate for the interpretation of your results."
  },
  {
    "objectID": "training/bulk-rnaseq_3/index.html#additional-visualistion-of-kegg-pathways",
    "href": "training/bulk-rnaseq_3/index.html#additional-visualistion-of-kegg-pathways",
    "title": "Introduction to RNA-Seq - Part 3",
    "section": "Additional visualistion of KEGG pathways",
    "text": "Additional visualistion of KEGG pathways\nThe special feature of a KEGG pathway is its standardized, structured graphical representation as a set of computable maps. This means that visualisation with tools like pathview is possible. In the below we take a pathway such as ‚ÄúLysosome‚Äù, use it‚Äôs standard KEGG layout and colour the genes involved in the pathway according to the LFC.\n\nlibrary(pathview)\n\n# The KEGG ID you want to visualize\nkegg_id &lt;- \"hsa04142\"\n\n# Organism code (hsa for human)\norganism_code &lt;- \"hsa\"\n\nlfc_values &lt;- pull(results_final, \"log2FoldChange\")\nnames(lfc_values) &lt;- pull(results_final, \"ENTREZID\")\n\npathview(\n  gene.data = lfc_values,\n  pathway.id = \"hsa04142\",\n  species = \"hsa\",\n  low = \"blue\",\n  mid = \"gray\",\n  high = \"red\",\n  kegg.native = TRUE,\n  out.suffix = \"lysosome_blue_red\"\n)"
  },
  {
    "objectID": "training/bulk-rnaseq_3/index.html#pathways-analysis-for-non-human-organisms",
    "href": "training/bulk-rnaseq_3/index.html#pathways-analysis-for-non-human-organisms",
    "title": "Introduction to RNA-Seq - Part 3",
    "section": "Pathways analysis for non-human organisms",
    "text": "Pathways analysis for non-human organisms\nDue to the nature of our dataset we have been naturally using a human-centric database provided in the Bioconductor package org.Hs.eg.db. Other model organisms such as org.Mm.eg.db (Mouse), org.Rn.eg.db (Rat) can be used be substituting the OrgDb argument in enrichGO\n\n# library(org.Mm.eg.db)\n\nenrich_go &lt;- enrichGO(\n  gene= sigGenes,\n  ## Change this for a different organism\n  OrgDb = org.Mm.eg.db,\n  keyType = \"ENSEMBL\",\n  ont = \"BP\",\n  universe = universe,\n  qvalueCutoff = 0.05,\n  readable=TRUE\n)\n\nIf organisms are not found in Bioconductor the relevant data can still be retrieved via the AnnotationHub package."
  },
  {
    "objectID": "posts/2025_10_30_tidybulk/index.html",
    "href": "posts/2025_10_30_tidybulk/index.html",
    "title": "Teaching an old dog new tricks; tidy analysis of RNA-seq",
    "section": "",
    "text": "I have been analysing RNA-seq data for rather longer than I care to mention, and have mostly stuck to the same tried and tested workflow. Whilst re-writing my tutorials for these pages it occurred to me that the materials, that mostly use base R, would benefit from some of the same tidy methodology I employ on a daily basis for general data manipulation and visualisation. Fortunately in recent years there has been a drive to introduce tidy frameworks for omics. Can this new framework replace my daily workhorse?\n\nWe‚Äôll start by downloading some example data. This is the same example dataset used in my recent bulk RNA-seq tutorial.\n\n## Create two folders for the meta data and counts\ndir.create(\"meta_data\", showWarnings = FALSE)\ndir.create(\"raw_counts\",showWarnings = FALSE)\n\n\n## Download the raw data files\ndownload.file(\"https://raw.githubusercontent.com/markdunning/markdunning.github.com/refs/heads/master/files/training/bulk_rnaseq/meta_data/sampleInfo.csv\", destfile = \"meta_data/sampleInfo.csv\")\ndownload.file(\"https://raw.githubusercontent.com/markdunning/markdunning.github.com/refs/heads/master/files/training/bulk_rnaseq/raw_counts/raw_counts_matrix.tsv\", destfile = \"raw_counts/raw_counts_matrix.tsv\")\n\nThe packages you will need can be installed via\n\nif (!require(BiocManager)) install.packages(\"BiocManager\")\n\nif (!require(SummarizedExperiment)) BiocManager::install(\"SummarizedExperiment\")\n\nif (!require(tidybulk)) BiocManager::install(\"tidybulk\")\n\nThe data have to be read into R first. It consists of two files containing the counts and ‚Äúmetadata‚Äù about the samples. Both are in tab-delimited files so we can use the read.table function from base R. However, since the counts are required to be in a numeric matrix form with rownames being gene or feature identifiers we have to manipulate the input data accordingly. Specifically, we need to set the rownames to be the gene names and remove the first column from the input data.\n\nmeta &lt;- read.csv(\"meta_data/sampleInfo.csv\") |&gt;\n  dplyr::mutate(condition = stringr::str_to_upper(condition))\n  \n\nraw &lt;- read.delim(\"raw_counts/raw_counts_matrix.tsv\")\ncounts &lt;- raw[,-1]\nrownames(counts) &lt;- raw$ENSEMBL\ncolnames(counts) &lt;- stringr::str_remove_all(colnames(counts), \"X\")\n\nThe route into using the tidy framework seems to be via a SummarizedExperiment object rather than the DESeqDataset that I am used to. We will briefly examine this format.\n\nlibrary(SummarizedExperiment)\ncaf_data &lt;- SummarizedExperiment(assays=list(counts=counts),\n                     colData=meta,)\ncaf_data\n\nclass: SummarizedExperiment \ndim: 57914 9 \nmetadata(0):\nassays(1): counts\nrownames(57914): ENSG00000000003 ENSG00000000005 ... ENSG00000284747\n  ENSG00000284748\nrowData names(0):\ncolnames(9): 1_CTR_BC_2 2_TGF_BC_4 ... 8_TGF_BC_14 9_IR_BC_15\ncolData names(5): Run condition Name Replicate Treated\n\n\nThe raw counts can be accessed using the assay function. Each entry is the number of counts assigned to a particular gene (row) in a given sample (column). The row and column names are the Ensembl gene identifier, and a sample name respectively.\n\n## Use select and slice to print fewer items to the screen\n## Feel free to remove these lines if you want to see the full output\nlibrary(dplyr)\nassay(caf_data) %&gt;% \n  select(1:4) %&gt;% \n  slice(1:10)\n\n                1_CTR_BC_2 2_TGF_BC_4 3_IR_BC_5 4_CTR_BC_6\nENSG00000000003       1579       1547      1342       1704\nENSG00000000005          0          0         0          0\nENSG00000000419       1774       1775      1866       1809\nENSG00000000457        698        617       601        733\nENSG00000000460        246        309       116        224\nENSG00000000938         10          5        11          6\nENSG00000000971       6370       5238      5008       6486\nENSG00000001036       4009       3335      3855       4224\nENSG00000001084       1037        940       928        970\nENSG00000001167       1498       1622      1042       1406\n\n\nIf we want to know more information about the biological samples we have to use the colData function\n\ncolData(caf_data) %&gt;% \n  data.frame \n\n                     Run condition  Name Replicate Treated\n1_CTR_BC_2   1_CTR_BC_2        CTR CTR_1         1       N\n2_TGF_BC_4    2_TGF_BC_4       TGF TGF_1         1       Y\n3_IR_BC_5      3_IR_BC_5        IR  IR_1         1       Y\n4_CTR_BC_6    4_CTR_BC_6       CTR CTR_2         2       N\n5_TGF_BC_7    5_TGF_BC_7       TGF TGF_2         2       Y\n6_IR_BC_12    6_IR_BC_12        IR  IR_2         2       Y\n7_CTR_BC_13 7_CTR_BC_13        CTR CTR_3         3       N\n8_TGF_BC_14  8_TGF_BC_14       TGF TGF_3         3       Y\n9_IR_BC_15    9_IR_BC_15        IR  IR_3         3       Y\n\n\nThe data representation was adopted during the days of microarrays as a way to standardize and unify the storage and handling of complex genomic data. It is a perfectly natural for the counts for a particular gene to be found by looking across columns for a particular row, and indeed this is often how we interact with count data in spreadsheets. Furthermore, the linear model interface of limma and associated packages made use of this data structure. Since we didn‚Äôt have ggplot2 and friends at the time we were happy using base R‚Äôs boxplot and other functions.\nHowever, the format is not immediately accessible to those familiar with a ‚Äútidyverse‚Äù mindset. I find this particularly jarring during my RNA-seq teaching which based upon the foundation of dplyr and ggplot2. It is somewhat disappointing to tell students that the code they learnt about in the introductory R classes cannot be applied to the new data type of RNA-seq without some serious data manipulation.\nThe problems arise because the data are ‚Äúwide‚Äù and not ‚Äúlong‚Äù. Consider the code to visualise the distribution of each sample as a boxplot (which is a common QC task)\n\n## Do not try to run this!\nggplot(data, aes(x = ..., y =...)) + geom_boxplot()\n\nFrom a ‚Äútidy‚Äù standpoint we require a a variable in our dataset that can be mapped to the x axis. In a boxplot this should be the sample name. Of course, we can get around this problem via the usage of pivot_longer from tidyr but this is a bit more work than I would like. I would rather concentrate on the data visualisation and interpretation\nWe might also like to subset our data according to particular sample groupings, or retrieve the data for a given gene and then plot. This is complicated by the counts and meta data being stored separately, meaning they have to be joined. Again, not impossible but does distract from the key learning objectives of learning about RNA-seq."
  },
  {
    "objectID": "posts/2025_10_30_tidybulk/index.html#pre-amble",
    "href": "posts/2025_10_30_tidybulk/index.html#pre-amble",
    "title": "Teaching an old dog new tricks; tidy analysis of RNA-seq",
    "section": "",
    "text": "I have been analysing RNA-seq data for rather longer than I care to mention, and have mostly stuck to the same tried and tested workflow. Whilst re-writing my tutorials for these pages it occurred to me that the materials, that mostly use base R, would benefit from some of the same tidy methodology I employ on a daily basis for general data manipulation and visualisation. Fortunately in recent years there has been a drive to introduce tidy frameworks for omics. Can this new framework replace my daily workhorse?\n\nWe‚Äôll start by downloading some example data. This is the same example dataset used in my recent bulk RNA-seq tutorial.\n\n## Create two folders for the meta data and counts\ndir.create(\"meta_data\", showWarnings = FALSE)\ndir.create(\"raw_counts\",showWarnings = FALSE)\n\n\n## Download the raw data files\ndownload.file(\"https://raw.githubusercontent.com/markdunning/markdunning.github.com/refs/heads/master/files/training/bulk_rnaseq/meta_data/sampleInfo.csv\", destfile = \"meta_data/sampleInfo.csv\")\ndownload.file(\"https://raw.githubusercontent.com/markdunning/markdunning.github.com/refs/heads/master/files/training/bulk_rnaseq/raw_counts/raw_counts_matrix.tsv\", destfile = \"raw_counts/raw_counts_matrix.tsv\")\n\nThe packages you will need can be installed via\n\nif (!require(BiocManager)) install.packages(\"BiocManager\")\n\nif (!require(SummarizedExperiment)) BiocManager::install(\"SummarizedExperiment\")\n\nif (!require(tidybulk)) BiocManager::install(\"tidybulk\")\n\nThe data have to be read into R first. It consists of two files containing the counts and ‚Äúmetadata‚Äù about the samples. Both are in tab-delimited files so we can use the read.table function from base R. However, since the counts are required to be in a numeric matrix form with rownames being gene or feature identifiers we have to manipulate the input data accordingly. Specifically, we need to set the rownames to be the gene names and remove the first column from the input data.\n\nmeta &lt;- read.csv(\"meta_data/sampleInfo.csv\") |&gt;\n  dplyr::mutate(condition = stringr::str_to_upper(condition))\n  \n\nraw &lt;- read.delim(\"raw_counts/raw_counts_matrix.tsv\")\ncounts &lt;- raw[,-1]\nrownames(counts) &lt;- raw$ENSEMBL\ncolnames(counts) &lt;- stringr::str_remove_all(colnames(counts), \"X\")\n\nThe route into using the tidy framework seems to be via a SummarizedExperiment object rather than the DESeqDataset that I am used to. We will briefly examine this format.\n\nlibrary(SummarizedExperiment)\ncaf_data &lt;- SummarizedExperiment(assays=list(counts=counts),\n                     colData=meta,)\ncaf_data\n\nclass: SummarizedExperiment \ndim: 57914 9 \nmetadata(0):\nassays(1): counts\nrownames(57914): ENSG00000000003 ENSG00000000005 ... ENSG00000284747\n  ENSG00000284748\nrowData names(0):\ncolnames(9): 1_CTR_BC_2 2_TGF_BC_4 ... 8_TGF_BC_14 9_IR_BC_15\ncolData names(5): Run condition Name Replicate Treated\n\n\nThe raw counts can be accessed using the assay function. Each entry is the number of counts assigned to a particular gene (row) in a given sample (column). The row and column names are the Ensembl gene identifier, and a sample name respectively.\n\n## Use select and slice to print fewer items to the screen\n## Feel free to remove these lines if you want to see the full output\nlibrary(dplyr)\nassay(caf_data) %&gt;% \n  select(1:4) %&gt;% \n  slice(1:10)\n\n                1_CTR_BC_2 2_TGF_BC_4 3_IR_BC_5 4_CTR_BC_6\nENSG00000000003       1579       1547      1342       1704\nENSG00000000005          0          0         0          0\nENSG00000000419       1774       1775      1866       1809\nENSG00000000457        698        617       601        733\nENSG00000000460        246        309       116        224\nENSG00000000938         10          5        11          6\nENSG00000000971       6370       5238      5008       6486\nENSG00000001036       4009       3335      3855       4224\nENSG00000001084       1037        940       928        970\nENSG00000001167       1498       1622      1042       1406\n\n\nIf we want to know more information about the biological samples we have to use the colData function\n\ncolData(caf_data) %&gt;% \n  data.frame \n\n                     Run condition  Name Replicate Treated\n1_CTR_BC_2   1_CTR_BC_2        CTR CTR_1         1       N\n2_TGF_BC_4    2_TGF_BC_4       TGF TGF_1         1       Y\n3_IR_BC_5      3_IR_BC_5        IR  IR_1         1       Y\n4_CTR_BC_6    4_CTR_BC_6       CTR CTR_2         2       N\n5_TGF_BC_7    5_TGF_BC_7       TGF TGF_2         2       Y\n6_IR_BC_12    6_IR_BC_12        IR  IR_2         2       Y\n7_CTR_BC_13 7_CTR_BC_13        CTR CTR_3         3       N\n8_TGF_BC_14  8_TGF_BC_14       TGF TGF_3         3       Y\n9_IR_BC_15    9_IR_BC_15        IR  IR_3         3       Y\n\n\nThe data representation was adopted during the days of microarrays as a way to standardize and unify the storage and handling of complex genomic data. It is a perfectly natural for the counts for a particular gene to be found by looking across columns for a particular row, and indeed this is often how we interact with count data in spreadsheets. Furthermore, the linear model interface of limma and associated packages made use of this data structure. Since we didn‚Äôt have ggplot2 and friends at the time we were happy using base R‚Äôs boxplot and other functions.\nHowever, the format is not immediately accessible to those familiar with a ‚Äútidyverse‚Äù mindset. I find this particularly jarring during my RNA-seq teaching which based upon the foundation of dplyr and ggplot2. It is somewhat disappointing to tell students that the code they learnt about in the introductory R classes cannot be applied to the new data type of RNA-seq without some serious data manipulation.\nThe problems arise because the data are ‚Äúwide‚Äù and not ‚Äúlong‚Äù. Consider the code to visualise the distribution of each sample as a boxplot (which is a common QC task)\n\n## Do not try to run this!\nggplot(data, aes(x = ..., y =...)) + geom_boxplot()\n\nFrom a ‚Äútidy‚Äù standpoint we require a a variable in our dataset that can be mapped to the x axis. In a boxplot this should be the sample name. Of course, we can get around this problem via the usage of pivot_longer from tidyr but this is a bit more work than I would like. I would rather concentrate on the data visualisation and interpretation\nWe might also like to subset our data according to particular sample groupings, or retrieve the data for a given gene and then plot. This is complicated by the counts and meta data being stored separately, meaning they have to be joined. Again, not impossible but does distract from the key learning objectives of learning about RNA-seq."
  },
  {
    "objectID": "posts/2025_10_30_tidybulk/index.html#introduction-to-tidybulk",
    "href": "posts/2025_10_30_tidybulk/index.html#introduction-to-tidybulk",
    "title": "Teaching an old dog new tricks; tidy analysis of RNA-seq",
    "section": "Introduction to tidybulk",
    "text": "Introduction to tidybulk\nThe tidybulk package solves these issues, and also provides a way of performing other common analysis tasks. Once the tidybulk function is applied, the long nature of the data in this format is immediately apparent as we have a huge amount of rows. However, we have all the information we require in the table to permit queries using standard tidyverse operations.\n\nlibrary(tidybulk)\ncaf_tidy &lt;- caf_data %&gt;% tidybulk()\n\n\n## Not evaluated to print excessive printing to screen for the HTML notes.\ncaf_tidy %&gt;% \n  slice_head(n = 10)\n\n# A tibble: 10 √ó 8\n   .feature        .sample    counts Run       condition Name  Replicate Treated\n   &lt;chr&gt;           &lt;chr&gt;       &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;  \n 1 ENSG00000000003 1_CTR_BC_2   1579 \"1_CTR_B‚Ä¶ CTR       CTR_1         1 N      \n 2 ENSG00000000005 1_CTR_BC_2      0 \"1_CTR_B‚Ä¶ CTR       CTR_1         1 N      \n 3 ENSG00000000419 1_CTR_BC_2   1774 \"1_CTR_B‚Ä¶ CTR       CTR_1         1 N      \n 4 ENSG00000000457 1_CTR_BC_2    698 \"1_CTR_B‚Ä¶ CTR       CTR_1         1 N      \n 5 ENSG00000000460 1_CTR_BC_2    246 \"1_CTR_B‚Ä¶ CTR       CTR_1         1 N      \n 6 ENSG00000000938 1_CTR_BC_2     10 \"1_CTR_B‚Ä¶ CTR       CTR_1         1 N      \n 7 ENSG00000000971 1_CTR_BC_2   6370 \"1_CTR_B‚Ä¶ CTR       CTR_1         1 N      \n 8 ENSG00000001036 1_CTR_BC_2   4009 \"1_CTR_B‚Ä¶ CTR       CTR_1         1 N      \n 9 ENSG00000001084 1_CTR_BC_2   1037 \"1_CTR_B‚Ä¶ CTR       CTR_1         1 N      \n10 ENSG00000001167 1_CTR_BC_2   1498 \"1_CTR_B‚Ä¶ CTR       CTR_1         1 N      \n\n\nWe can now remove the caf_data object to save memory\n\nrm(caf_data)\n\nSay for example we want the counts for a particular gene, and which sample it is most highly-expressed in\n\ncaf_tidy %&gt;% \n  filter(.feature == \"ENSG00000000003\") %&gt;% \n    dplyr::select(counts,Run) %&gt;% \n    arrange(desc(counts)) %&gt;% \n  slice(1:10)\n\n# A tibble: 9 √ó 2\n  counts Run           \n   &lt;int&gt; &lt;chr&gt;         \n1   1704 \"4_CTR_BC_6\"  \n2   1579 \"1_CTR_BC_2 \" \n3   1556 \"7_CTR_BC_13 \"\n4   1547 \"2_TGF_BC_4\"  \n5   1395 \"5_TGF_BC_7\"  \n6   1370 \"8_TGF_BC_14\" \n7   1342 \"3_IR_BC_5\"   \n8   1269 \"9_IR_BC_15\"  \n9   1264 \"6_IR_BC_12\"  \n\n\nOr calculate the average expression in different groups.\n\ncaf_tidy %&gt;% \n  filter(.feature == \"ENSG00000000003\") %&gt;% \n    group_by(condition) %&gt;% \n    summarise(mean(counts))\n\n# A tibble: 3 √ó 2\n  condition `mean(counts)`\n  &lt;chr&gt;              &lt;dbl&gt;\n1 CTR                1613 \n2 IR                 1292.\n3 TGF                1437.\n\n\nA basic QC metric is to count the total number of reads for each sample. In a typical bulk RNA-seq study we should be getting 10s of millions of reads - although the total number will vary. Any samples with dramatically lower numbers could be cause for concern.\n\ncaf_tidy %&gt;% \n  group_by(.sample) %&gt;% \n  summarise(LibrarySize = sum(counts)) %&gt;% \n  mutate(`Library Size - Millions of Reads` = LibrarySize / 1e6)\n\n# A tibble: 9 √ó 3\n  .sample     LibrarySize `Library Size - Millions of Reads`\n  &lt;chr&gt;             &lt;int&gt;                              &lt;dbl&gt;\n1 1_CTR_BC_2     37966392                               38.0\n2 2_TGF_BC_4     42302453                               42.3\n3 3_IR_BC_5      33300002                               33.3\n4 4_CTR_BC_6     39401879                               39.4\n5 5_TGF_BC_7     37716366                               37.7\n6 6_IR_BC_12     32599748                               32.6\n7 7_CTR_BC_13    34273109                               34.3\n8 8_TGF_BC_14    38522174                               38.5\n9 9_IR_BC_15     36478190                               36.5\n\n\nThe resulting data can be visualised using a geom_col in ggplot2 for example.\n\nlibrary(ggplot2)\ncaf_tidy %&gt;% \n  group_by(.sample) %&gt;% \n  summarise(LibrarySize = sum(counts)) %&gt;% \n  mutate(`Library Size - Millions of Reads` = LibrarySize / 1e6) %&gt;% \n  ggplot(aes(x=.sample, y = `Library Size - Millions of Reads`)) + geom_col(fill=\"steelblue\")\n\n\n\n\n\n\n\n\nIt looks very promising so far üéâ. The above were examples of using the standard tidyverse operations. The tidybulk package also has functions for implementing the steps in a standard RNA-seq workflow."
  },
  {
    "objectID": "posts/2025_10_30_tidybulk/index.html#filtering-to-expressed-features",
    "href": "posts/2025_10_30_tidybulk/index.html#filtering-to-expressed-features",
    "title": "Teaching an old dog new tricks; tidy analysis of RNA-seq",
    "section": "Filtering to expressed features",
    "text": "Filtering to expressed features\nGenes with extremely low read counts across all samples offer limited evidence of differential expression. These genes can introduce noise into the analysis, potentially affecting the accuracy of statistical methods used downstream. Furthermore, they increase the number of statistical tests performed, leading to a higher risk of false discoveries and reduced power to detect truly differentially expressed genes.\nTo mitigate these issues, we should filter out genes with insufficient read counts before proceeding with further analysis. This can be achieved using the keep_abundant or identify_abundant functions within the tidybulk package. These functions leverage the filterByExpr function from the edgeR package (Law et al., 2016), which effectively identifies genes with adequate read counts for reliable differential expression analysis. By default, this filter retains genes with at least 10 counts in a minimum number of samples, typically set to the size of the smallest group in the experimental design.\n\ncounts_filtered &lt;- caf_tidy %&gt;% keep_abundant(factor_of_interest=condition)"
  },
  {
    "objectID": "posts/2025_10_30_tidybulk/index.html#scaling-counts-to-normalise",
    "href": "posts/2025_10_30_tidybulk/index.html#scaling-counts-to-normalise",
    "title": "Teaching an old dog new tricks; tidy analysis of RNA-seq",
    "section": "Scaling counts to normalise",
    "text": "Scaling counts to normalise\nTo account for variations in sequencing depth and library composition across samples, we perform count scaling, often referred to as normalization. This process aims to remove systematic biases that do not reflect true biological differences.\nWithin the tidybulk package, the scale_abundance function generates scaled counts. Scaling factors are calculated based on abundant (filtered) transcripts and then applied to all transcripts.\nWe can select from various normalization methods. In this analysis, we will utilize the default method: trimmed mean of M values (TMM) as implemented in the edgeR package (Robinson and Oshlack, 2010).\nIt‚Äôs important to note that TMM normalization, like most scaling methods, scales the counts relative to a single reference sample\n\ncounts_scaled &lt;- counts_filtered %&gt;% scale_abundance()"
  },
  {
    "objectID": "posts/2025_10_30_tidybulk/index.html#dimensionality-reduction",
    "href": "posts/2025_10_30_tidybulk/index.html#dimensionality-reduction",
    "title": "Teaching an old dog new tricks; tidy analysis of RNA-seq",
    "section": "Dimensionality reduction",
    "text": "Dimensionality reduction\nMy next go-to method for RNA-seq is to use a PCA to visualise my data. Principal Component Analysis (PCA) and Multi-Dimensional Scaling (MDS) plots are among the most crucial visualizations for analyzing RNA-sequencing data as these techniques reduce the dimensionality of the data, allowing us to identify the primary sources of variation.\nThere is are a couple of recommended steps before running the PCA to filter out lowly-expressed genes and normalize for differences in sequencing depth (scale_abundance).\n\n# 1. Scale/Normalize the counts\ncaf_tidy_scaled &lt;- caf_tidy %&gt;%\n  # Filter out lowly expressed genes (recommended best practice)\n  identify_abundant() %&gt;%\n  keep_abundant(factor_of_interest = condition) %&gt;%\n  # Normalize for sequencing depth (creates a 'count_scaled' column)\n  # Default method is TMM (recommended for bulk RNA-seq)\n  scale_abundance()\n\nThe tidybulk package has several dimensionality reduction techniques available, so you can use whichever one you prefer. I‚Äôll stick with PCA and base the analysis on the 500 most-variable genes - which is the default in the plotPCA function of DESeq2.\n\nntop_genes &lt;- 500\n\npca_results &lt;- caf_tidy_scaled %&gt;%\n  reduce_dimensions(\n    method = \"PCA\",\n    .value = count_scaled,\n    top = ntop_genes,\n    transform = log1p \n  )\n\npca_results %&gt;% \n  slice_head(n = 10)\n\n# A tibble: 10 √ó 14\n   .feature     .sample counts Run   condition Name  Replicate Treated .abundant\n   &lt;chr&gt;        &lt;chr&gt;    &lt;int&gt; &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;   &lt;lgl&gt;    \n 1 ENSG0000000‚Ä¶ 1_CTR_‚Ä¶   1579 \"1_C‚Ä¶ CTR       CTR_1         1 N       TRUE     \n 2 ENSG0000000‚Ä¶ 1_CTR_‚Ä¶   1774 \"1_C‚Ä¶ CTR       CTR_1         1 N       TRUE     \n 3 ENSG0000000‚Ä¶ 1_CTR_‚Ä¶    698 \"1_C‚Ä¶ CTR       CTR_1         1 N       TRUE     \n 4 ENSG0000000‚Ä¶ 1_CTR_‚Ä¶    246 \"1_C‚Ä¶ CTR       CTR_1         1 N       TRUE     \n 5 ENSG0000000‚Ä¶ 1_CTR_‚Ä¶   6370 \"1_C‚Ä¶ CTR       CTR_1         1 N       TRUE     \n 6 ENSG0000000‚Ä¶ 1_CTR_‚Ä¶   4009 \"1_C‚Ä¶ CTR       CTR_1         1 N       TRUE     \n 7 ENSG0000000‚Ä¶ 1_CTR_‚Ä¶   1037 \"1_C‚Ä¶ CTR       CTR_1         1 N       TRUE     \n 8 ENSG0000000‚Ä¶ 1_CTR_‚Ä¶   1498 \"1_C‚Ä¶ CTR       CTR_1         1 N       TRUE     \n 9 ENSG0000000‚Ä¶ 1_CTR_‚Ä¶    285 \"1_C‚Ä¶ CTR       CTR_1         1 N       TRUE     \n10 ENSG0000000‚Ä¶ 1_CTR_‚Ä¶   3948 \"1_C‚Ä¶ CTR       CTR_1         1 N       TRUE     \n# ‚Ñπ 5 more variables: TMM &lt;dbl&gt;, multiplier &lt;dbl&gt;, counts_scaled &lt;dbl&gt;,\n#   PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;\n\n\nIf you print the pca_results object to screen you will notice that it is still in ‚Äúlong‚Äù/‚Äútidy‚Äù format, which on this occasion is not particularly useful for visualisation as many of the variables we need for plotting are repeated many times. To provide a simple summary of PC values for each sample we can use the pivot_sample function. Because we are already using the tidy format, the biological and experimental groups we might want to include on the plot are already available to us.\n\npca_results %&gt;% pivot_sample() %&gt;% \n  dplyr::select(.sample,condition,contains(\"PC\"))\n\n# A tibble: 9 √ó 4\n  .sample     condition    PC1   PC2\n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 1_CTR_BC_2  CTR        -6.07 11.1 \n2 2_TGF_BC_4  TGF       -20.6  -8.72\n3 3_IR_BC_5   IR         23.8  -3.27\n4 4_CTR_BC_6  CTR        -5.40 10.6 \n5 5_TGF_BC_7  TGF       -18.9  -7.34\n6 6_IR_BC_12  IR         21.3  -3.21\n7 7_CTR_BC_13 CTR        -1.78 12.8 \n8 8_TGF_BC_14 TGF       -16.7  -6.44\n9 9_IR_BC_15  IR         24.4  -5.46\n\n\nA basic PCA visualisation will show the values of PC1 and PC2 using a scatter plot with ggplot2. The results are pretty similar indeed to the DESeq2 equivalent workflow\n\npca_results %&gt;%\n    pivot_sample() %&gt;%\n    ggplot(aes(x=PC1, y=PC2, col = condition)) +\n    geom_point()"
  },
  {
    "objectID": "posts/2025_10_30_tidybulk/index.html#gene-annotations",
    "href": "posts/2025_10_30_tidybulk/index.html#gene-annotations",
    "title": "Teaching an old dog new tricks; tidy analysis of RNA-seq",
    "section": "Gene Annotations",
    "text": "Gene Annotations\nAt the moment we don‚Äôt have particularly meaningful gene names that we can use. We have an Ensembl ID, and have ways to convert between. One of which is using an organism-specific package in Bioconductor. First, we get all the IDs we have.\n\nens_ids &lt;- pull(caf_tidy, .feature) %&gt;% unique\n\nThe overall strategy is to use org.Hs.eg.db to convert between one type of ID (ENSEMBL in our case) to another. We can try the official gene symbol and gene name. For non-human data, equivalent packages are available. e.g.¬†org.Mm.eg.db for mouse.\n\nlibrary(org.Hs.eg.db)\nanno &lt;- AnnotationDbi::select(org.Hs.eg.db,\n                              keys = ens_ids,\n                              columns = c(\"SYMBOL\",\"GENENAME\"),\n                              keytype = \"ENSEMBL\")\nanno %&gt;% slice(1:10)\n\n           ENSEMBL SYMBOL\n1  ENSG00000000003 TSPAN6\n2  ENSG00000000005   TNMD\n3  ENSG00000000419   DPM1\n4  ENSG00000000457  SCYL3\n5  ENSG00000000460  FIRRM\n6  ENSG00000000938    FGR\n7  ENSG00000000971    CFH\n8  ENSG00000001036  FUCA2\n9  ENSG00000001084   GCLC\n10 ENSG00000001167   NFYA\n                                                      GENENAME\n1                                                tetraspanin 6\n2                                                  tenomodulin\n3  dolichyl-phosphate mannosyltransferase subunit 1, catalytic\n4                                     SCY1 like pseudokinase 3\n5    FIGNL1 interacting regulator of recombination and mitosis\n6               FGR proto-oncogene, Src family tyrosine kinase\n7                                          complement factor H\n8                                         alpha-L-fucosidase 2\n9                  glutamate-cysteine ligase catalytic subunit\n10                nuclear transcription factor Y subunit alpha"
  },
  {
    "objectID": "posts/2025_10_30_tidybulk/index.html#testing-for-differential-expression",
    "href": "posts/2025_10_30_tidybulk/index.html#testing-for-differential-expression",
    "title": "Teaching an old dog new tricks; tidy analysis of RNA-seq",
    "section": "Testing for differential expression",
    "text": "Testing for differential expression\nThe tidybulk package has simplified workflows to test for differential expression between different conditions. The workflow is not completely automated however because we still need to specify what sample groups to compare and which contrasts to make. This is achieved via the .formula argument. The .contrasts argument also allows us to explicitly define the direction of the contrast and which group to use as a baseline; which will affect the sign of the fold-change.\nNote that I am using the caf_tidy object here rather than the scaled version as DESeq2 will do it‚Äôs own normalization as part of the workflow.\n\ncounts_de &lt;- caf_tidy %&gt;% \n    test_differential_abundance(\n      .formula = ~ condition,\n      method = \"deseq2\",\n      .contrasts = list(c(\"condition\", \"TGF\",\"CTR\")),\n      ## As we're only doing one contrast, this will make the column names cleaner\n      omit_contrast_in_colnames = TRUE\n    )\n\nWarning: The `.contrasts` argument of `test_differential_abundance()` is deprecated as\nof tidybulk 1.7.4.\n‚Ñπ The argument .contrasts is now deprecated please use contrasts (without the\n  dot).\n\n\n=====================================\ntidybulk says: All testing methods use raw counts, irrespective of if scale_abundance\nor adjust_abundance have been calculated. Therefore, it is essential to add covariates\nsuch as batch effects (if applicable) in the formula.\n=====================================\nThis message is displayed once per session.\n\n\nWarning in eval(dots[[i]][[action]], env, env): tidybulk says: highly abundant\ntranscripts were not identified (i.e. identify_abundant()) or filtered (i.e.,\nkeep_abundant), therefore this operation will be performed on unfiltered data.\nIn rare occasions this could be wanted. In standard whole-transcriptome\nworkflows is generally unwanted.\n\n\nestimating size factors\nestimating dispersions\ngene-wise dispersion estimates\nmean-dispersion relationship\nfinal dispersion estimates\nfitting model and testing\ntidybulk says: to access the raw results (fitted GLM) do `attr(..., \"internals\")$DESeq2`\n\n\nRunning the code gives me the same messages printed to screen as if I was running the DESeq function (which it is clearly doing under the hood). There is also a warning about not filtering highly abundant transcripts. I didn‚Äôt think that removing such transcripts was required for DESeq2, but perhaps I will have to read up on that.\n\ncounts_de %&gt;% \n  slice_head(n = 10)\n\n# A tibble: 10 √ó 14\n   .feature      .sample counts Run   condition Name  Replicate Treated baseMean\n   &lt;chr&gt;         &lt;chr&gt;    &lt;int&gt; &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 ENSG00000000‚Ä¶ 1_CTR_‚Ä¶   1579 \"1_C‚Ä¶ CTR       CTR_1         1 N       1431.   \n 2 ENSG00000000‚Ä¶ 1_CTR_‚Ä¶      0 \"1_C‚Ä¶ CTR       CTR_1         1 N          0.114\n 3 ENSG00000000‚Ä¶ 1_CTR_‚Ä¶   1774 \"1_C‚Ä¶ CTR       CTR_1         1 N       1791.   \n 4 ENSG00000000‚Ä¶ 1_CTR_‚Ä¶    698 \"1_C‚Ä¶ CTR       CTR_1         1 N        641.   \n 5 ENSG00000000‚Ä¶ 1_CTR_‚Ä¶    246 \"1_C‚Ä¶ CTR       CTR_1         1 N        206.   \n 6 ENSG00000000‚Ä¶ 1_CTR_‚Ä¶     10 \"1_C‚Ä¶ CTR       CTR_1         1 N          7.41 \n 7 ENSG00000000‚Ä¶ 1_CTR_‚Ä¶   6370 \"1_C‚Ä¶ CTR       CTR_1         1 N       5318.   \n 8 ENSG00000001‚Ä¶ 1_CTR_‚Ä¶   4009 \"1_C‚Ä¶ CTR       CTR_1         1 N       3630.   \n 9 ENSG00000001‚Ä¶ 1_CTR_‚Ä¶   1037 \"1_C‚Ä¶ CTR       CTR_1         1 N        923.   \n10 ENSG00000001‚Ä¶ 1_CTR_‚Ä¶   1498 \"1_C‚Ä¶ CTR       CTR_1         1 N       1261.   \n# ‚Ñπ 5 more variables: log2FoldChange &lt;dbl&gt;, lfcSE &lt;dbl&gt;, stat &lt;dbl&gt;,\n#   pvalue &lt;dbl&gt;, padj &lt;dbl&gt;\n\n\nThe results are still in a long format table. Again, this is actually not very helpful and would prefer to have a single row for each gene tested. The function pivot_transcript performs the reshaping. We can now add the gene annotation we created earlier, but retain the original Ensembl IDs so we can retrieve count information. The final arrange line orders by significance.\n\nresults_table &lt;- counts_de %&gt;% \n  pivot_transcript() %&gt;% \n  left_join(anno, by=c(\".feature\"=\"ENSEMBL\")) %&gt;% \n  arrange(padj)\nresults_table %&gt;% slice_head(n = 10)\n\n# A tibble: 10 √ó 9\n   .feature      baseMean log2FoldChange  lfcSE  stat    pvalue      padj SYMBOL\n   &lt;chr&gt;            &lt;dbl&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt; \n 1 ENSG00000119‚Ä¶    6472.           2.37 0.0708  33.5 2.44e-246 4.37e-242 LTBP2 \n 2 ENSG00000120‚Ä¶   22002.           1.44 0.0590  24.3 9.25e-131 8.29e-127 TGFBI \n 3 ENSG00000172‚Ä¶    1482.           2.38 0.0984  24.2 1.11e-129 6.61e-126 LRRC15\n 4 ENSG00000049‚Ä¶     932.           4.00 0.176   22.8 7.23e-115 3.24e-111 ELN   \n 5 ENSG00000060‚Ä¶    1512.           2.12 0.0953  22.3 6.45e-110 2.31e-106 COL11‚Ä¶\n 6 ENSG00000115‚Ä¶  683349.           1.45 0.0687  21.2 2.18e- 99 6.52e- 96 FN1   \n 7 ENSG00000187‚Ä¶   30609.           1.46 0.0693  21.1 1.78e- 98 4.55e- 95 COL4A1\n 8 ENSG00000186‚Ä¶   14380.           1.47 0.0700  21.0 9.71e- 98 2.18e- 94 THBS2 \n 9 ENSG00000087‚Ä¶   51862.           1.28 0.0661  19.4 9.03e- 84 1.80e- 80 MMP2  \n10 ENSG00000139‚Ä¶    2302.           2.07 0.109   19.0 2.02e- 80 3.62e- 77 AMIGO2\n# ‚Ñπ 1 more variable: GENENAME &lt;chr&gt;\n\n\nWe can write the results to a spreadsheet for further investigation.\n\nwrite.csv(results_table, \"DESeq2_TGF_vs_CTR.csv\",quote=FALSE,row.names = FALSE)\n\n## Could remove counts_de if no longer needed\n## rm(counts_de)\n\nA volcano plot is a common visualisation that shows the degree of significance and magnitude of change. Genes of biological significance are likely to be those with low p-value and more extreme fold-change.\nIt would be perfectly possible to make the plot using standard ggplot2 code. However, the EnhancedVolcano package simplifies the process and offers some additional features such as automatically labeling the significant genes.\n\nlibrary(EnhancedVolcano)\nEnhancedVolcano(results_table, \n                lab = results_table$SYMBOL,\n                x = \"log2FoldChange\",\n                y = \"padj\")\n\n\n\n\n\n\n\n\nSome ‚Äúsanity checks‚Äù are always a good idea too. This can include visualising the scaled counts of the top genes to see if their significance is driven by biological effects, and not technical. First we get the names of the most significant genes. The names have to be in ensembl format as we are going to retrieve information on these from the counts data - which has ensembl as an identifier.\n\nN &lt;- 10\ntop_genes &lt;- slice_min(results_table, padj, n=N) %&gt;% pull(.feature)\ntop_genes\n\n [1] \"ENSG00000119681\" \"ENSG00000120708\" \"ENSG00000172061\" \"ENSG00000049540\"\n [5] \"ENSG00000060718\" \"ENSG00000115414\" \"ENSG00000187498\" \"ENSG00000186340\"\n [9] \"ENSG00000087245\" \"ENSG00000139211\"\n\n\nThe annotation information is included to allow more meaningful labels.\n\nplot_data &lt;- caf_tidy %&gt;% \n  scale_abundance() %&gt;% \n  filter(.feature %in% top_genes) %&gt;% \n  left_join(anno, by = c(\".feature\" = \"ENSEMBL\"))\n\nA series of boxplots can now be created, with a facet introduced so that a separate plot is made for each gene.\n\nplot_data %&gt;% \n  ggplot(aes(x = condition, y = counts_scaled,col=condition)) + geom_jitter(width = 0.1) + scale_y_log10() + facet_wrap(~SYMBOL,scales=\"free_y\")\n\n\n\n\n\n\n\n\nYou can use the results_table to carry out your enrichment or pathways analysis in your usual workflow. However, I note that tidybulk includes test_gene_enrichment and test_gene_overrepresentation functions that I expect will automate this as part of the same workflow."
  },
  {
    "objectID": "posts/2025_11_06_tidymodels_TCGA_part1/index.html",
    "href": "posts/2025_11_06_tidymodels_TCGA_part1/index.html",
    "title": "Tidymodels for omics data: Part 1",
    "section": "",
    "text": "This will be the first in a series where I look at machine learning techniques applied to omics data. However, before we get ahead of ourselves we‚Äôll need some data. I decided to use breast cancer samples available through The Cancer Genome Atlas (TCGA).\nWe need the TCGAbiolinks package that will do most of the work of downloading, and tidybulk for manipulation. Some other packages are required for data exploration.\n\nif(!require(BiocManager)) install.packages(\"BiocManager\")\n\nif(!require(TCGAbiolinks)) BiocManager::install(\"TCGAbiolinks\")\nif(!require(SummarizedExperiment)) BiocManager::install(\"SummarizedExperiment\")\nif(!require(tidybulk)) BiocManager::install(\"tidybulk\")\n\nif(!require(dplyr)) install.packages(\"dplyr\")\nif(!require(ggplot2)) install.packages(\"ggplot2\")\nif(!require(tidyr)) install.packages(\"tidyr\")\n\n## needed for some example survival analysis\nif(!require(survminer)) install.packages(\"survminer\")\n\n## required for heatmap example\nif(!require(tidyHeatmap)) install.packages(\"tidyHeatmap\")"
  },
  {
    "objectID": "posts/2025_11_06_tidymodels_TCGA_part1/index.html#pre-amble",
    "href": "posts/2025_11_06_tidymodels_TCGA_part1/index.html#pre-amble",
    "title": "Tidymodels for omics data: Part 1",
    "section": "",
    "text": "This will be the first in a series where I look at machine learning techniques applied to omics data. However, before we get ahead of ourselves we‚Äôll need some data. I decided to use breast cancer samples available through The Cancer Genome Atlas (TCGA).\nWe need the TCGAbiolinks package that will do most of the work of downloading, and tidybulk for manipulation. Some other packages are required for data exploration.\n\nif(!require(BiocManager)) install.packages(\"BiocManager\")\n\nif(!require(TCGAbiolinks)) BiocManager::install(\"TCGAbiolinks\")\nif(!require(SummarizedExperiment)) BiocManager::install(\"SummarizedExperiment\")\nif(!require(tidybulk)) BiocManager::install(\"tidybulk\")\n\nif(!require(dplyr)) install.packages(\"dplyr\")\nif(!require(ggplot2)) install.packages(\"ggplot2\")\nif(!require(tidyr)) install.packages(\"tidyr\")\n\n## needed for some example survival analysis\nif(!require(survminer)) install.packages(\"survminer\")\n\n## required for heatmap example\nif(!require(tidyHeatmap)) install.packages(\"tidyHeatmap\")"
  },
  {
    "objectID": "posts/2025_11_06_tidymodels_TCGA_part1/index.html#downloading-the-data",
    "href": "posts/2025_11_06_tidymodels_TCGA_part1/index.html#downloading-the-data",
    "title": "Tidymodels for omics data: Part 1",
    "section": "Downloading the data",
    "text": "Downloading the data\nFirst of all, we need to decide what dataset we want to download. The Cancer Genome Atlas (TCGA) studies have unique codes (e.g.¬†breast cancer is ‚ÄúTCGA-BRCA‚Äù) and if you don‚Äôt know the code you can query it. TCGA-BRCA is one of the first studies returned.\nN.B. I‚Äôm only showing the first few here.\n\nlibrary(TCGAbiolinks)\n# Retrieve the full list of projects\nall_gdc_projects &lt;- getGDCprojects()\n\n# View the result\n# The output is typically a data frame or tibble\nall_gdc_projects |&gt;\n  dplyr::select(id, name) |&gt;\n  head(n = 5)\n\n           id                                                           name\n1 CTSP-DLBCL1         CTSP Diffuse Large B-Cell Lymphoma (DLBCL) CALGB 50303\n2   TCGA-BRCA                                      Breast Invasive Carcinoma\n3   TCGA-LUAD                                            Lung Adenocarcinoma\n4     CPTAC-3     CPTAC-Brain, Head and Neck, Kidney, Lung, Pancreas, Uterus\n5 APOLLO-LUAD APOLLO1: Proteogenomic characterization of lung adenocarcinoma\n\n\nTo see all the data interactively within RStudio you can use the View function.\n\nall_gdc_projects |&gt;\n  dplyr::select(id, name) |&gt; View()\n\nWhat data are available for breast cancer?\n\ngetProjectSummary(\"TCGA-BRCA\")\n\n$file_count\n[1] 70776\n\n$data_categories\n   file_count case_count                data_category\n1       21134       1098  Simple Nucleotide Variation\n2        9282       1098             Sequencing Reads\n3        5317       1098                  Biospecimen\n4        2288       1098                     Clinical\n5       14346       1098        Copy Number Variation\n6        4876       1097      Transcriptome Profiling\n7        3714       1097              DNA Methylation\n8         919        881           Proteome Profiling\n9        3128        927 Somatic Structural Variation\n10       5772       1098         Structural Variation\n\n$case_count\n[1] 1098\n\n$file_size\n[1] 6.249966e+14\n\n\nWe‚Äôll first obtain some clinical information for the TCGA breast cancer samples. The function we will eventually use to download counts will give us some information about the samples, but not everything we would want. For example, in my initial development of these materials I noticed the Estrogen Receptor (ER) status was missing which is a vital clinical indicator variable. For analyses that use site-specific characteristis (e.g.¬†breast in this case), this kind of clinical information is more useful. Detailed clinical information can be downloaded with the following command:-\n\nproject ‚ÄúTCGA-BRCA‚Äù specifies the cancer cohort and must match one of the IDs returned by getGDCprojects\ndata.category ‚ÄúClinical‚Äù specifies the type of data. In this case information about the patients and their disease.\ndata.type ‚ÄúClinical Supplement‚Äù specifies a subset of clinical data: This often points to highly detailed or manually curated clinical record\ndata.format ‚ÄúBCR Biotab‚Äù files are comprehensive, non-standardized tables created by the Biospecimen Core Resource (BCR) and can be converted into a data frame for manipulation in R.\n\n\nquery &lt;- GDCquery(project = \"TCGA-BRCA\",\n                  data.category = \"Clinical\",\n                  data.type = \"Clinical Supplement\",\n                  data.format = \"BCR Biotab\")\n\nThe function we have used only get the download ready and doesn‚Äôt actually fetch any data. To actually download the data we need GDCdownload.\n\nGDCdownload(query)\n\nThis downloads the raw files to our computer. If we want the data in a usable form in R we have to use a final function GDCprepare. In this case it gives a list of data frames. Functions such as head and colnames could be used to inspect the contents of each (I‚Äôm using the base lapply as I don‚Äôt want to get into purrrin this particular tutorial). I‚Äôm looking for something about ER status and it looks like the best bet is clinical_patient_brca. We will save this as a data frame for use later.\nN.B. I‚Äôm not showing the output as it‚Äôs quite lengthy.\n\nlibrary(TCGAbiolinks)\nclinical.all &lt;- GDCprepare(query)\nlapply(clinical.all, head)\nlapply(clinical.all, colnames)\n## tidyverse equivlent\n## purrr::map(clinical.all, head)\n\ntcga_brca.clin &lt;- clinical.all$clinical_patient_brca\nreadr::write_tsv(tcga_brca.clin, \"tcga_brca.clin.tsv\")\n\nTo get the transcriptomic data we are going to follow a similar approach using the GDCquery, GDCdownload and GDCprepare functions. This will take a lot longer depending on your network connection. The output of GDCprepare in this case is a SummarizedExperiment object which is a standard data type for RNA-seq. As a final step in the code I save the SummarizedExperiment to disk and check if this file exists before starting. This makes sure I don‚Äôt repeat the lengthy download.\nFor my transcriptomic data I am choosing the STAR - Counts option and requesting tumours only (TCGA also holds data for ‚Äúnormal‚Äù/healthy tissue). If repeating these steps on a different disease you may need to change these options.\n\n\n\n\n\n\nNote\n\n\n\nIf you are interested in other types of data (Proteome, Methylation, Exome,‚Ä¶) the general procedure is the same as presented here, but the values for workflow.type and data.type will need to be different. The TCGAbiolinks webpage has many examples of downloading data of different types.\n\n\nTCGAbiolinks will make use of a temporary folder to download file to. I noticed that on my Windows laptop the names of these temporary folders were extremely long and Windows seems to have a limit on a file name. Hence in the following code it will make use of a specific temporary folder. Otherwise on Mac or Unix it will download to your working directory.\n\n# 1. Load necessary packages\n\nlibrary(SummarizedExperiment)\nlibrary(TCGAbiolinks)\n\nif(!file.exists(\"brca.data_full.rds\")){\n\n# --- Define the Query to include just the Tumour samples ---\nquery.brca_tumor &lt;- GDCquery(\n  project = \"TCGA-BRCA\",\n  data.category = \"Transcriptome Profiling\",\n  data.type = \"Gene Expression Quantification\",\n  workflow.type = \"STAR - Counts\",\n  # Request \"Primary Tumor only\n  sample.type =\"Primary Tumor\"\n)\n\n## I'm on Windows and had to change the temporary directory\n\nif(.Platform$OS.type == \"windows\")\n  GDCdownload(query.brca_tumor,directory = \"C:\\\\tmp\")\nelse GDCdownload(query.brca_tumor)\n\n# --- Prepare the Data ---\n# This converts the downloaded files into a single SummarizedExperiment object\nif(.Platform$OS.type == \"windows\") GDCprepare(query = query.brca_tumor,directory = \"C:\\\\tmp\")\nelse brca.data_full &lt;- GDCprepare(query = query.brca_tumor)\n\nsaveRDS(brca.data_full, \"brca.data_full.rds\")\n} else {\n  brca.data_full &lt;- readRDS(\"brca.data_full.rds\")\n}\n\nThe object, for me at least is 653.44MB. We going to see how to reduce the amount of data we are working with.\n\nbrca.data_full\n\nclass: RangedSummarizedExperiment \ndim: 60660 1111 \nmetadata(1): data_release\nassays(6): unstranded stranded_first ... fpkm_unstrand fpkm_uq_unstrand\nrownames(60660): ENSG00000000003.15 ENSG00000000005.6 ...\n  ENSG00000288674.1 ENSG00000288675.1\nrowData names(10): source type ... hgnc_id havana_gene\ncolnames(1111): TCGA-EW-A2FS-01A-11R-A17B-07\n  TCGA-OL-A6VR-01A-32R-A33J-07 ... TCGA-E2-A1IU-01A-11R-A14D-07\n  TCGA-D8-A1XS-01A-11R-A14M-07\ncolData names(94): barcode patient ... paper_PARADIGM Clusters\n  paper_Pan-Gyn Clusters\n\n\nFirstly, this object contains many types of ‚Äúcount‚Äù, and normalised versions thereof. We‚Äôre only going to need one of these for our purposes, and each one we keep is going to make our data dramatically larger.\n\nassayNames(brca.data_full)\n\n[1] \"unstranded\"       \"stranded_first\"   \"stranded_second\"  \"tpm_unstrand\"    \n[5] \"fpkm_unstrand\"    \"fpkm_uq_unstrand\"\n\n\nThe unstranded assay column contains the integer count of reads mapped to each gene, summed across both the forward and reverse strands. This is the closest representation of the true number of RNA fragments sequenced for that gene. If our main priority was for differential expression (DE), we would choose these values. However, we will prefer the fpkm_uq_unstrand as these are better suited for cohort-level visualisation. The data have already undergone some normalisation make values comparable across samples. The ‚ÄúFPKM‚Äù stands for the standard normalisation of sequencing depth (per millions reaads) and gene length, whereas UQ means ‚ÄúUpper-Quartile‚Äù scaling to reduce the influence of extreme outliers.\nWe‚Äôll create a new SummarizedExperiment object from scratch that will hold just these data. We can also take the opportunity to join the sample information included with the SummarizedExperiment as standard, with the data we queried.\n\ntcga_brca.clin &lt;- readr::read_tsv(\"tcga_brca.clin.tsv\", show_col_types = FALSE)\n\nfpkm_uq_assay &lt;- assay(brca.data_full, \"fpkm_uq_unstrand\")\n\n# get the rowData (gene information) and colData (sample information)\n# These components are used directly from the original object\ngene_info &lt;- rowData(brca.data_full)\nsample_info &lt;- colData(brca.data_full) |&gt;\n  data.frame() |&gt;\n  dplyr::left_join(tcga_brca.clin, by = c(\"patient\"=\"bcr_patient_barcode\"))\n \n# create a new SummarizedExperiment object\nbrca_fpkm_uq_unstrand &lt;- SummarizedExperiment(\n  assays = SimpleList(fpkm_uq = fpkm_uq_assay),\n  rowData = gene_info,\n  colData = sample_info\n)\n\n## print the output to check\n\nbrca_fpkm_uq_unstrand\n\nclass: SummarizedExperiment \ndim: 60660 1111 \nmetadata(0):\nassays(1): fpkm_uq\nrownames(60660): ENSG00000000003.15 ENSG00000000005.6 ...\n  ENSG00000288674.1 ENSG00000288675.1\nrowData names(10): source type ... hgnc_id havana_gene\ncolnames(1111): TCGA-EW-A2FS-01A-11R-A17B-07\n  TCGA-OL-A6VR-01A-32R-A33J-07 ... TCGA-E2-A1IU-01A-11R-A14D-07\n  TCGA-D8-A1XS-01A-11R-A14M-07\ncolData names(205): barcode patient ... tissue_source_site\n  tumor_tissue_site\n\n# save to disk\nsaveRDS(brca_fpkm_uq_unstrand, \"brca_fpkm_uq_unstrand.rds\")\nrm(fpkm_uq_assay)\n\nSince we don‚Äôt need the object we originally created we can delete it from memory in R.\n\nrm(brca.data_full)"
  },
  {
    "objectID": "posts/2025_11_06_tidymodels_TCGA_part1/index.html#reducing-the-amount-of-data",
    "href": "posts/2025_11_06_tidymodels_TCGA_part1/index.html#reducing-the-amount-of-data",
    "title": "Tidymodels for omics data: Part 1",
    "section": "Reducing the amount of data",
    "text": "Reducing the amount of data\nWe are not going to be running any machine learning methods or visualisation on the data we have now. More than anything else, it would take far too long to run and require huge amounts of compute resource. The data consists of 60660 ‚Äúgenes‚Äù and 1111 samples. Considering the genes first of all, not all of them are going to be informative and will only contribute noise to our data making any machine learning more complicated. In a previous post I introduced the tidybulk package for dealing with RNA-seq in a manner consistent with the tidyverse framework in R.\n\nlibrary(tidybulk)\n\nThe package I will be eventually using (SPOILER ALERT:tidymodels) has some pre-processing steps for removing uninformative features that are usually recommended. However, I am going to revert to some domain knowledge and use some functionality from tidybulk which are specifically developed for RNA-seq. First of all, we can remove genes that are lowly-expressed using the keep_abundant function. I played around with the settings for keep_abundant here as the defaults were a bit too aggressive. On inspecting the names of the genes retained (see commented code below) I found that PTEN and ESR1 were no longer in my data using the default, and from my knowledge of breast cancer I figure these are important.\n\n##NB using the base pipe |&gt; here as we haven't loaded tidyverse yet\n\nbrca_fpkm_uq_unstrand_expressed &lt;- brca_fpkm_uq_unstrand |&gt;\n  keep_abundant(minimum_counts = 0.5,           # Lower the expression threshold\n    minimum_proportion = 0.025     # Lower the proportion of samples required)\n)\n\nrm(brca_fpkm_uq_unstrand)\n\n# uncomment to inspect the genes retained\n# rowData(brca_fpkm_uq_unstrand_expressed) |&gt; data.frame() |&gt; View()\n\nThe reduction in the number of genes is quite dramatic - but this is a good thing for our purposes as it will make things run quicker. The next step is to remove genes that do not have enough variability in the data. Variability is important as genes whose expression level stays the same across samples are not going to be very informative for distinguishing different subtypes. However, in the following code I will keep the names of the variable genes, but not actually do the filtering yet. Conveniently, tidybulk allows the usage of the pipe (|&gt; or %&gt;%) so we don‚Äôt have to create any new variables. Explicitly creating a tidybulk object means we can use dplyr operations such as pulling the gene_name name.\n\nvar_genes &lt;- brca_fpkm_uq_unstrand_expressed |&gt;\n  keep_variable(top = 2000) |&gt;\n  tidybulk() |&gt;\n  filter(!duplicated(gene_name)) |&gt;\n  dplyr::pull(gene_name)\n\nvar_genes[1:10]\n\n [1] \"SCGB2A2\"    \"SCGB1D2\"    \"TFF1\"       \"PIP\"        \"CPB1\"      \n [6] \"AC093001.1\" \"S100A7\"     \"CLEC3A\"     \"LTF\"        \"IGHG1\"     \n\n\nI‚Äôd like my data to be somewhat informed by biology, so will also use a set of Cancer genes. I got the set of genes following from OncoKB\n\nif(!file.exists(\"cancerGeneList.tsv\")) download.file(\"https://raw.githubusercontent.com/markdunning/markdunning.github.com/refs/heads/master/posts/2025_11_06_tidymodels_TCGA_part1/cancerGeneList.tsv\", destfile = \"cancerGeneList.tsv\")\ncancer_genes &lt;- read.delim(\"cancerGeneList.tsv\")\ncancer_genes |&gt; head(n=10)\n\n   Hugo.Symbol Entrez.Gene.ID  GRCh37.Isoform  GRCh37.RefSeq  GRCh38.Isoform\n1         ABL1             25 ENST00000318560    NM_005157.4 ENST00000318560\n2         AKT1            207 ENST00000349310 NM_001014431.1 ENST00000349310\n3          ALK            238 ENST00000389048    NM_004304.4 ENST00000389048\n4        AMER1         139285 ENST00000330258    NM_152424.3 ENST00000374869\n5          APC            324 ENST00000257430    NM_000038.5 ENST00000257430\n6           AR            367 ENST00000374690    NM_000044.3 ENST00000374690\n7       ARID1A           8289 ENST00000324856    NM_006015.4 ENST00000324856\n8        ASXL1         171023 ENST00000375687    NM_015338.5 ENST00000375687\n9          ATM            472 ENST00000278616    NM_000051.3 ENST00000278616\n10        ATRX            546 ENST00000373344    NM_000489.3 ENST00000373344\n    GRCh38.RefSeq Gene.Type X..of.occurrence.within.resources..Column.J.P.\n1     NM_005157.4  ONCOGENE                                              7\n2  NM_001014431.1  ONCOGENE                                              7\n3     NM_004304.4  ONCOGENE                                              7\n4     NM_152424.3       TSG                                              7\n5     NM_000038.5       TSG                                              7\n6     NM_000044.3  ONCOGENE                                              7\n7     NM_006015.4       TSG                                              7\n8     NM_015338.5       TSG                                              7\n9     NM_000051.3       TSG                                              7\n10    NM_000489.3       TSG                                              7\n   OncoKB.Annotated MSK.IMPACT MSK.HEME FOUNDATION.ONE FOUNDATION.ONE.HEME\n1               Yes        Yes      Yes            Yes                 Yes\n2               Yes        Yes      Yes            Yes                 Yes\n3               Yes        Yes      Yes            Yes                 Yes\n4               Yes        Yes      Yes            Yes                 Yes\n5               Yes        Yes      Yes            Yes                 Yes\n6               Yes        Yes      Yes            Yes                 Yes\n7               Yes        Yes      Yes            Yes                 Yes\n8               Yes        Yes      Yes            Yes                 Yes\n9               Yes        Yes      Yes            Yes                 Yes\n10              Yes        Yes      Yes            Yes                 Yes\n   Vogelstein COSMIC.CGC..v99.\n1         Yes              Yes\n2         Yes              Yes\n3         Yes              Yes\n4         Yes              Yes\n5         Yes              Yes\n6         Yes              Yes\n7         Yes              Yes\n8         Yes              Yes\n9         Yes              Yes\n10        Yes              Yes\n                                           Gene.Aliases\n1                                      ABL, JTK7, c-ABL\n2                       AKT, PKB, PRKBA, RAC, RAC-alpha\n3                                                 CD246\n4                 FAM123B, FLJ39827, RP11-403E24.2, WTX\n5                                        DP2.5, PPP1R46\n6                 AIS, DHTR, HUMARA, NR3C4, SBMA, SMAX1\n7  B120, BAF250, BAF250a, C10rf4, C1orf4, P270, SMARCF1\n8                                              KIAA0978\n9                      ATA, ATC, ATD, ATDC, TEL1, TELO1\n10                          JMS, MRX52, RAD54, XH2, XNP\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you would prefer a purely data-driven approach that doesn‚Äôt use any prior knowledge you could instead use the result of keep_variable as your filtered object.\n\nbrca_gene_filtered &lt;- brca_fpkm_uq_unstrand_expressed |&gt;\n  keep_variable(top = 2000) \n\n\n\nNow we have a set of gene names that we want to keep in the analysis. The filtering this time is more efficient on the SummarizedExperiment object.\n\ncancer_gene_names &lt;- cancer_genes$Hugo.Symbol\n\n\nto_keep &lt;- rowData(brca_fpkm_uq_unstrand_expressed)$gene_name %in% c(cancer_gene_names, var_genes)\n\nbrca_gene_filtered &lt;- brca_fpkm_uq_unstrand_expressed[to_keep,]\n\nbrca_gene_filtered\n\nclass: SummarizedExperiment \ndim: 3119 1111 \nmetadata(0):\nassays(1): fpkm_uq\nrownames(3119): ENSG00000002016.18 ENSG00000002834.18 ...\n  ENSG00000287906.1 ENSG00000287914.1\nrowData names(11): source type ... havana_gene .abundant\ncolnames(1111): TCGA-EW-A2FS-01A-11R-A17B-07\n  TCGA-OL-A6VR-01A-32R-A33J-07 ... TCGA-E2-A1IU-01A-11R-A14D-07\n  TCGA-D8-A1XS-01A-11R-A14M-07\ncolData names(205): barcode patient ... tissue_source_site\n  tumor_tissue_site\n\nsaveRDS(brca_gene_filtered, \"brca_gene_filtered_SE.rds\")"
  },
  {
    "objectID": "posts/2025_11_06_tidymodels_TCGA_part1/index.html#some-data-exploration",
    "href": "posts/2025_11_06_tidymodels_TCGA_part1/index.html#some-data-exploration",
    "title": "Tidymodels for omics data: Part 1",
    "section": "Some data Exploration",
    "text": "Some data Exploration\nBefore embarking on any machine learning (which we will tackle next time), it is worth spending some time getting familiar with your data. Armed with some domain knowledge we can run a few basic checks. If you haven‚Äôt worked with RNA-seq data in R before, it might be worth taking a look over these examples.\nWhen working on breast cancer, and coming from a purely computational background, I quickly learnt about the importance of Estrogen Receptor (ER) status and the gene that encodes the Estrogen Receptor protein ESR1. The status of a breast cancer patient is a well-established biomarker and can indicate whether the patient‚Äôs cancer is likely to respond to hormone-blocking therapies. In other words, patients that can been categorised as ER positive should have higher expression of ESR1. In this dataset ER status looks to have been investigated by Immunohistochemistry (IHC) and recorded in the er_status_by_ihc. This is one of the variables present in the BCR clinical data.\nTo get the data for just ESR1 It‚Äôs actually more efficient in this case to filter the gene from the SummarizedExperiment object before calling tidybulk. I‚Äôm not sure if this is the intended workflow, but our data are so large that calling tidybulk on the entire dataset takes too much RAM in my case.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidybulk)\n\nbrca_fpkm_uq_unstrand[rowData(brca_fpkm_uq_unstrand)$gene_name == \"ESR1\"] |&gt;\n  tidybulk() |&gt;\n  ggplot(aes(x = er_status_by_ihc, y = fpkm_uq)) + geom_boxplot() + geom_jitter(width = 0.1)\n\n\n\n\n\n\n\n\nPhew! So ESR1 expression is indeed higher in ER positive samples. The plot also highlights some discrepancy in the recording of the ER status which should be cleaned prior to analysis. For the sake of the plot we can filter out the offending entries.\n\nbrca_fpkm_uq_unstrand[rowData(brca_fpkm_uq_unstrand)$gene_name == \"ESR1\"] %&gt;% \n  tidybulk() %&gt;% \n  filter(er_status_by_ihc %in% c(\"Positive\", \"Negative\")) %&gt;% \n  ggplot(aes(x = er_status_by_ihc, y = fpkm_uq)) + geom_boxplot() + geom_jitter(width = 0.1) + xlab(\"ER Status\")\n\n\n\n\n\n\n\n\nIn a similar vein, the gene ERBB2 should be higher in patients that are positive for Her2 staining by IHC.\n\nbrca_fpkm_uq_unstrand[rowData(brca_fpkm_uq_unstrand)$gene_name == \"ERBB2\"] %&gt;% \n  tidybulk() %&gt;% \n  filter(her2_status_by_ihc %in% c(\"Positive\", \"Negative\")) %&gt;% \n  ggplot(aes(x = her2_status_by_ihc, y = fpkm_uq)) + geom_boxplot() + geom_jitter() + scale_y_log10()\n\n\n\n\n\n\n\n\nBreast cancer was traditionally classified into five intrinsic subtypes, and microarray studies sought to identify genes that define these subtypes. On such 50 gene panel was derived using a prediction analysis of microarray (PAM) and hence called PAM50. The samples in the TCGA dataset have been classified using this method, and we can check the expression level of ESR1 and ERBB2 respectively.\n\nbrca_fpkm_uq_unstrand[rowData(brca_fpkm_uq_unstrand)$gene_name %in% c(\"ESR1\",\"ERBB2\")] %&gt;% \n  tidybulk() %&gt;% \n  filter(!is.na(paper_BRCA_Subtype_PAM50)) %&gt;% \n  ggplot(aes(x = paper_BRCA_Subtype_PAM50, y = fpkm_uq, fill = paper_BRCA_Subtype_PAM50)) + \n  geom_boxplot() + \n  geom_jitter(alpha=0.1) + \n  scale_y_log10() + \n  facet_wrap(~gene_name) + \n  scale_fill_manual(values = c(\"red\", \"pink\", \"darkblue\", \"lightblue\", \"yellow\"))\n\n\n\n\n\n\n\n\nAgain, this all makes perfect sense as Luminal A and Luminal B are ER-positive groups, Basal and Her2 are ER-negative and ERBB2 should be highly expressed in the Her2 subtype.\nSo, are the Pam50 subtypes are major source of variation in our data? The tidybulk package allows us to run dimensionality reduction techniques such as PCA to look at this issue. The reduce_dimensions function computes the Principal Components and these can be added to our data ready for visualisation. Normally tidybulk would require a scale_abundance step here to correct for differences in sequencing depth between samples, but since we are using values that are already normalised to some extent this is not needed on this ocassion.\n\nbrca_fpkm_uq_unstrand |&gt;\n  reduce_dimensions(method = \"PCA\")  |&gt; \n  ## not needed here: scale_abundance() |&gt;\n  pivot_sample()  |&gt;\n  select(.sample, PC1, PC2)\n\n# A tibble: 1,111 √ó 3\n   .sample                         PC1    PC2\n   &lt;chr&gt;                         &lt;dbl&gt;  &lt;dbl&gt;\n 1 TCGA-EW-A2FS-01A-11R-A17B-07  9.35  -8.84 \n 2 TCGA-OL-A6VR-01A-32R-A33J-07 -6.26  -4.46 \n 3 TCGA-E9-A226-01A-21R-A157-07 -8.62   4.47 \n 4 TCGA-A8-A08H-01A-21R-A00Z-07  0.362  5.87 \n 5 TCGA-D8-A27H-01A-11R-A16F-07  7.76  16.8  \n 6 TCGA-D8-A3Z6-01A-11R-A239-07 -5.76  -5.94 \n 7 TCGA-B6-A1KN-01A-11R-A13Q-07  0.728  0.144\n 8 TCGA-BH-A0DL-01A-11R-A115-07 20.8    6.48 \n 9 TCGA-A8-A09X-01A-11R-A00Z-07 13.6   -4.60 \n10 TCGA-BH-A2L8-01A-11R-A18M-07  5.41  -7.91 \n# ‚Ñπ 1,101 more rows\n\n\nIt is now straightforward to plot, and also colour according to the PAM50 subtype of each sample.\n\nbrca_fpkm_uq_unstrand |&gt; \n  reduce_dimensions(method = \"PCA\")  |&gt;\n  pivot_sample()  |&gt;\n  filter(!is.na(paper_BRCA_Subtype_PAM50))  |&gt;\n  ggplot(aes(x = PC1, y = PC2, col = paper_BRCA_Subtype_PAM50 )) + \n  geom_point()  + \n  scale_color_manual(values = c(\"red\", \"pink\", \"darkblue\", \"lightblue\", \"yellow\"))\n\n\n\n\n\n\n\n\n\nA heatmap of genes from the literature\nThe PCA looks pretty good, with the main separation being between basal and Luminal A/B. Furthermore, we can get from the literature the names of the genes used in the PAM50 classifier. I got these from the genefu Bioconductor package and saved as a small text file. My previous go-to package for heatmaps was pheatmap, but whilst writing this up I came across tidyHeatmap\n\ntidyHeatmap\n\nThis package works quite well with our workflow of using a ‚Äútidy‚Äù framework. It requires data in a tbl_df or tibble format, which we can create from tidybulk once we are done with pre-processing the data. Although the data we are working with are normalised, we must apply a log\\(_2\\) transformation for the purposes of visualisation. I‚Äôve also applied some basic data cleaning\nThe tidyHeatmap function heatmap allows us to quickly add sample annotations to the plot, and since our data were using the tidy framework we have plenty of clinical variables at our fingertips. I‚Äôm choosing ER status and PAM50 classification, but you could add others such age, grade, or anything else.\n\nif(!file.exists(\"PAM50_genes.txt\")) download.file(\"https://raw.githubusercontent.com/markdunning/markdunning.github.com/refs/heads/master/posts/2025_11_06_tidymodels_TCGA_part1/PAM50_genes.txt\", destfile = \"PAM50_genes.txt\")\n\npam50_genes &lt;- read.table(\"PAM50_genes.txt\")[,1]\n\nlibrary(tidyHeatmap)\n\n brca_fpkm_uq_unstrand[rowData(brca_fpkm_uq_unstrand)$gene_name %in% pam50_genes,] %&gt;% \n  ## once filtered, make a \"tidy\" version\n  tidybulk() %&gt;% \n  mutate(logcount = log2(fpkm_uq + 1)) %&gt;% \n  ## get the counts, samples and genes\n  rename(ER = er_status_by_ihc, PAM50 =paper_BRCA_Subtype_PAM50) %&gt;%\n  as_tibble() %&gt;% \n  mutate(ER = ifelse(ER %in% c(\"Positive\", \"Negative\"), ER, NA)) %&gt;% \n  filter(!is.na(ER), !is.na(PAM50)) %&gt;% \n  tidyHeatmap::heatmap(\n        .column = .sample,\n        .row = gene_name,\n        .value = logcount,   \n        scale = \"row\",\n        palette_value = circlize::colorRamp2(\n            seq(-5, 5, length.out = 11), \n            RColorBrewer::brewer.pal(11, \"RdBu\")\n        )\n    ) %&gt;% \n  annotation_tile(ER ,palette = c(\"orange\",\"purple\")) %&gt;% \n  ## I couldn't work out how to assign particular colours, so had to experiment at bit with the correct order to specify the colours in\n  annotation_tile(PAM50,palette = c(\"darkblue\", \"pink\", \"lightblue\", \"yellow\", \"red\"))\n\n\n\n\n\n\n\n\nShowing the data in this manner also us to see which genes look to be over- or under-expressed in certain subtypes. Eventually, we will progress to doing our own classification so it will be useful to compare to this information.\n\n\nSurvival Analysis\nFinally, a pertinent question to ask is whether the subtypes are clinically-meaningful. If we know what subtype a patient has, what can this tell us about their survival prospects? A kaplan-meier (KM) curve is a standard way to estimate and visualize the survival probability over a specified period, and can be created by the survival package and visualised using the package survminer. The curve starts at 100% (or probability 1.0) at time zero and steps down each time a patient dies. A tick mark on the curve indicates a patient who was lost to follow-up or was still alive when the data was collected (censored). These patients contribute to the probability calculation up until the time they were last observed.\nThe clinical data we have needs modifying slightly to run the analysis. In particular we need to create ‚ÄúStatus‚Äù and ‚ÄúTime‚Äù columns which records if a patient is alive (0) or dead (1) at the end of the study, and the time to either death or total follow-up time. We will also remove any patients where the Time or status is missing or inaccurate. The dataset also includes some metastatic or recurrent tumours, but for the purposes of the curve we will use only ‚Äúprimary‚Äù samples.\n\nlibrary(survminer)\nlibrary(survival)\nclin_data_corrected &lt;- colData(brca_fpkm_uq_unstrand) %&gt;% \n  data.frame() %&gt;% \n  dplyr::filter(classification_of_tumor == \"primary\") %&gt;% \n  #Define Time (Coalesce death date and follow-up date)\n  dplyr::mutate(\n    # Use days_to_death if available; otherwise, use days_to_last_follow_up\n    Time = as.numeric(dplyr::coalesce(days_to_death, days_to_last_follow_up)),\n    # Define Status \n    # Event=1 (Dead), Censored=0 (Alive)\n    Status = ifelse(vital_status.x == \"Dead\", 1, 0)\n  ) |&gt;\n  # Remove rows with NA in Time, Status, or Subtype\n  dplyr::filter(!is.na(Time), Time &gt; 0, Status %in% c(0, 1), paper_BRCA_Subtype_PAM50 != \"NA\") |&gt;\n  dplyr::mutate(PAM50 = paper_BRCA_Subtype_PAM50)\n\nRunning the analysis and plotting can then be achieved using the survfit and ggsurvplot packages respectively. The p-value requested is the p-value for there being any difference in survival between different groups and is calculated using a log-rank test.\n\n# --- Run Survival Analysis ---\nfit &lt;- survfit(Surv(Time, Status) ~ PAM50, data = clin_data_corrected)\n\n# --- Plot ---\nggsurvplot(\n  fit, \n  data = clin_data_corrected, \n  pval = TRUE,\n  palette = c(\"red\", \"pink\", \"darkblue\", \"lightblue\", \"yellow\")\n)\n\n\n\n\n\n\n\n\nThere are plenty more examples on the survminer website on how to customise this further"
  },
  {
    "objectID": "posts/2025_11_10_tidymodels_TCGA_part2/index.html",
    "href": "posts/2025_11_10_tidymodels_TCGA_part2/index.html",
    "title": "Tidymodels for omics data: Part 2",
    "section": "",
    "text": "In the previous section we described how to download TCGA data for breast cancers and manipulated them using a combination of tidybulk and dplyr to retain a set of expressed, variable genes plus a set of known cancer genes.\nThere is a very extensive set of clinical information recorded for each sample / patient, but to keep things simple we will start with a task for being able to predict Estrogen Receptor status from the expression data, which can be used as an indicator of whether a patient will respond to certain treatments. This is clearly not going to get us a Nature paper or a Nobel prize, but it should work well and introduce some of the key concepts of machine learning. There are a set of packages that we will need:-\n\nif(!require(tidymodels)) install.packages(\"tidymodels\")\n\n\nif(!require(dplyr)) install.packages(\"dplyr\")\nif(!require(ggplot2)) install.packages(\"ggplot2\")\nif(!require(forcats)) install.packages(\"forcats\")\n\nYou also need the processed data from the previous section and the code to download this is:-\n\n### get the saved RDS\ndir.create(\"raw_data\", showWarnings = FALSE)\nif(!file.exists(\"raw_data/brca_train_tidy.rds\")) download.file(\"https://github.com/markdunning/markdunning.github.com/raw/refs/heads/master/posts/2025_11_06_tidymodels_TCGA_part1/brca_train_tidy.rds\", destfile = \"raw_data/brca_train_tidy.rds\")\nif(!file.exists(\"raw_data/brca_test_tidy.rds\")) download.file(\"https://github.com/markdunning/markdunning.github.com/raw/refs/heads/master/posts/2025_11_06_tidymodels_TCGA_part1/brca_test_tidy.rds\", destfile = \"raw_data/brca_test_tidy.rds\")\n\n\nbrca_train_tidy &lt;- readRDS(\"raw_data/brca_train_tidy.rds\")\nbrca_test_tidy &lt;- readRDS(\"raw_data/brca_test_tidy.rds\")\n\nTo keep things simple we will create a data frame of just ER status and ESR1 expression level. For reasons that will become apparent shortly we also create a binary (0 or 1) representation of ER status with 1 being equivalent to Positive. We‚Äôll also change the order of the ER factor so that Positive is before Negative.\n\ner_train &lt;- dplyr::select(brca_train_tidy, ER  = er_status_by_ihc, ESR1) |&gt; \n  dplyr::mutate(ER_Numeric = ifelse(ER== \"Positive\", 1,0)) |&gt;\n  dplyr::mutate(ER  = forcats::fct_rev(ER))\n\ner_test &lt;- dplyr::select(brca_test_tidy, ER  = er_status_by_ihc, ESR1) |&gt; \n  dplyr::mutate(ER_Numeric = ifelse(ER== \"Positive\", 1,0)) |&gt;\n  dplyr::mutate(ER  = forcats::fct_rev(ER))\n\nWe have already seen the relationship between the ER status and the expression level of ESR1 in the form of a boxplot. The relationship is striking, but doesn‚Äôt always hold true that higher ESR1 means Positive. There are clearly some Negative samples with ESR1 expression over 10, and some Positive samples with ESR1 around 8.\n\nlibrary(ggplot2)\nggplot(er_train, aes(x = ER, y = ESR1)) + geom_boxplot() + geom_jitter(width = 0.1, alpha= 0.4) \n\n\n\n\n\n\n\n\nWe can attempt to draw a horizontal line on the boxplot at sensible point and use this to infer the ER status and add this information to the data. Congratulations! We have just done our first classification üéâ\n\ner_train |&gt;\n  mutate(ER_inferred = ifelse(ESR1 &gt; 10, \"Positive\",\"Negative\")) |&gt;\n  head()\n\n        ER      ESR1 ER_Numeric ER_inferred\n1 Negative  7.048912          0    Negative\n2 Negative 14.477387          0    Positive\n3 Negative  8.382184          0    Negative\n4 Negative 15.049325          0    Positive\n5 Negative  8.129185          0    Negative\n6 Negative  7.051278          0    Negative\n\n\nOn the same boxplot as before we can colour points according to which ER Status they are assigned under our new rule. This emphasises that the grouping is not perfect, but perhaps it is good enough for our purposes.\n\ner_train |&gt;\n  mutate(ER_inferred = ifelse(ESR1 &gt; 10, \"Positive\",\"Negative\")) |&gt;\n  ggplot(aes(x = ER, y = ESR1)) + geom_boxplot() + geom_jitter(aes(col = ER_inferred),width = 0.1, alpha= 0.4) + geom_hline(yintercept = 10, col=\"red\", lty=2, size=2)\n\n\n\n\n\n\n\n\nWe can count up how many times samples get allocated to the wrong group, and this will actually serves as a metric later on for evaluating how well we are doing\n\ner_train |&gt;\n  dplyr::mutate(ER_inferred = ifelse(ESR1 &gt; 10, \"Positive\",\"Negative\")) |&gt;\n  dplyr::count(ER, ER_inferred) |&gt;  # Count combinations\n  tidyr::pivot_wider(\n    names_from = ER,\n    values_from = n,\n    values_fill = 0\n  )\n\n# A tibble: 2 √ó 3\n  ER_inferred Positive Negative\n  &lt;chr&gt;          &lt;int&gt;    &lt;int&gt;\n1 Negative          17      146\n2 Positive         637       47\n\n\nOf course, although informed by the plot, we have just picked an arbitrary value of ESR1. Is this the best possible value we could have picked? Can we prove that the rule will work for other datasets too? These are questions we can answer using machine learning approaches. The first task is to split partition our data into distinct training and testing sets. The overall aim is to learn about the data by modeling and refine our choice of parameters using the training, and then see how this performs in a test set. The crucial part is that the training and testing datasets are kept completely separate."
  },
  {
    "objectID": "posts/2025_11_10_tidymodels_TCGA_part2/index.html#pre-amble",
    "href": "posts/2025_11_10_tidymodels_TCGA_part2/index.html#pre-amble",
    "title": "Tidymodels for omics data: Part 2",
    "section": "",
    "text": "This will be the first in a series where I look at machine learning techniques applied to omics data. However, before we get ahead of ourselves we‚Äôll need some data. I decided to use breast cancer samples available through The Cancer Genome Atlas (TCGA).\nWe need the TCGAbiolinks package that will do most of the work of downloading, and tidybulk for manipulation.\n\nif(!require(tidymodels)) install.packages(\"tidymodels\")"
  },
  {
    "objectID": "posts/2025_11_10_tidymodels_TCGA_part2/index.html#downloading-the-data",
    "href": "posts/2025_11_10_tidymodels_TCGA_part2/index.html#downloading-the-data",
    "title": "Tidymodels for omics data: Part 2",
    "section": "Downloading the data",
    "text": "Downloading the data\n\n### get the saved RDS"
  },
  {
    "objectID": "posts/2025_11_10_tidymodels_TCGA_part2/index.html#pre-processing",
    "href": "posts/2025_11_10_tidymodels_TCGA_part2/index.html#pre-processing",
    "title": "Tidymodels for omics data: Part 2",
    "section": "",
    "text": "The object brca_gene_filtered_SE.rds is a SummarizedExperiment, which is a specialised object type used for RNA-seq and other omics type data. The tidybulk package can be used to convert this into a ‚Äútidy‚Äù format for easy manipulation with the tidyverse set of package. A tidy format is also amenable to fitting models in R, as the general format of a model is in the form y ~ x with y and x being columns some data frame. y is often referred to as the response variable and x as the predictor variable.\nIn reality we will often have more than one predictor and our analysis will try and work out the best combination of variables to predict the outcome. However, in this example we will just use the expression of the ESR1 gene to predict the response of ER status (er_status_by_ihc). Hence we create a data frame with ESR1 expression and ER status as columns. For this reasons that will become apparent shortly we also create a binary (0 or 1) representation of ER status with 1 being equivalent to Positive. We‚Äôll also change the order of the ER factor so that Positive is before Negative.\n\n\n\n\n\n\nNote\n\n\n\nSince we are only using one gene the processing is simplified, as filtering by ESR1 means the values in fpkm_uq (our normalised counts) are already the expression of ESR1. If we wanted to include multiple genes we would need a column for each gene.\n\n\n\nlibrary(tidybulk)\nlibrary(SummarizedExperiment)\n\nbrca_gene_filtered &lt;- readRDS(\"raw_data/brca_gene_filtered_SE.rds\")\n\ner_data &lt;- brca_gene_filtered[rowData(brca_gene_filtered)$gene_name == \"ESR1\"] |&gt;\n  tidybulk() |&gt;\n  dplyr::select(ER=er_status_by_ihc, ESR1 = fpkm_uq) |&gt;\n  dplyr::filter(ER %in% c(\"Positive\",\"Negative\")) |&gt; \n  dplyr::mutate(ER_Numeric = ifelse(ER== \"Positive\", 1,0)) |&gt;\n  dplyr::mutate(ESR1 = log2(ESR1 + 1)) |&gt;\n  dplyr::mutate(ER  = forcats::fct_rev(ER))\n\nWe have already seen the relationship between the ER status and the expression level of ESR1 in the form of a boxplot. The relationship is striking, but doesn‚Äôt always hold true that higher ESR1 means Positive. There are clearly some Negative samples with ESR1 expression over 6, and some Positive samples with ESR1 around 0.\n\nlibrary(ggplot2)\nggplot(er_data, aes(x = ER, y = ESR1)) + geom_boxplot() + geom_jitter(width = 0.1, alpha= 0.4) \n\n\n\n\n\n\n\n\nWe can attempt to draw a horizontal line on the boxplot at sensible point and use this to infer the ER status and add this information to the data. Congratulations! We have just done our first classification üéâ\n\ner_data |&gt;\n  mutate(ER_inferred = ifelse(ESR1 &gt; 3, \"Positive\",\"Negative\")) |&gt;\n  head()\n\n# A tibble: 6 √ó 4\n  ER        ESR1 ER_Numeric ER_inferred\n  &lt;fct&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;      \n1 Positive 5.40           1 Positive   \n2 Positive 4.37           1 Positive   \n3 Positive 3.38           1 Positive   \n4 Negative 0.222          0 Negative   \n5 Positive 4.57           1 Positive   \n6 Negative 5.03           0 Positive   \n\n\nOn the same boxplot as before we can colour points according to which ER Status they are assigned under our new rule. This emphasises that the grouping is not perfect, but perhaps it is good enough for our purposes.\n\ner_data |&gt;\n  mutate(ER_inferred = ifelse(ESR1 &gt; 3, \"Positive\",\"Negative\")) |&gt;\n  ggplot(aes(x = ER, y = ESR1)) + geom_boxplot() + geom_jitter(aes(col = ER_inferred),width = 0.1, alpha= 0.4) + geom_hline(yintercept = 3, col=\"red\", lty=2, size=2)\n\n\n\n\n\n\n\n\nWe can count up how many times samples get allocated to the wrong group, and this will actually serves as a metric later on for evaluating how well we are doing\n\ner_data |&gt;\n  dplyr::mutate(ER_inferred = ifelse(ESR1 &gt; 3, \"Positive\",\"Negative\")) |&gt;\n  dplyr::count(ER, ER_inferred) |&gt;  # Count combinations\n  tidyr::pivot_wider(\n    names_from = ER,\n    values_from = n,\n    values_fill = 0\n  )\n\n# A tibble: 2 √ó 3\n  ER_inferred Positive Negative\n  &lt;chr&gt;          &lt;int&gt;    &lt;int&gt;\n1 Negative          62      227\n2 Positive         756       15\n\n\nOf course, although informed by the plot, we have just picked an arbitrary value of ESR1. Is this the best possible value we could have picked? Can we prove that the rule will work for other datasets too? These are questions we can answer using machine learning approaches. The first task is to split partition our data into distinct training and testing sets. The overall aim is to learn about the data by modeling and refine our choice of parameters using the training, and then see how this performs in a test set. The crucial part is that the training and testing datasets are kept completely separate.\nIt is possible to use the sample function from R to pick rows from our dataset, or even the slice_sample from dplyr, but the most straightforward way of splitting the data into training and testing is using the tidymodels package. We will learn lots about this package in due course. The same way that tidyverse is a collection of packages with a common philosophy for data manipulation and visualisaton, tidymodels is a ecosystem of packages for all steps of machine learning. The first task is often to split data into traiing and testing sets, which is performed by the initial_split function after loading tidymodels. During the split it is common to use the majority (say 80%) of the data for training. You can also make sure that the your outcome of interest has roughly the same proportion in training and testing. Once a split is created with initial_split you can extract the training and testing data.\n\nlibrary(tidymodels)\n\n## Setting a 'seed' makes sure the results are reproducible\nset.seed(42) \ndata_split &lt;- initial_split(er_data, \n                            prop = 0.80, \n                            strata = ER)\n\n# Create the training data set\ner_train &lt;- training(data_split)\n\n# Create the testing data set\ner_test &lt;- testing(data_split)\n\nThis is not usually required, but we can inspect the first few rows of the training\n\ner_train |&gt; \n  head()\n\n# A tibble: 6 √ó 3\n  ER        ESR1 ER_Numeric\n  &lt;fct&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n1 Negative 0.222          0\n2 Negative 5.03           0\n3 Negative 0.528          0\n4 Negative 5.62           0\n5 Negative 0.475          0\n6 Negative 0.221          0\n\n\nand test data\n\ner_test |&gt;\n  head()\n\n# A tibble: 6 √ó 3\n  ER        ESR1 ER_Numeric\n  &lt;fct&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n1 Positive 3.21           1\n2 Positive 3.82           1\n3 Negative 0.831          0\n4 Negative 1.14           0\n5 Positive 4.84           1\n6 Positive 4.31           1\n\n\nand check that indeed the training data has around 80% of our original data\n\nnrow(er_train) / nrow(er_data)\n\n[1] 0.7990566\n\n\nand our testing data should be around 20%\n\nnrow(er_test) / nrow(er_data)\n\n[1] 0.2009434\n\n\nWe can also check the ER Positive / Negative balance in our original dataset.\n\ndplyr::count(er_data, ER) |&gt;\n  mutate(prop = n / nrow(er_data))\n\n# A tibble: 2 √ó 3\n  ER           n  prop\n  &lt;fct&gt;    &lt;int&gt; &lt;dbl&gt;\n1 Positive   818 0.772\n2 Negative   242 0.228\n\n\nand check that it is preserved in the training:-\n\ndplyr::count(er_train, ER) |&gt;\n  mutate(prop = n / nrow(er_train))\n\n# A tibble: 2 √ó 3\n  ER           n  prop\n  &lt;fct&gt;    &lt;int&gt; &lt;dbl&gt;\n1 Positive   654 0.772\n2 Negative   193 0.228\n\n\nand testing data. We don‚Äôt need to do this in practice, it‚Äôs just to reassure us that tidymodels is splitting the data as we expect.\n\ndplyr::count(er_test, ER) |&gt;\n  mutate(prop = n / nrow(er_test))\n\n# A tibble: 2 √ó 3\n  ER           n  prop\n  &lt;fct&gt;    &lt;int&gt; &lt;dbl&gt;\n1 Positive   164 0.770\n2 Negative    49 0.230\n\n\nWe should be good to go with the machine learning task, but first we will have to go through a few definitions."
  },
  {
    "objectID": "posts/2025_11_10_tidymodels_TCGA_part2/index.html#logistic-regression",
    "href": "posts/2025_11_10_tidymodels_TCGA_part2/index.html#logistic-regression",
    "title": "Tidymodels for omics data: Part 2",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\n# Fit the logistic regression model using the logged ESR1 expression\n# Formula: Outcome ~ Predictor, using the binomial family for logistic regression\nsimple_logit_fit &lt;- glm(\n  ER_Numeric ~ ESR1, \n  data = er_train, \n  family = \"binomial\"\n)\n\n# View the model summary (shows coefficients and significance)\nsummary(simple_logit_fit)\n\n\nCall:\nglm(formula = ER_Numeric ~ ESR1, family = \"binomial\", data = er_train)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.85399    0.26155  -10.91   &lt;2e-16 ***\nESR1         1.35386    0.09254   14.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 909.14  on 846  degrees of freedom\nResidual deviance: 312.88  on 845  degrees of freedom\nAIC: 316.88\n\nNumber of Fisher Scoring iterations: 7\n\n\nPredictions\n\n# Predict probabilities (0 to 1) on the test set\ntest_probabilities &lt;- predict(\n  simple_logit_fit, \n  newdata = er_test, \n  type = \"response\" # 'response' gives probabilities\n)\n\n# 2. Convert probabilities to a class prediction (0 or 1)\n# We use the standard threshold of 0.5: if P(Y=1) &gt; 0.5, predict 1 (Positive)\npredicted_class_numeric &lt;- ifelse(test_probabilities &gt; 0.5, 1, 0)\n\n# 3. Calculate Accuracy\n# Accuracy is the number of correct predictions divided by the total number of predictions\nactual_class_numeric &lt;- er_test$ER_Numeric\n\n# Calculate the number of correct predictions (where predicted == actual)\ncorrect_predictions &lt;- sum(predicted_class_numeric == actual_class_numeric)\n\n# Calculate Accuracy\naccuracy &lt;- correct_predictions / length(actual_class_numeric)\n\ncat(\"Model Accuracy:\", round(accuracy, 4), \"\\n\")\n\nModel Accuracy: 0.939 \n\n\nWhat is going on?\n\nlibrary(ggplot2)\n\nbind_cols(er_test, Predicted_prob  = test_probabilities) %&gt;% \n  ggplot(aes(x = ESR1, y = Predicted_prob, col = as.factor(ER))) + geom_point()"
  },
  {
    "objectID": "posts/2025_11_10_tidymodels_TCGA_part2/index.html#decision-tree",
    "href": "posts/2025_11_10_tidymodels_TCGA_part2/index.html#decision-tree",
    "title": "Tidymodels for omics data: Part 2",
    "section": "Decision Tree",
    "text": "Decision Tree\nIt is common practice to try various statistical methods on the same dataset and compare how they perform using the metrics described above. The next method we will try is that of a decision tree, which actually follows quite nicely for the example we used at the start of this section where we picked a threshold on the ESR1 expression and used that to classify. As it‚Äôs name implies, the decision tree is a way of forming rules based on threshold in the form of ‚Äúif X is &gt; ‚Ä¶ then Y belongs to class A‚Äù. If is often visualised in the form of a tree. As we only have only variable (ESR1) the tree will look a bit boring, but can go grow more much more complex.\nWe use the rpart package for this, which we should already have from installing tidymodels. The code looks pretty similar to the glm from before. Briefly, the function will try different possibilities of ESR1 as potential values for the cut-off. For each cut-off, it splits the data into two classes. It then assess the ‚Äúpurity‚Äù of each class; in other words the % of Positive and Negative in each class. The split resulting in the most ‚Äúpure‚Äù classes is chosen.\n\nlibrary(rpart)\nset.seed(42)\n# Fit the tree model\nsimple_tree_fit &lt;- rpart(\n  ER ~ ESR1,\n  data = er_train,\n  method = \"class\" # Specifies a classification tree\n)\n\nPrinting the output gives some information about the cutoffs it has determined\n\nsimple_tree_fit\n\nn= 847 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 847 193 Positive (0.77213695 0.22786305)  \n  2) ESR1&gt;=2.365644 646  19 Positive (0.97058824 0.02941176) *\n  3) ESR1&lt; 2.365644 201  27 Negative (0.13432836 0.86567164) *\n\n\nIt has chosen 2.36 as it‚Äôs cutoff value, with an ESR1 value &gt; 2.36 being put into the Positive class. In total 646 samples were placed in the Positive class with 19 samples being incorrectly classified. This gives a purity measure of 97%. The Negative class is defined by ESR1 being less than 2.36 and comprises 201 samples.\n\n\n\n\n\n\nImportant\n\n\n\nRemember that the number of miss-classifications above are reported on the training data. These are intended for use in tweaking and refining our model. For true assessment of how the model performs on unseen data we need to make some predictions.\nThis is especially true for decision tree that are prone to ‚Äúoverfitting‚Äù. In other words they can memorize the unique quirks and noise of the training data. The \\(97.1\\%\\) purity simply means the tree found the best rule for the specific 847 samples in the training data. The goal of machine learning is generalization; to make correct predictions on data the model has never seen before. The only way to know if the rule (\\(\\text{ESR1} \\ge 2.366\\)) is truly robust is to test it on the completely separate testing data .\n\n\nTo see the tree-like nature of the model we can use the rpart.plot package.\n\n# Load the visualization package\nlibrary(rpart.plot)\n\n# Plot the tree diagram\nrpart.plot(\n  simple_tree_fit,\n  type = 4,      # Draws the full tree structure\n  extra = 101,   # Displays the class name and prediction accuracy\n  roundint = FALSE, # Keep decimal points on the split value\n  main = \"ER Status Classification by ESR1 Expression\"\n)\n\n\n\n\n\n\n\n\nAs with the logistic regression method, we can use the predict method to make predictions from our testing data. Unlike the logistic regression though we don‚Äôt get probabilities for individual samples, only classifications.\n\n# Make class predictions on the test set\ntree_predictions &lt;- predict(\n    simple_tree_fit, \n    newdata = er_test, \n    type = \"class\"\n)\n\ntree_predictions[1:10]\n\n       1        2        3        4        5        6        7        8 \nPositive Positive Negative Negative Positive Positive Positive Positive \n       9       10 \nPositive Positive \nLevels: Positive Negative\n\n\nWe can make the confusion matrix from these predictions using the conf_mat as before.\n\nbind_cols(er_test, Predicted_class  = tree_predictions) %&gt;% \n  conf_mat(ER, Predicted_class)\n\n          Truth\nPrediction Positive Negative\n  Positive      154        2\n  Negative       10       47\n\n\nBy adopting the tidymodels framework we are able to reuse a lot of code. We can compute the accuracy, sensitivity and specificity using the class_metrics function that we already defined:-\n\nbind_cols(er_test, Predicted_class  = tree_predictions) %&gt;% \n  class_metrics(truth=ER, estimate = Predicted_class)\n\n# A tibble: 3 √ó 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.944\n2 sensitivity binary         0.939\n3 specificity binary         0.959\n\n\nThe decision tree seems to offer very slight improvements over the logistic regression. The elephant in the room is of course that we haven‚Äôt set a very challenging task for us to accomplish. Things will get more complex as the start to add other genes as variables, or use other outcome variables (e.g.¬†the PAM50 classes)."
  },
  {
    "objectID": "posts/2025_11_10_tidymodels_TCGA_part2/index.html#introducting-tidymodels",
    "href": "posts/2025_11_10_tidymodels_TCGA_part2/index.html#introducting-tidymodels",
    "title": "Tidymodels for omics data: Part 2",
    "section": "Introducting tidymodels",
    "text": "Introducting tidymodels\nModel Specification\n\n# Specify a logistic regression model\nlogit_spec &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%          # The underlying R function to use\n  set_mode(\"classification\")     # The task: predicting a class\n\nRecipe\n\n# We assume the log-transform has already been applied outside the recipe\ner_logit_recipe &lt;- recipe(ER ~ ESR1, data = er_train)\n\nWorkflow and training\n\nlogit_workflow &lt;- workflow() %&gt;%\n  add_model(logit_spec) %&gt;%\n  add_recipe(er_logit_recipe)\n\n# Fit the workflow to the training data (train the model)\ner_logit_fit &lt;- logit_workflow %&gt;%\n  fit(data = er_train)\n\n\n# Extract the underlying glm object\nglm_object &lt;- er_logit_fit %&gt;%\n  extract_fit_engine()\n\n# View the standard summary\nsummary(glm_object)\n\n\nCall:\nstats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.85399    0.26155   10.91   &lt;2e-16 ***\nESR1        -1.35386    0.09254  -14.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 909.14  on 846  degrees of freedom\nResidual deviance: 312.88  on 845  degrees of freedom\nAIC: 316.88\n\nNumber of Fisher Scoring iterations: 7\n\n\nMaking predictions\n\n# Generate CLASS predictions (the model's final decision: Positive or Negative)\nclass_pred &lt;- predict(er_logit_fit, \n                      new_data = er_test, \n                      type = \"class\")\n\n# Generate PROBABILITY predictions (P(ER+), used for the sigmoid curve and AUC)\nprob_pred &lt;- predict(er_logit_fit, \n                     new_data = er_test, \n                     type = \"prob\")\n\n# --- 2. Combine and Prepare for Evaluation ---\n\n# Combine the test data, class predictions, and probability predictions\ner_results &lt;- er_test %&gt;%\n  select(ER) %&gt;%       # Keep the true outcome\n  bind_cols(class_pred) %&gt;%  # Add the predicted class (.pred_class)\n  bind_cols(prob_pred)      # Add the probabilities (.pred_Negative, .pred_Positive)\n\n# View the first few rows of the results\nhead(er_results)\n\n# A tibble: 6 √ó 4\n  ER       .pred_class .pred_Positive .pred_Negative\n  &lt;fct&gt;    &lt;fct&gt;                &lt;dbl&gt;          &lt;dbl&gt;\n1 Positive Positive             0.816         0.184 \n2 Positive Positive             0.911         0.0892\n3 Negative Negative             0.151         0.849 \n4 Negative Negative             0.213         0.787 \n5 Positive Positive             0.976         0.0242\n6 Positive Positive             0.952         0.0485\n\n\n\ner_results %&gt;%\n  accuracy(truth = ER, estimate = .pred_class)\n\n# A tibble: 1 √ó 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.939\n\n\nCan also make a ROC curve\n\n# Calculate the AUC, using the probability of the positive class (.pred_Positive)\nroc_data &lt;- er_results %&gt;%\n  roc_curve(truth = ER, .pred_Positive)\n\n\n# Calculate the AUC value to display on the plot\nroc_auc_value &lt;- er_results %&gt;%\n  roc_auc(truth = ER, .pred_Positive) %&gt;%\n  pull(.estimate) # Extracts the numeric AUC value\n\n# Plot the ROC curve\nroc_plot &lt;- roc_data %&gt;%\n  autoplot() +\n  # Add the diagonal reference line for a random classifier (AUC = 0.5)\n  geom_abline(lty = 2, color = \"gray50\") +\n  \n  # Annotate with the calculated AUC value\n  annotate(\"text\", \n           x = 0.75, \n           y = 0.25, \n           label = paste(\"AUC =\", round(roc_auc_value, 4)), \n           size = 5) +\n  \n  labs(\n    title = \"ROC Curve for ER Status Classification (ESR1 Gene)\",\n    subtitle = \"True Positive Rate vs. False Positive Rate\"\n  ) +\n  theme_minimal()\n\nprint(roc_plot)\n\n\n\n\n\n\n\n\n\nplot_data &lt;- bind_cols(er_test, Predicted_prob  = test_probabilities) \n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"ER Status Classifier Threshold Demo\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"threshold\", \"Classification Threshold (P(ER+))\",\n                  min = 0.05, max = 0.99, value = 0.5, step = 0.01),\n# Combined Output for all metrics\n      h4(\"Accuracy:\"),\n      verbatimTextOutput(\"accuracy_output\"),\n      \n      h4(\"Confusion Matrix Counts:\"),\n      # Output for the structured confusion matrix\n      htmlOutput(\"matrix_output\") \n    ),\n    mainPanel(\n      h4(\"Confusion Matrix:\"),\n      verbatimTextOutput(\"matrix_output\"),\n      plotOutput(\"sigmoid_plot\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  \n  # Reactive Prediction Logic (Remains the same)\nreactive_metrics &lt;- reactive({\n    thresh &lt;- input$threshold\n    \n    # 1. Re-classify based on the new threshold (0 or 1)\n    # The output of ifelse() is numeric, so convert it to a factor with defined levels\n    predicted_class &lt;- factor(\n        ifelse(test_probabilities &gt; thresh, 1, 0),\n        levels = c(\"0\", \"1\") # IMPORTANT: Defines both levels\n    )\n    \n    # 2. Get the actual class (ensure it's also a factor with defined levels)\n    actual_class &lt;- factor(\n        er_test$ER_Numeric, \n        levels = c(\"0\", \"1\") # IMPORTANT: Defines both levels\n    )\n    \n    # 3. Generate the table (now guaranteed to be 2x2)\n    conf_matrix &lt;- table(Predicted = predicted_class, Actual = actual_class)\n    \n    # 4. Extract values by position (safer than name)\n    # The dimensions are guaranteed to be in the order 0, 1 for both axes\n    TN &lt;- conf_matrix[1, 1] # Predicted 0, Actual 0\n    FP &lt;- conf_matrix[2, 1] # Predicted 1, Actual 0\n    FN &lt;- conf_matrix[1, 2] # Predicted 0, Actual 1\n    TP &lt;- conf_matrix[2, 2] # Predicted 1, Actual 1\n    \n    # Calculate accuracy\n    accuracy &lt;- (TP + TN) / sum(conf_matrix)\n\n    return(list(TP=TP, TN=TN, FP=FP, FN=FN, accuracy=accuracy))\n  })\n  \n  # --- Output: Accuracy ---\n  output$accuracy_output &lt;- renderText({\n    metrics &lt;- reactive_metrics()\n    paste0(round(metrics$accuracy, 4), \n           \" (\", metrics$TP + metrics$TN, \" correct)\")\n  })\n  \n  # --- Output: Confusion Matrix (Structured HTML) ---\noutput$matrix_output &lt;- renderUI({\n    metrics &lt;- reactive_metrics()\n    \n    # 1. Create a clean character vector for the metrics\n    matrix_lines &lt;- c(\n      paste(\"&lt;b&gt;True Positives (TP):&lt;/b&gt;\", metrics$TP),\n      paste(\"&lt;b&gt;True Negatives (TN):&lt;/b&gt;\", metrics$TN),\n      paste(\"&lt;b&gt;False Positives (FP):&lt;/b&gt;\", metrics$FP),\n      paste(\"&lt;b&gt;False Negatives (FN):&lt;/b&gt;\", metrics$FN)\n    )\n    \n    # 2. Collapse the vector into a single string, using &lt;br/&gt; for line breaks\n    final_html_string &lt;- paste(matrix_lines, collapse = \"&lt;br/&gt;\")\n    \n    # 3. Return the single character string wrapped in HTML()\n    HTML(final_html_string)\n  })\n  \n  # --- Output: Plot (Add the dynamic threshold) ---\n  output$sigmoid_plot &lt;- renderPlot({\n    # Placeholder for your plotting logic\n    \n    # ... (code to generate the sigmoid plot)\n    \n    # Add the dynamic threshold line\n    ggplot(plot_data, aes(x = ESR1_Log, y = Predicted_Prob)) +\n      stat_smooth(method = \"glm\", method.args = list(family = \"binomial\"), se = FALSE, color = \"black\") +\n      geom_point(aes(color = factor(Actual_ER)), alpha = 0.6) +\n      geom_hline(yintercept = input$threshold, linetype = \"dashed\", color = \"darkred\", linewidth = 1) +\n      labs(title = paste(\"Threshold =\", round(input$threshold, 2))) +\n      theme_minimal()\n  })\n}\n\n# shinyApp(ui = ui, server = server)\n\n# --- 4. Run the App ---\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "posts/2025_11_10_tidymodels_TCGA_part2/index.html#fitting-a-logistic-regression",
    "href": "posts/2025_11_10_tidymodels_TCGA_part2/index.html#fitting-a-logistic-regression",
    "title": "Tidymodels for omics data: Part 2",
    "section": "Fitting a Logistic Regression",
    "text": "Fitting a Logistic Regression\nThe curve actually represents a series of probabilities between 0 and 1 which can be used to assign to a particular point to either Positive or Negative group. If the probability is closer to 1 for a given observation then it is more likely to belong to the Positive class, and if the probability is close to 0 then it is more likely to be Negative. The definition of ‚Äúclose‚Äù typically means &gt; 0.5 belong to Positive but we can change this.\nSo let‚Äôs look at the code to fit such a curve using a technique called logistic regression within R. We will use the glm function that has a similar interface to the lm function we saw briefly. Setting family = binomial is required as it tells glm to fit the S-shaped curve that we need.\nNote that we use the training portion of our data, er_train to fit the model.\n\nsimple_logit_fit &lt;- glm(\n  # Formula: Outcome ~ Predictor,\n  ER_Numeric ~ ESR1, \n  data = er_train, \n  #set the \"family\" to binomial for logistic regression\n  family = \"binomial\"\n)\n\n# View the model summary\nsummary(simple_logit_fit)\n\n\nCall:\nglm(formula = ER_Numeric ~ ESR1, family = \"binomial\", data = er_train)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -10.27507    0.77119  -13.32   &lt;2e-16 ***\nESR1          0.95392    0.06539   14.59   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 909.14  on 846  degrees of freedom\nResidual deviance: 323.46  on 845  degrees of freedom\nAIC: 327.46\n\nNumber of Fisher Scoring iterations: 6\n\n\nThe interpretation is a bit trickier because we don‚Äôt have a slope and intercept with this kind of model. Instead the coefficient for ESR1 signifies how the log-odds of being Positive increases as the level of ESR1 increase by one unit. The odds are more intuitive and these can be calculated with:-\n\nlog_odds &lt;- coef(simple_logit_fit)[2]\nodds_ratio &lt;- exp(log_odds)\nodds_ratio\n\n    ESR1 \n2.595869 \n\n\nThe odds ratio is 2.59587 meaning the increasing ESR1 expression means you are around 3 times more likely to be ER positive than negative. Now that we have built the model we can use it to make predictions given a set of ESR1. If we want to make sure our model has not been biased by any specific patterns only observed in our training data we should predict using a set of data the model has not ‚Äúseen‚Äù before. This is why we split our data into training and testing sets. If we set type = response the result will be probability for each observation in the testing set.\n\ntest_probabilities &lt;- predict(\n  simple_logit_fit, \n  newdata = er_test, \n  type = \"response\" # 'response' gives probabilities\n)\ntest_probabilities[1:10]\n\n        1         2         3         4         5         6         7         8 \n0.8427194 0.9157402 0.1917981 0.2893535 0.9665688 0.9455396 0.8351112 0.9872810 \n        9        10 \n0.9768193 0.9832141 \n\n\nWe can now plot the probabilities against the respective ESR1 value, which hopefully gives the S-shaped curve we are trying to fit. To convert the probabilities to a Positive or Negative label we set a threshold such as 0.5. In the below plot we also mark the predictions that are incorrect.\n\ner_test |&gt;\n  mutate(Prob = test_probabilities, `Actual Label` = ER) |&gt;\n  mutate(`Predicted Label` = ifelse(test_probabilities &gt; 0.5, \"Positive\", \"Negative\")) |&gt;\n  mutate(`Correct Prediction` = as.factor(ER == `Predicted Label`)) |&gt;\n  ggplot(aes(x = ESR1, y = Prob, col = `Actual Label`, shape = `Correct Prediction`, size = `Correct Prediction`, alpha=`Correct Prediction`)) + \n  geom_point() + \n  geom_hline(yintercept = 0.5, lty = 2) + \n  scale_shape_manual(values = c(4, 16)) +\n  scale_size_manual(values = c(5,2)) + \n  scale_alpha_manual(values = c(1,0.4))"
  },
  {
    "objectID": "posts/2025_11_10_tidymodels_TCGA_part2/index.html#evaluating-the-fit",
    "href": "posts/2025_11_10_tidymodels_TCGA_part2/index.html#evaluating-the-fit",
    "title": "Tidymodels for omics data: Part 2",
    "section": "Evaluating the fit",
    "text": "Evaluating the fit\nOn inspection, it looks there are fewer cases that should be Negative that have been labeled as Positive, than Positive cases labeled as Negative. Overall though, most of the predictions look correct. To formalise this and attach some metrics we can create what is called a confusion matrix. To create this we can use the yardstick package that is included as part of tidymodels.\n\nlibrary(yardstick)\ner_test %&gt;% \n  mutate(Predicted_ER = factor(ifelse(test_probabilities &gt; 0.5, \"Positive\", \"Negative\"), levels = c(\"Positive\",\"Negative\"))) %&gt;% \n  conf_mat(ER, Predicted_ER)\n\n          Truth\nPrediction Positive Negative\n  Positive      155        5\n  Negative        9       44\n\n\nThe numbers in the table have names and meanings associated with them:-\n\nTrue Negatives (TN) 44 Correctly predicted Negative\nTrue Positives (TP) 155 Correctly predicted Positive\nFalse Positives (FP) 5 Incorrectly predicted Positive when the tumor was actually Negative. Also known as Type I Error\nFalse Negatives (FN) 9 Incorrectly predicted Negative when the tumor was actually Positive. Also known as Type II Error\n\nThree common metrics for classification problems such as this are:-\n\nAccuracy = Accuracy is the total number of correct predictions divided by the total samples.\n\n\\(TP  +TN / (Total)\\) = \\(155 + 44 / 213\\) \\(\\approx\\) 93.4%\n\nSensitivity (True Positive Rate) = how well the model finds all the actual Positive cases.\n\n\\(TP  / (TP + FN)\\) = \\(155 / (155  + 9)\\) \\(\\approx\\) 94.5%\n\nSpecificity (True Negative Rate) = how well the model avoids incorrectly classifying actual Negative cases.\n\n\\(TN  / (TN + FP)\\) = \\(44 / (44  + 5)\\) \\(\\approx\\) 89.83%\n\n\nFortunately we don‚Äôt need to type all those equations by hand as the yardstick package will allow us to define a set of metrics and use these to evaluate our predictions. This package is just one of the many that comprise the tidymodels ecosystem.\n\nclass_metrics &lt;- metric_set(accuracy, sensitivity, specificity)\n\ner_test %&gt;% \n  mutate(Predicted_ER = factor(ifelse(test_probabilities &gt; 0.5, \"Positive\", \"Negative\"), levels = c(\"Positive\",\"Negative\"))) %&gt;% \n  class_metrics(truth=ER, estimate = Predicted_ER) \n\n# A tibble: 3 √ó 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.934\n2 sensitivity binary         0.945\n3 specificity binary         0.898\n\n\nThe metrics have all been calculated on the basis of using a probability of 0.5 to classify samples as Positive or Negative. This was a fairly arbitrary choice and we could experiment with other values. To save us time, the roc_curve function will calculate the specificity and sensitivity for a range of thresholds.\n\ner_test %&gt;% \n  mutate(Prob = test_probabilities) %&gt;% \n  roc_curve(ER, Prob) %&gt;% \n  head(n = 10)\n\n# A tibble: 10 √ó 3\n   .threshold specificity sensitivity\n        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 -Inf            0            1    \n 2    0.00341      0            1    \n 3    0.00608      0            0.994\n 4    0.00621      0.0204       0.994\n 5    0.0104       0.0408       0.994\n 6    0.0114       0.0612       0.994\n 7    0.0139       0.0816       0.994\n 8    0.0147       0.102        0.994\n 9    0.0155       0.122        0.994\n10    0.0167       0.122        0.988\n\n\nWe can see for example setting a threshold close to 0 means that all Positive cases are identified, but the specificity is miserable as there are too many false positives. Plotting sensitivity against 1 - specificity (the False Positive rates) gives a very famous curve called the ROC curve (‚ÄúReceiver Operating Characteristics‚Äù). We can create this plot using the autoplot function after using the roc_curve function.\n\n\n\n\n\n\nNote\n\n\n\nIn case you were wondering where this name originates:- From wikipedia\n\nThe ROC curve was first developed by electrical engineers and radar engineers during World War II for detecting enemy objects in battlefields, starting in 1941, which led to its name (‚Äúreceiver operating characteristic‚Äù).\n\n\n\n\ner_test %&gt;% \n  mutate(Prob = test_probabilities) %&gt;% \n  roc_curve(ER, Prob) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\nA curve near the top-left corner indicates a model with high discriminatory power. i.e.¬†the model can achieve high Sensitivity (finding true positives) without incurring a significant cost in Specificity (avoiding false positives). If the curve is close to the diagonal line then it is not much better than guessing.\n\n\n\n\n\n\nImportant\n\n\n\nWhen deciding the threshold there is usually a trade-off between specificity and sensitivity. Do you want to make sure that you capture all your positive cases at the expense of a few false positives? In which case you would lower the threshold.\nOr do you want to be absolutely sure about the cases you identify, at the expense of missing a few positives? If so, then increase the threshold.\nIf a treatment following a positive diagnosis is invasive, expensive, or has severe side effects, you want to be highly certain before proceeding. Here, the cost of a False Positive (treating a healthy person) might be much higher than the cost of a False Negative.\nOn the other hand you are developing a cancer screening test, which if positive would lead to further investigations, you might want to lower threshold so you don‚Äôt ignore any potential true cases at an early stage. In this scenario, the cost of a few unnecessary follow-up procedures (False Positive) is deemed acceptable compared to the devastating cost of missing an early-stage cancer (False Negative).\n\n\nThe ROC curve also leads to another diagnostic metric that can be used to assess how effective our model is at predicting. Given that we want the curve to be in the top-left, that area under the curve or AUC is an important measure. Since both axes are limited between 0 and 1 the maximum possible area is 1, and the closer to 1 we are means a better model.\n\ner_test %&gt;% \n  mutate(Prob = test_probabilities) %&gt;% \n  roc_auc(ER, Prob)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.964\n\n\nWe‚Äôve now seen several metrics; accuracy, sensitivity, specificity and AUC. Some or all of these can be used to assess our model. Furthermore they can be used to compare different types of model as they can be calculated on the results of applying other statistical methods to classify our data."
  },
  {
    "objectID": "posts/2025_11_10_tidymodels_TCGA_part2/index.html#model-specification",
    "href": "posts/2025_11_10_tidymodels_TCGA_part2/index.html#model-specification",
    "title": "Tidymodels for omics data: Part 2",
    "section": "Model Specification",
    "text": "Model Specification\ntidymodels recommends that models are specified in a particular manner. This defines the particular statistical modeling approach to be used (Logistic regression or decision tree, or others), the particular package to be used (the engine) and type of modeling task to be performed (classication or regression). The specifications for the models we have used above for logistic regression:-\n\nlibrary(tidymodels)\n# Specify a logistic regression model\nlogit_spec &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%          # The underlying R function to use\n  set_mode(\"classification\")     # The task: predicting a class\n\nand the decision tree:-\n\ndecision_tree_spec &lt;- decision_tree() %&gt;% \n  set_engine(\"rpart\") %&gt;% \n  set_mode(\"classification\")"
  },
  {
    "objectID": "posts/2025_11_10_tidymodels_TCGA_part2/index.html#recipes-and-workflows",
    "href": "posts/2025_11_10_tidymodels_TCGA_part2/index.html#recipes-and-workflows",
    "title": "Tidymodels for omics data: Part 2",
    "section": "Recipes and workflows",
    "text": "Recipes and workflows\n‚ÄúRecipes‚Äù typically cover the pre-processing steps such as transforming variables onto a suitable scale (e.g.¬†log\\(_2\\) transformation) and removing variables with low variability. However, we have already done this using tidybulk so our recipe is very short. For other projects I suggest looking at the official documentation\n\nhttps://www.tidymodels.org/start/recipes/#recipe\n\nPreviously when using glm for logistic regression we had to use a numeric form of the variable we are trying to predict (i.e.¬†0 or 1 for Negative and Positive respectively). However, tidymodels prefers a factor and will perform any conversions internally if needs be.\n\n# We assume the log-transform has already been applied outside the recipe\ner_recipe &lt;- recipe(ER ~ ESR1, data = er_train) # %&gt;% \n  # step_normalize() - scale the data\n  # step_nzv - remove features with low variance.\n\n‚ÄúWorkflows‚Äù in tidymodels are a good idea because they are the centralised container that brings together the essential parts of the modeling process: the model definition and the data processing recipe. They ensure your work is consistent, easy to manage, and scalable. The workflow itself doesn‚Äôt contain the code to fit the model, but it contains all the instructions and objects needed for the fit() function to execute the model fitting."
  },
  {
    "objectID": "posts/2025_11_10_tidymodels_TCGA_part2/index.html#fitting-the-model-and-predicting",
    "href": "posts/2025_11_10_tidymodels_TCGA_part2/index.html#fitting-the-model-and-predicting",
    "title": "Tidymodels for omics data: Part 2",
    "section": "Fitting the model and predicting",
    "text": "Fitting the model and predicting\nHere is the code for making a workflow to define and fit the logistic regression:-\n\n## Create the workflow\nlogit_workflow &lt;- workflow() %&gt;%\n  add_model(logit_spec) %&gt;%\n  add_recipe(er_recipe)\n\n# Fit the workflow to the training data (i.e. train the model)\ner_logit_fit &lt;- logit_workflow %&gt;%\n  fit(data = er_train)\n\nPrinting the fitted model itself shows some of the output that we have seen previously.\n\ner_logit_fit\n\n‚ïê‚ïê Workflow [trained] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: logistic_reg()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n0 Recipe Steps\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n(Intercept)         ESR1  \n    10.2751      -0.9539  \n\nDegrees of Freedom: 846 Total (i.e. Null);  845 Residual\nNull Deviance:      909.1 \nResidual Deviance: 323.5    AIC: 327.5\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe eagle-eyed of you might have noticed that the coefficients calculated using the tidymodels form of the model have an opposite sign to the original fit. This arises because tidymodels did some internal conversion of our numeric ER values to a factor. In our original logistic model fit we set the ‚Äúlevels‚Äù of the factor so that Positive came before Negative. This was to help with the interpretation of the model. However, tidymodels has represented the levels in the opposite way so that ‚Äú0‚Äù is the baseline. This shouldn‚Äôt actually affect our predictions as we shall see.\n\n\nMake predictions from our testing data can now be done by ‚Äúpiping‚Äù the model we have just created into the predict function and then binding the results back to the test data.\n\ner_results &lt;- er_logit_fit %&gt;% \n  predict(new_data = er_test, type = \"class\") %&gt;% ## make predictions from the test data\n  bind_cols(er_test) # Add the predicted class\n\n# View the first few rows of the results\nhead(er_results)\n\n# A tibble: 6 √ó 4\n  .pred_class ER        ESR1 ER_Numeric\n  &lt;fct&gt;       &lt;fct&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n1 Positive    Positive 12.5           1\n2 Positive    Positive 13.3           1\n3 Negative    Negative  9.26          0\n4 Negative    Negative  9.83          0\n5 Positive    Positive 14.3           1\n6 Positive    Positive 13.8           1\n\n\nThe result can then be evaluated using the metrics we defined previously.\n\ner_results %&gt;%\n  class_metrics(truth = ER, estimate = .pred_class)\n\n# A tibble: 3 √ó 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.934\n2 sensitivity binary         0.945\n3 specificity binary         0.898\n\n\nThankfully we reach the same results as before, but using a coding framework that is a bit more flexible and in line with other styles of R programming we may be familiar with from packages such as dplyr and ggplot2."
  },
  {
    "objectID": "posts/2025_11_10_tidymodels_TCGA_part2/index.html#decision-tree-using-tidymodels",
    "href": "posts/2025_11_10_tidymodels_TCGA_part2/index.html#decision-tree-using-tidymodels",
    "title": "Tidymodels for omics data: Part 2",
    "section": "Decision tree using tidymodels",
    "text": "Decision tree using tidymodels\nTo use a decision tree rather than a logistic regression we first have to create a new model specification. To keep things consistent we‚Äôll use rpart to fit the model, but note that we‚Äôre not fitting the model at this point but just saying that rpart will be used.\n\ntree_spec &lt;- decision_tree() %&gt;%\n  set_engine(\"rpart\") %&gt;%       # We will use the 'rpart' package to actually fit the model\n  set_mode(\"classification\")    # Specify the task type\n\nCreating a workflow and fitting is pretty similar to above, but just using our new specification of a decision tree instead. This makes our code more re-usable and easier to maintain.\n\ntree_workflow &lt;- workflow() %&gt;%\n  add_model(tree_spec) %&gt;% \n  add_recipe(er_recipe) # Re-use the same recipe as above\n\n\ntidymodels_tree_fit &lt;- tree_workflow %&gt;%\n  fit(data = er_train)\n\nTaking a look at the result show that we get the same output as when we used rpart manually.\n\ntidymodels_tree_fit\n\n‚ïê‚ïê Workflow [trained] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: decision_tree()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n0 Recipe Steps\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nn= 847 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 847 193 Positive (0.7721370 0.2278630)  \n  2) ESR1&gt;=11.47898 648  20 Positive (0.9691358 0.0308642) *\n  3) ESR1&lt; 11.47898 199  26 Negative (0.1306533 0.8693467) *\n\n\nAnd the prediction code is the same, but using the model we have just created.\n\ntidymodels_tree_fit %&gt;% \n  predict(new_data = er_test, type = \"class\") %&gt;% ## make predictions from the test data\n  bind_cols(er_test) # Add the predicted class\n\n# A tibble: 213 √ó 4\n   .pred_class ER        ESR1 ER_Numeric\n   &lt;fct&gt;       &lt;fct&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n 1 Positive    Positive 12.5           1\n 2 Positive    Positive 13.3           1\n 3 Negative    Negative  9.26          0\n 4 Negative    Negative  9.83          0\n 5 Positive    Positive 14.3           1\n 6 Positive    Positive 13.8           1\n 7 Positive    Positive 12.5           1\n 8 Positive    Positive 15.3           1\n 9 Positive    Positive 14.7           1\n10 Positive    Positive 15.0           1\n# ‚Ñπ 203 more rows"
  },
  {
    "objectID": "posts/2025_11_17_tidymodels_TCGA_part3/index.html",
    "href": "posts/2025_11_17_tidymodels_TCGA_part3/index.html",
    "title": "Tidymodels for omics data: Part 3a",
    "section": "",
    "text": "In the first section in this series we described how to download TCGA data for breast cancers and manipulated them using a combination of tidybulk and dplyr to retain a set of expressed, variable genes plus a set of known cancer genes.\nThere is a very extensive set of clinical information recorded for each sample / patient, but to keep things simple we will start with a task for being able to predict Estrogen Receptor status from the expression data, which can be used as an indicator of whether a patient will respond to certain treatments. This is clearly not going to get us a Nature paper or a Nobel prize, but it should work well and introduce some of the key concepts of machine learning. There are a set of packages that we will need:-\n\nif(!require(tidymodels)) install.packages(\"tidymodels\")\n\nif(!require(dplyr)) install.packages(\"dplyr\")\nif(!require(ggplot2)) install.packages(\"ggplot2\")\n\nYou also need the processed data from the first section and the code to download this is:-\n\n### get the saved RDS\ndir.create(\"raw_data\", showWarnings = FALSE)\nif(!file.exists(\"raw_data/brca_train_tidy.rds\")) download.file(\"https://github.com/markdunning/markdunning.github.com/raw/refs/heads/master/posts/2025_11_06_tidymodels_TCGA_part1/brca_train_tidy.rds\", destfile = \"raw_data/brca_train_tidy.rds\")\nif(!file.exists(\"raw_data/brca_test_tidy.rds\")) download.file(\"https://github.com/markdunning/markdunning.github.com/raw/refs/heads/master/posts/2025_11_06_tidymodels_TCGA_part1/brca_test_tidy.rds\", destfile = \"raw_data/brca_test_tidy.rds\")\n\nLoad the pre-prepared tidy data\n\nbrca_train_tidy &lt;- readRDS(\"raw_data/brca_train_tidy.rds\")\nbrca_test_tidy &lt;- readRDS(\"raw_data/brca_test_tidy.rds\")\n\nThe previous section was hopefully a gentle introduction to Machine Learning, and we didn‚Äôt set the bar too high for what for our task. The simple models we built using ESR1 to classify breast cancer patients into Estrogen Receptor Positive or Negative were extremely effective (as measured by accuracy, sensitivity and specificity). In reality, faced with this problem there wouldn‚Äôt be much be much justification in exploring other models when simple models, that are easily explainable perform very well.\nRather than congratulating ourselves lets see how we can cope without ESR1 in our dataset, or any genes that correlate highly with it.\n\nbrca_train_tidy %&gt;% \n  select(CLEC3A:BRINP2) %&gt;% \n  cor() %&gt;% \n  data.frame() %&gt;% \n  tibble::rownames_to_column(\"Cor_with_Gene\") %&gt;% \n  select(ESR1, Cor_with_Gene) %&gt;% \n  slice_max(ESR1, n = 10) \n\n        ESR1 Cor_with_Gene\n1  1.0000000          ESR1\n2  0.8961402    AL078582.2\n3  0.8397965          AGR3\n4  0.7097549          AGR2\n5  0.6985503         GFRA1\n6  0.6948485     SERPINA11\n7  0.6874275       CYP2B7P\n8  0.6854794    AC093838.1\n9  0.6759798     LINC01087\n10 0.6756978          TFF1\n\n\nLet‚Äôs proceed by picking genes with an absolute correlation &lt; 0.7 with ESR1 as our possible predictors.\n\nkept_genes &lt;- brca_train_tidy %&gt;% \n  select(CLEC3A:BRINP2) %&gt;% \n  cor() %&gt;% \n  data.frame() %&gt;% \n  tibble::rownames_to_column(\"Cor_with_Gene\") %&gt;% \n  select(ESR1, Cor_with_Gene) %&gt;% #\n  filter(abs(ESR1) &lt;0.7) %&gt;% \n  pull(Cor_with_Gene)\n\nNow we restrict our data to just these genes, and the Estrogen Receptor status (renamed to ER for convenience).\n\ner_train &lt;- brca_train_tidy %&gt;% \n  select(all_of(kept_genes), ER = er_status_by_ihc) %&gt;% \n  mutate(ER = as.factor(ER))\n\ner_test &lt;- brca_test_tidy %&gt;% \n  select(all_of(kept_genes), ER = er_status_by_ihc) %&gt;% \n  mutate(ER = as.factor(ER))"
  },
  {
    "objectID": "posts/2025_11_17_tidymodels_TCGA_part3/index.html#pre-processing",
    "href": "posts/2025_11_17_tidymodels_TCGA_part3/index.html#pre-processing",
    "title": "Tidymodels for omics data: Part 3",
    "section": "",
    "text": "The object brca_gene_filtered_SE.rds is a SummarizedExperiment, which is a specialised object type used for RNA-seq and other omics type data. The tidybulk package can be used to convert this into a ‚Äútidy‚Äù format for easy manipulation with the tidyverse set of package. A tidy format is also ameanable to fitting models in R, as the general format of a model is in the form y ~ x with y and x being columns some data frame. y is often referred to as the response variable and x as the predictor variable.\n\nlibrary(tidybulk)\nlibrary(SummarizedExperiment)\n\nbrca_gene_filtered &lt;- readRDS(\"raw_data/brca_gene_filtered_SE.rds\")\n\nbc_data &lt;- brca_gene_filtered |&gt;\n  keep_variable(top = 100) |&gt;\n  tidybulk() |&gt;\n  dplyr::select(.sample,normalised_count = fpkm_uq, ER = er_status_by_ihc, gene_name) |&gt;\n  dplyr::mutate(normalised_count = log2(normalised_count + 1)) |&gt;\n  dplyr::filter(ER %in% c(\"Positive\",\"Negative\")) |&gt; \n  dplyr::mutate(ER  = forcats::fct_rev(ER)) |&gt;\n  tidyr::pivot_wider(id_cols = c(.sample, ER),names_from = gene_name, values_from = normalised_count) |&gt;\n  dplyr::select(-.sample)\n\n\n\nAs we saw previous, it is imperative that we train and test our models on completely seperate datasets\n\nlibrary(tidymodels)\n\nset.seed(42) \ndata_split &lt;- initial_split(bc_data, \n                            prop = 0.80, \n                            strata = ER)\n\n# Create the training data set\nbc_train &lt;- training(data_split)\n\n# Create the testing data set\nbc_test &lt;- testing(data_split)\n\nNew specification for GLMNet\n\nlibrary(tidymodels)\n#Use logistic_reg with a penalty and set it to Lasso (L1 regularization)\n# Fixed Lasso Specification: Use a small, fixed penalty value (e.g., 0.01)\nlasso_spec_fixed &lt;- logistic_reg(\n  penalty = 0.01,  # Fixed penalty (lambda)\n  mixture = 1       \n) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"classification\")\n\nNew recipe with removing correlated features\n\ner_recipe_multigene &lt;- recipe(ER ~ ., data = bc_train) %&gt;%\n  # Add a step to remove highly correlated features\n  step_corr(all_predictors(), threshold = 0.9)\n\nFitting the model to the train data\n\n# Define the workflow using the fixed specification and your multi-gene recipe\nlasso_workflow_fixed &lt;- workflow() %&gt;%\n  add_model(lasso_spec_fixed) %&gt;%\n  add_recipe(er_recipe_multigene)\n\n# Fit the model directly to your training data\nfixed_lasso_fit &lt;- lasso_workflow_fixed %&gt;%\n  fit(data = bc_train)\n\nLook at the coefficients\n\n# View the coefficients to see the shrinkage/zeroing effect\ntidy(fixed_lasso_fit) |&gt; arrange(desc(abs(estimate)))\n\n# A tibble: 89 √ó 3\n   term        estimate penalty\n   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)   1.84      0.01\n 2 ESR1         -0.696     0.01\n 3 AGR3         -0.222     0.01\n 4 SLC7A2       -0.0830    0.01\n 5 IGLV1-40      0.0572    0.01\n 6 CYP2T1P      -0.0507    0.01\n 7 S100A8        0.0463    0.01\n 8 CLEC3A       -0.0407    0.01\n 9 GABRP         0.0309    0.01\n10 FDCSP         0.0293    0.01\n# ‚Ñπ 79 more rows\n\n\n\nclass_metrics &lt;- metric_set(accuracy, specificity, sensitivity)\n\nfixed_lasso_fit |&gt;\n  predict(new_data = bc_test) |&gt;\n  bind_cols(bc_test) |&gt;\n  class_metrics(truth = ER, estimate = .pred_class)\n\n# A tibble: 3 √ó 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.934\n2 specificity binary         0.939\n3 sensitivity binary         0.933"
  },
  {
    "objectID": "posts/2025_11_17_tidymodels_TCGA_part3/index.html#fitting-a-logistic-regression",
    "href": "posts/2025_11_17_tidymodels_TCGA_part3/index.html#fitting-a-logistic-regression",
    "title": "Tidymodels for omics data: Part 2",
    "section": "Fitting a Logistic Regression",
    "text": "Fitting a Logistic Regression\nThe curve actually represents a series of probabilities between 0 and 1 which can be used to assign to a particular point to either Positive or Negative group. If the probability is closer to 1 for a given observation then it is more likely to belong to the Positive class, and if the probability is close to 0 then it is more likely to be Negative. The definition of ‚Äúclose‚Äù typically means &gt; 0.5 belong to Positive but we can change this.\nSo let‚Äôs look at the code to fit such a curve using a technique called logistic regression within R. We will use the glm function that has a similar interface to the lm function we saw briefly. Setting family = binomial is required as it tells glm to fit the S-shaped curve that we need.\nNote that we use the training portion of our data, er_train to fit the model.\n\nsimple_logit_fit &lt;- glm(\n  # Formula: Outcome ~ Predictor,\n  ER_Numeric ~ ESR1, \n  data = er_train, \n  #set the \"family\" to binomial for logistic regression\n  family = \"binomial\"\n)\n\n# View the model summary\nsummary(simple_logit_fit)\n\n\nCall:\nglm(formula = ER_Numeric ~ ESR1, family = \"binomial\", data = er_train)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.85399    0.26155  -10.91   &lt;2e-16 ***\nESR1         1.35386    0.09254   14.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 909.14  on 846  degrees of freedom\nResidual deviance: 312.88  on 845  degrees of freedom\nAIC: 316.88\n\nNumber of Fisher Scoring iterations: 7\n\n\nThe interpretation is a bit trickier because we don‚Äôt have a slope and intercept with this kind of model. Instead the coefficient for ESR1 signifies how the log-odds of being Positive increases as the level of ESR1 increase by one unit. The odds are more intuitive and these can be calculated with:-\n\nlog_odds &lt;- coef(simple_logit_fit)[2]\nodds_ratio &lt;- exp(log_odds)\nodds_ratio\n\n   ESR1 \n3.87236 \n\n\nThe odds ratio is 3.87236 meaning the increasing ESR1 expression means you are around 4 times more likely to be ER positive than negative. Now that we have built the model we can use it to make predictions given a set of ESR1. If we want to make sure our model has not been biased by any specific patterns only observed in our training data we should predict using a set of data the model has not ‚Äúseen‚Äù before. This is why we split our data into training and testing sets. If we set type = response the result will be probability for each observation in the testing set.\n\ntest_probabilities &lt;- predict(\n  simple_logit_fit, \n  newdata = er_test, \n  type = \"response\" # 'response' gives probabilities\n)\ntest_probabilities[1:10]\n\n        1         2         3         4         5         6         7         8 \n0.8156191 0.9108220 0.1506655 0.2125649 0.9757988 0.9515301 0.8307946 0.9932624 \n        9        10 \n0.9862456 0.9904669 \n\n\nWe can now plot the probabilities against the respective ESR1 value, which hopefully gives the S-shaped curve we are trying to fit. To convert the probabilities to a Positive or Negative label we set a threshold such as 0.5. In the below plot we also mark the predictions that are incorrect.\n\ner_test |&gt;\n  mutate(Prob = test_probabilities, `Actual Label` = ER) |&gt;\n  mutate(`Predicted Label` = ifelse(test_probabilities &gt; 0.5, \"Positive\", \"Negative\")) |&gt;\n  mutate(`Correct Prediction` = as.factor(ER == `Predicted Label`)) |&gt;\n  ggplot(aes(x = ESR1, y = Prob, col = `Actual Label`, shape = `Correct Prediction`, size = `Correct Prediction`, alpha=`Correct Prediction`)) + \n  geom_point() + \n  geom_hline(yintercept = 0.5, lty = 2) + \n  scale_shape_manual(values = c(4, 16)) +\n  scale_size_manual(values = c(5,2)) + \n  scale_alpha_manual(values = c(1,0.4))"
  },
  {
    "objectID": "posts/2025_11_17_tidymodels_TCGA_part3/index.html#evaluating-the-fit",
    "href": "posts/2025_11_17_tidymodels_TCGA_part3/index.html#evaluating-the-fit",
    "title": "Tidymodels for omics data: Part 2",
    "section": "Evaluating the fit",
    "text": "Evaluating the fit\nOn inspection, it looks there are fewer cases that should be Negative that have been labeled as Positive, than Positive cases labeled as Negative. Overall though, most of the predictions look correct. To formalise this and attach some metrics we can create what is called a confusion matrix. To create this we can use the yardstick package that is included as part of tidymodels.\n\nlibrary(yardstick)\ner_test %&gt;% \n  mutate(Predicted_ER = factor(ifelse(test_probabilities &gt; 0.5, \"Positive\", \"Negative\"), levels = c(\"Positive\",\"Negative\"))) %&gt;% \n  conf_mat(ER, Predicted_ER)\n\n          Truth\nPrediction Positive Negative\n  Positive      154        3\n  Negative       10       46\n\n\nThe numbers in the table have names and meanings associated with them:-\n\nTrue Negatives (TN) 46 Correctly predicted Negative\nTrue Positives (TP) 154 Correctly predicted Positive\nFalse Positives (FP) 3 Incorrectly predicted Positive when the tumor was actually Negative. Also known as Type I Error\nFalse Negatives (FN) 10 Incorrectly predicted Negative when the tumor was actually Positive. Also known as Type II Error\n\nThree common metrics for classification problems such as this are:-\n\nAccuracy = Accuracy is the total number of correct predictions divided by the total samples.\n\n\\(TP  +TN / (Total)\\) = \\(154 + 46 / 213\\) $$93.3%\n\nSensitivity (True Positive Rate) = how well the model finds all the actual Positive cases.\n\n\\(TP  / (TP + FN)\\) = \\(154 / (154  + 10)\\) $$93.3%\n\nSpecificity (True Negative Rate) = how well the model avoids incorrectly classifying actual Negative cases.\n\n\\(TN  / (TN + FP)\\) = \\(46 / (46  + 3)\\) $$93.3%\n\n\nFortunately we don‚Äôt need to type all those equations by hand as the yardstick package will allow us to define a set of metrics and use these to evaluate our predictions. This pacakge is just one of the many that comprise the tidymodels ecosystem.\n\nclass_metrics &lt;- metric_set(accuracy, sensitivity, specificity)\n\ner_test %&gt;% \n  mutate(Predicted_ER = factor(ifelse(test_probabilities &gt; 0.5, \"Positive\", \"Negative\"), levels = c(\"Positive\",\"Negative\"))) %&gt;% \n  class_metrics(truth=ER, estimate = Predicted_ER) \n\n# A tibble: 3 √ó 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.939\n2 sensitivity binary         0.939\n3 specificity binary         0.939\n\n\nThe metrics have all been calculated on the basis of using a probability of 0.5 to classify samples as Positive or Negative. This was a fairly arbitrary choice and we could experiment with other values. To save us time, the roc_curve function will calculate the specificity and sensitivity for a range of thresholds.\n\ner_test %&gt;% \n  mutate(Prob = test_probabilities) %&gt;% \n  roc_curve(ER, Prob) %&gt;% \n  slice_head(n = 10)\n\n# A tibble: 10 √ó 3\n   .threshold specificity sensitivity\n        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1  -Inf           0            1    \n 2     0.0569      0            1    \n 3     0.0590      0            0.994\n 4     0.0592      0.0204       0.994\n 5     0.0616      0.0408       0.994\n 6     0.0630      0.0612       0.994\n 7     0.0633      0.0816       0.994\n 8     0.0646      0.0816       0.988\n 9     0.0649      0.102        0.988\n10     0.0664      0.122        0.988\n\n\nWe can see for example setting a threshold close to 0 means that all Positive cases are identified, but the specificity is miserable as there are too many false positives. Plotting sensitivity against 1 - specificity (the False Positive rates) gives a very famous curve called the ROC curve (‚ÄúReceiver Operating Characteristics‚Äù). We can create this plot using the autoplot function after using the roc_curve function.\n\n\n\n\n\n\nNote\n\n\n\nIn case you were wondering where this name originates:- From wikipedia\n\nThe ROC curve was first developed by electrical engineers and radar engineers during World War II for detecting enemy objects in battlefields, starting in 1941, which led to its name (‚Äúreceiver operating characteristic‚Äù).\n\n\n\n\ner_test %&gt;% \n  mutate(Prob = test_probabilities) %&gt;% \n  roc_curve(ER, Prob) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\nA curve near the top-left corner indicates a model with high discriminatory power. i.e.¬†the model can achieve high Sensitivity (finding true positives) without incurring a significant cost in Specificity (avoiding false positives). If the curve is close to the diagonal line then it is not much better than guessing.\n\n\n\n\n\n\nImportant\n\n\n\nWhen deciding the threshold there is usually a trade-off between specificity and sensitivity. Do you want to make sure that you capture all your positive cases at the expense of a few false positives? In which case you would lower the threshold.\nOr do you want to be absolutely sure about the cases you identify, at the expense of missing a few positives? If so, then increase the threshold.\nIf a treatment following a positive diagnosis is invasive, expensive, or has severe side effects, you want to be highly certain before proceeding. Here, the cost of a False Positive (treating a healthy person) might be much higher than the cost of a False Negative.\nOn the other hand you are developing a cancer screening test, which if positive would lead to further investigations, you might want to lower threshold so you don‚Äôt ignore any potential true cases at an early stage. In this scenario, the cost of a few unnecessary follow-up procedures (False Positive) is deemed acceptable compared to the devastating cost of missing an early-stage cancer (False Negative).\n\n\nThe ROC curve also leads to another diagnostic metric that can be used to assess how effective our model is at predicting. Given that we want the curve to be in the top-left, that area under the curve or AUC is an important measure. Since both axes are limited between 0 and 1 the maximum possible area is 1, and the closer to 1 we are means a better model.\n\ner_test %&gt;% \n  mutate(Prob = test_probabilities) %&gt;% \n  roc_auc(ER, Prob)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.964\n\n\nWe‚Äôve now seen several metrics; accuracy, sensitivity, specificity and AUC. Some or all of these can be used to assess our model. Furthermore they can be used to compare different types of model as they can be calculated on the results of applying other statistical methods to classify our data."
  },
  {
    "objectID": "posts/2025_11_17_tidymodels_TCGA_part3/index.html#model-specification",
    "href": "posts/2025_11_17_tidymodels_TCGA_part3/index.html#model-specification",
    "title": "Tidymodels for omics data: Part 2",
    "section": "Model Specification",
    "text": "Model Specification\ntidymodels recommends that models are specified in a particular manner. This defines the particular statistical modeling approach to be used (Logistic regression or decision tree, or others), the particular package to be used (the engine) and type of modeling task to be performed (classication or regression). The specifications for the models we have used above for logistic regression:-\n\n# Specify a logistic regression model\nlogit_spec &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%          # The underlying R function to use\n  set_mode(\"classification\")     # The task: predicting a class\n\nand the decision tree:-\n\ndecision_tree_spec &lt;- decision_tree() %&gt;% \n  set_engine(\"rpart\") %&gt;% \n  set_mode(\"classification\")"
  },
  {
    "objectID": "posts/2025_11_17_tidymodels_TCGA_part3/index.html#recipes-and-workflows",
    "href": "posts/2025_11_17_tidymodels_TCGA_part3/index.html#recipes-and-workflows",
    "title": "Tidymodels for omics data: Part 2",
    "section": "Recipes and workflows",
    "text": "Recipes and workflows\n‚ÄúRecipes‚Äù typically cover the pre-processing steps such as transforming variables onto a suitable scale (e.g.¬†log\\(_2\\) transformation) and removing variables with low variability. However, we have already done this using tidybulk so our recipe is very short. For other projects I suggest looking at the official documentation\n\nhttps://www.tidymodels.org/start/recipes/#recipe\n\nPreviously when using glm for logistic regression we had to use a numeric form of the variable we are trying to predict (i.e.¬†0 or 1 for Negative and Positive respectively). However, tidymodels prefers a factor and will perform any conversions internally if needs be.\n\n# We assume the log-transform has already been applied outside the recipe\ner_recipe &lt;- recipe(ER ~ ESR1, data = er_train) # %&gt;% \n  # step_normalize() - scale the data\n  # step_nzv - remove features with low variance.\n\n‚ÄúWorkflows‚Äù in tidymodels are a good idea because they are the centralised container that brings together the essential parts of the modeling process: the model definition and the data processing recipe. They ensure your work is consistent, easy to manage, and scalable. The workflow itself doesn‚Äôt contain the code to fit the model, but it contains all the instructions and objects needed for the fit() function to execute the model fitting."
  },
  {
    "objectID": "posts/2025_11_17_tidymodels_TCGA_part3/index.html#fitting-the-model-and-predicting",
    "href": "posts/2025_11_17_tidymodels_TCGA_part3/index.html#fitting-the-model-and-predicting",
    "title": "Tidymodels for omics data: Part 2",
    "section": "Fitting the model and predicting",
    "text": "Fitting the model and predicting\nHere is the code for making a workflow to define and fit the logistic regression:-\n\n## Create the workflow\nlogit_workflow &lt;- workflow() %&gt;%\n  add_model(logit_spec) %&gt;%\n  add_recipe(er_recipe)\n\n# Fit the workflow to the training data (i.e. train the model)\ner_logit_fit &lt;- logit_workflow %&gt;%\n  fit(data = er_train)\n\nPrinting the fitted model itself shows some of the output that we have seen previously.\n\ner_logit_fit\n\n‚ïê‚ïê Workflow [trained] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: logistic_reg()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n0 Recipe Steps\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n(Intercept)         ESR1  \n      2.854       -1.354  \n\nDegrees of Freedom: 846 Total (i.e. Null);  845 Residual\nNull Deviance:      909.1 \nResidual Deviance: 312.9    AIC: 316.9\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe eagle-eyed of you might have noticed that the coefficients calculated using the tidymodels form of the model have an opposite sign to the original fit. This arises because tidymodels did some internal conversion of our numeric ER values to a factor. In our original logistic model fit we set the ‚Äúlevels‚Äù of the factor so that Positive came before Negative. This was to help with the interpretation of the model. However, tidymodels has represented the levels in the opposite way so that ‚Äú0‚Äù is the baseline. This shouldn‚Äôt actually affect our predictions as we shall see.\n\n\nMake predictions from our testing data can now be done by ‚Äúpiping‚Äù the model we have just created into the predict function and then binding the results back to the test data.\n\ner_results &lt;- er_logit_fit %&gt;% \n  predict(new_data = er_test, type = \"class\") %&gt;% ## make predictions from the test data\n  bind_cols(er_test) # Add the predicted class\n\n# View the first few rows of the results\nhead(er_results)\n\n# A tibble: 6 √ó 4\n  .pred_class ER        ESR1 ER_Numeric\n  &lt;fct&gt;       &lt;fct&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n1 Positive    Positive 3.21           1\n2 Positive    Positive 3.82           1\n3 Negative    Negative 0.831          0\n4 Negative    Negative 1.14           0\n5 Positive    Positive 4.84           1\n6 Positive    Positive 4.31           1\n\n\nThe result can then be evaluated using the metrics we defined previously.\n\ner_results %&gt;%\n  class_metrics(truth = ER, estimate = .pred_class)\n\n# A tibble: 3 √ó 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.939\n2 sensitivity binary         0.939\n3 specificity binary         0.939\n\n\nThankfully we reach the same results as before, but using a coding framework that is a bit more flexible and in line with other styles of R programming we may be familiar with from packages such as dplyr and ggplot2."
  },
  {
    "objectID": "posts/2025_11_17_tidymodels_TCGA_part3/index.html#decision-tree-using-tidymodels",
    "href": "posts/2025_11_17_tidymodels_TCGA_part3/index.html#decision-tree-using-tidymodels",
    "title": "Tidymodels for omics data: Part 2",
    "section": "Decision tree using tidymodels",
    "text": "Decision tree using tidymodels\nTo use a decision tree rather than a logistic regression we first have to create a new model specification. To keep things consistent we‚Äôll use rpart to fit the model, but note that we‚Äôre not fitting the model at this point but just saying that rpart will be used.\n\ntree_spec &lt;- decision_tree() %&gt;%\n  set_engine(\"rpart\") %&gt;%       # We will use the 'rpart' package to actually fit the model\n  set_mode(\"classification\")    # Specify the task type\n\nCreating a workflow and fitting is pretty similar to above, but just using our new specification of a decision tree instead. This makes our code more re-usable and easier to maintain.\n\ntree_workflow &lt;- workflow() %&gt;%\n  add_model(tree_spec) %&gt;% \n  add_recipe(er_recipe) # Re-use the same recipe as above\n\n\ntidymodels_tree_fit &lt;- tree_workflow %&gt;%\n  fit(data = er_train)\n\nTaking a look at the result show that we get the same output as when we used rpart manually.\n\ntidymodels_tree_fit\n\n‚ïê‚ïê Workflow [trained] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: decision_tree()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n0 Recipe Steps\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nn= 847 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 847 193 Positive (0.77213695 0.22786305)  \n  2) ESR1&gt;=2.365644 646  19 Positive (0.97058824 0.02941176) *\n  3) ESR1&lt; 2.365644 201  27 Negative (0.13432836 0.86567164) *\n\n\nAnd the prediction code is the same, but using the model we have just created.\n\ntidymodels_tree_fit %&gt;% \n  predict(new_data = er_test, type = \"class\") %&gt;% ## make predictions from the test data\n  bind_cols(er_test) # Add the predicted class\n\n# A tibble: 213 √ó 4\n   .pred_class ER        ESR1 ER_Numeric\n   &lt;fct&gt;       &lt;fct&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n 1 Positive    Positive 3.21           1\n 2 Positive    Positive 3.82           1\n 3 Negative    Negative 0.831          0\n 4 Negative    Negative 1.14           0\n 5 Positive    Positive 4.84           1\n 6 Positive    Positive 4.31           1\n 7 Positive    Positive 3.28           1\n 8 Positive    Positive 5.80           1\n 9 Positive    Positive 5.26           1\n10 Positive    Positive 5.54           1\n# ‚Ñπ 203 more rows"
  },
  {
    "objectID": "posts/2025_11_06_tidymodels_TCGA_part1/index.html#preparing-the-data-for-machine-learning",
    "href": "posts/2025_11_06_tidymodels_TCGA_part1/index.html#preparing-the-data-for-machine-learning",
    "title": "Tidymodels for omics data: Part 1",
    "section": "Preparing the data for Machine Learning",
    "text": "Preparing the data for Machine Learning\nThe machine learning tasks we have planned require a slightly different strategy for processing. Firstly, whist FPKM are an easy way to compare expression across different gene lengths and library sizes for visualization, it‚Äôs not a statistically robust input for advanced modeling tasks. For the most reliable and highest-performing model(s), particularly when using linear methods the input data must adhere to statistical assumptions (such as a stable variance across the whole range of counts). Furthermore, FPKM incorporates cohort-level information during calibration. A central tenent of Machine Learning is that data are dividing into training and testing sets that crucially are processed completely independently such that no information from the testing set is used during training. The cohort-level nature of FPKM violates this assumption and potentially leads to an issue called ‚Äúdata leakage‚Äù which could compromise the validity of our models and lead to overly-optimistic predictions.\nWe will therefore take the raw counts from TCGA as our stating point. Our approach will be to first split the data into independent training and testing sets, and then process the training data. Our testing data will be transformed using parameters learnt from the training data. Finally, we will state our intentions of modeling the Estrogen Receptor (ER) status and therefore include only samples where we have a Positive or Negative observations for this variable.\nFirst we will create a matrix of raw counts of all available samples.\n\nlibrary(tidymodels)\nlibrary(DESeq2)\n\nset.seed(42)\n\n## Load the full dataset we created previously\n\nbrca.data_full &lt;- readRDS(\"brca.data_full.rds\")\n\ntcga_brca.clin &lt;- readr::read_tsv(\"tcga_brca.clin.tsv\", show_col_types = FALSE)\n\n# get the gene annotations stored in the full data\ngene_info &lt;- rowData(brca.data_full)\n\n## Join the sample information extra clinical variables\n## Add the barcode as rownames, which will make subsetting easier\n\nsample_info &lt;- colData(brca.data_full) |&gt;\n  data.frame() |&gt;\n  dplyr::left_join(tcga_brca.clin, by = c(\"patient\"=\"bcr_patient_barcode\")) %&gt;% \n  filter(er_status_by_ihc %in% c(\"Positive\",\"Negative\")) %&gt;% \n  tibble::column_to_rownames(\"barcode\")\n\nraw_assay &lt;- assay(brca.data_full, \"unstranded\")[,rownames(sample_info)] \nrownames(raw_assay) &lt;- gene_info$gene_name\n\nIt is possible to use the sample function from R to pick rows from our dataset, or even the slice_sample from dplyr, but the most straightforward way of splitting the data into training and testing is using the tidymodels package. We will learn lots about this package in due course. The same way that tidyverse is a collection of packages with a common philosophy for data manipulation and visualisaton, tidymodels is a ecosystem of packages for all steps of machine learning. The first task is often to split data into training and testing sets, which is performed by the initial_split function after loading tidymodels. During the split it is common to use the majority (say 80%) of the data for training. You can also make sure that the your outcome of interest has roughly the same proportion in training and testing.\nOnce a split is created with initial_split you can extract the training and testing data. We actually just want the barcodes of the training and testing sets, and we can use these to create DESeq2 objects from the raw counts.\n\nlibrary(tidymodels)\nlibrary(DESeq2)\n\nset.seed(42)\n\ndata_split &lt;- data.frame(Sample = rownames(sample_info), ER = sample_info$er_status_by_ihc) %&gt;% \n  initial_split(prop = 0.8, strata  = \"ER\")\n\ntraining_samples &lt;- training(data_split) %&gt;% pull(Sample)\ntesting_samples &lt;- testing(data_split) %&gt;% pull(Sample)\n\nbrca_training &lt;- DESeqDataSetFromMatrix(raw_assay[,training_samples],\n                                        sample_info[training_samples,],\n                                        design = ~er_status_by_ihc)\n\nbrca_testing &lt;- DESeqDataSetFromMatrix(raw_assay[,testing_samples],\n                                        sample_info[testing_samples,],\n                                        design = ~er_status_by_ihc)\n\nWe now perform the statistical normalization, variance stabilization, and feature selection steps, ensuring that all parameters are calculated solely from the training data before being applied consistently to both sets.\nThe DESeq2 function estimateSizeFactors() calculates a normalization factor for each sample (a column) in the training set. This accounts for differences in sequencing depth (library size). This must be calculated first, as it is needed for the subsequent steps. The estimateDispersions() function then calculates the gene-wise biological variability. Critically, it fits a function that models the relationship between a gene‚Äôs mean expression and its variance across the entire training cohort. Once estimated, this function can be added directly to the testing data to ensure both training and testing data are processed the same.\nWe convert the normalized counts to a log\\(_2\\) scale (adding the essential \\(+1\\) pseudo-count to handle zeros). This transformation helps stabilize the variance but, more importantly here, makes the variance calculation reflective of biological fold change rather than raw magnitude. We choose the Top 250 Most Variable Genes for two reasons to combat the ‚ÄúCurse of Dimensionality‚Äù by reducing the feature space. This step calculates the variability only on the training data, preventing ‚Äúleakage‚Äù. The resulting list, most_var_genes, is the filter to be used for both sets.\nestimateSizeFactors() is run independently on the test data, which is fine because size factors are a sample-specific property (based on that sample‚Äôs own depth) and do not leak cohort information.\n\nbrca_training &lt;- estimateSizeFactors(brca_training)\nbrca_training &lt;- estimateDispersions(brca_training)\n\nnormalised_counts &lt;- log2(counts(brca_training, normalized = TRUE) + 1)\n\nmost_var_genes &lt;- rownames(normalised_counts)[order(rowVars(normalised_counts),decreasing = TRUE)[1:250]]\n\nbrca_testing &lt;- estimateSizeFactors(brca_testing)\ndispersionFunction(brca_testing) &lt;- dispersionFunction(brca_training)\n\nFinally we perform the VST step to calibrate both datasets, and then convert to a ‚Äútidy‚Äù format that we will need for machine learning.\n\nvsd_train &lt;- varianceStabilizingTransformation(brca_training, blind = FALSE)\nvsd_test &lt;- varianceStabilizingTransformation(brca_testing, blind = FALSE)\n\nbrca_train_tidy &lt;- assay(vsd_train)[most_var_genes,] %&gt;% \n  t() %&gt;% \n  data.frame() %&gt;% \n  tibble::rownames_to_column(\"barcode\") %&gt;% \n  left_join(tibble::rownames_to_column(sample_info, \"barcode\"))\n\nbrca_test_tidy &lt;- assay(vsd_test)[most_var_genes,] %&gt;% \n  t() %&gt;% \n  data.frame() %&gt;% \n  tibble::rownames_to_column(\"barcode\") %&gt;% \n  left_join(tibble::rownames_to_column(sample_info, \"barcode\"))\n\nsaveRDS(brca_train_tidy, \"brca_train_tidy.rds\")\nsaveRDS(brca_test_tidy, \"brca_test_tidy.rds\")"
  },
  {
    "objectID": "posts/2025_11_17_tidymodels_TCGA_part3/index.html#decision-trees-to-random-forests",
    "href": "posts/2025_11_17_tidymodels_TCGA_part3/index.html#decision-trees-to-random-forests",
    "title": "Tidymodels for omics data: Part 3",
    "section": "",
    "text": "Specify a decision tree\n\ndecision_tree_spec &lt;- decision_tree() %&gt;% \n  set_engine(\"rpart\") %&gt;%       \n  set_mode(\"classification\")\n\n\ndecision_tree_workflow_fixed &lt;- workflow() %&gt;%\n  add_model(decision_tree_spec) %&gt;%\n  add_recipe(er_recipe_multigene)\n\n# Fit the model directly to your training data\ndecision_tree_fit &lt;- decision_tree_workflow_fixed %&gt;%\n  fit(data = bc_train)\n\ndecision_tree_fit\n\n‚ïê‚ïê Workflow [trained] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: decision_tree()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n1 Recipe Step\n\n‚Ä¢ step_corr()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nn= 847 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 847 193 Positive (0.77213695 0.22786305)  \n   2) ESR1&gt;=2.365644 646  19 Positive (0.97058824 0.02941176) *\n   3) ESR1&lt; 2.365644 201  27 Negative (0.13432836 0.86567164)  \n     6) CLEC3A&gt;=2.225815 25  12 Negative (0.48000000 0.52000000)  \n      12) IGLV3-10&lt; 3.430763 12   2 Positive (0.83333333 0.16666667) *\n      13) IGLV3-10&gt;=3.430763 13   2 Negative (0.15384615 0.84615385) *\n     7) CLEC3A&lt; 2.225815 176  15 Negative (0.08522727 0.91477273) *\n\n\n\nlibrary(rpart.plot)\nrpart.plot(extract_fit_engine(decision_tree_fit))"
  },
  {
    "objectID": "posts/2025_11_17_tidymodels_TCGA_part3/index.html#introducing-glmnet",
    "href": "posts/2025_11_17_tidymodels_TCGA_part3/index.html#introducing-glmnet",
    "title": "Tidymodels for omics data: Part 3a",
    "section": "Introducing glmnet",
    "text": "Introducing glmnet\nWith the number of features (genes) we have in our dataset, even after filtering, there is still the potential for us to suffer from the ‚Äúcurse of dimensionality‚Äù and overfit our data. In other words, we could build a model so specific that it is of little use beyond the dataset that it was trained upon.\nMethods that provide regularization are very appealing for this reason because they constrain the model, helping it find the optimal combination of features. One such method that implements these concepts in tidymodels is called ‚Äúglmnet‚Äù.\nThe amount to which the contribution of less-informative features is ‚Äúshrunk‚Äù is controlled by two arguments: penalty (known as \\(\\mathbf{\\lambda}\\) in the literature) and mixture (known as \\(\\mathbf{\\alpha}\\)). \\(\\mathbf{\\lambda}\\) is a single, non-negative numerical value that controls the strength of the regularization (shrinkage) applied to the model. If \\(\\mathbf{\\lambda = 0}\\), no penalty is applied, and the model reverts to standard, unregularized Logistic Regression. If \\(\\mathbf{\\lambda &gt; 0}\\), coefficients are shrunk towards zero to prevent overfitting.\n\\(\\mathbf{\\alpha}\\) controls how the features are shrunk and defines the characteristics of the resulting model, ranging between 0 and 1. Lasso (\\(\\mathbf{\\alpha = 1}\\)) forces the coefficients of less important genes exactly to zero, resulting in a sparse model (automatic feature selection). Ridge (\\(\\mathbf{\\alpha = 0}\\)) shrinks all coefficients towards zero but never removes any gene completely. Any value between \\(\\mathbf{0 &lt; \\alpha &lt; 1}\\) is known as the Elastic Net penalty. This blend ensures the model benefits from Lasso‚Äôs feature selection while retaining the stability that Ridge provides when dealing with highly correlated features (a common characteristic in RNA-seq data).\nValues such as these, that can take a range of values and affect the fitting of the model, are known as Hyperparameters. This is a distinct concept from a parameter, which is a quantity that is estimated during the model fit, such as the coefficients (\\(\\beta\\) values) of a regression model.\nWe will pick some values of penalty and mixture to see how the model works. In practice, we would want to use a range of values and tune our model to find the best combination, which will be a topic for another time though.\nAs with other models we have used, the first stage is to create a model specification.\n\nlibrary(tidymodels)\n\nlasso_spec_fixed &lt;- logistic_reg(\n  penalty = 0.01,  # Fixed penalty (lambda)\n  mixture = 1 # set alpha = 1 for lasso      \n) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"classification\")\n\nAs before we are using data that have already been processed using domain-specific tools, so there is no need for much in our ‚Äúrecipe‚Äù (sometimes you will see steps to features with remove small or zero variances and here). However, we need to change the formula from using a single gene to predict to using all available features. The syntax for this is ER ~., which avoids having to type all the feature names.\nHowever, we need to introduce a step that will normalize (or standardise) our features (genes) by subtracting the mean and dividing by the standard deviation. In other words to create a ‚Äúz-score‚Äù that will make all the features comparable and stop any one feature from dominating the model or obscuring the contribution from other features. Features with a higher mean or variance could also be unfairly shrunken by lasso.\n\ner_recipe_multigene &lt;- recipe(ER ~ ., data = er_train)  %&gt;% \n  step_normalize(all_predictors())\n\nWe now combine the recipe and specification into a workflow and fitting the model to the train data should hopefully follow a familiar pattern:-\n\n# Define the workflow using the fixed specification and your multi-gene recipe\nlasso_workflow_fixed &lt;- workflow() %&gt;%\n  add_model(lasso_spec_fixed) %&gt;%\n  add_recipe(er_recipe_multigene)\n\n# Fit the model directly to your training data\nfixed_lasso_fit &lt;- lasso_workflow_fixed %&gt;%\n  fit(data = er_train)\n\nPrinting the model gives us some insight into the modeling process. After printing the recipe and model specification, we see how the model is learning from the data.\n\nLambda (\\(\\lambda\\)) ‚ÄúPenalty Strength‚Äù. This is the \\(\\lambda\\) value applied at that specific step. It decreases as you move down the table.\nDf (Degrees of Freedom) This is the count of genes whose coefficients are non-zero at that \\(\\lambda\\) value.\n%Dev (% Deviance Explained) - Model Fit on Training Data. This is the percentage of deviance (a measure of error, similar to \\(R^2\\)) that is explained by the model. A higher value is better.\n\n\nfixed_lasso_fit\n\n‚ïê‚ïê Workflow [trained] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: logistic_reg()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n1 Recipe Step\n\n‚Ä¢ step_normalize()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"binomial\",      alpha = ~1) \n\n     Df  %Dev   Lambda\n1     0  0.00 0.274900\n2     1  6.70 0.250500\n3     3 13.88 0.228300\n4     5 20.31 0.208000\n5     6 25.83 0.189500\n6     6 30.60 0.172700\n7     6 34.67 0.157300\n8     8 38.26 0.143400\n9     8 41.51 0.130600\n10    8 44.33 0.119000\n11    8 46.78 0.108400\n12    9 48.95 0.098810\n13    9 50.94 0.090030\n14    9 52.69 0.082030\n15    9 54.24 0.074740\n16   12 55.71 0.068100\n17   15 57.09 0.062050\n18   15 58.34 0.056540\n19   15 59.45 0.051520\n20   16 60.50 0.046940\n21   16 61.44 0.042770\n22   19 62.32 0.038970\n23   21 63.17 0.035510\n24   21 63.96 0.032350\n25   22 64.67 0.029480\n26   25 65.31 0.026860\n27   25 65.94 0.024480\n28   26 66.52 0.022300\n29   26 67.06 0.020320\n30   26 67.57 0.018510\n31   27 68.17 0.016870\n32   30 68.76 0.015370\n33   36 69.37 0.014010\n34   36 70.00 0.012760\n35   44 70.64 0.011630\n36   47 71.47 0.010590\n37   51 72.36 0.009654\n38   57 73.25 0.008796\n39   63 74.13 0.008015\n40   63 75.03 0.007303\n41   64 75.83 0.006654\n42   67 76.63 0.006063\n43   71 77.55 0.005524\n44   73 78.48 0.005033\n45   78 79.45 0.004586\n46   86 80.46 0.004179\n\n...\nand 54 more lines.\n\n\nEven though we have told the model what value of \\(\\lambda\\) we want to use it is trying out different values of \\(\\lambda\\) and assessing the model fit at each point. In practical terms, this would allow us to tune the behaviour of the model but that is beyond the scope of this particular section. By printing the coefficients of the model we can see what genes it has shruken to zero and effectively discarded from the model and the relative influence of particular genes on the model.\n\ntidy(fixed_lasso_fit) %&gt;% arrange(estimate)\n\n# A tibble: 246 √ó 3\n   term     estimate penalty\n   &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;\n 1 IGLV6.57  -0.295     0.01\n 2 KLK5      -0.272     0.01\n 3 KIF1A     -0.215     0.01\n 4 KRT4      -0.206     0.01\n 5 GSTM1     -0.199     0.01\n 6 ZIC1      -0.146     0.01\n 7 ACTL8     -0.136     0.01\n 8 FABP7     -0.130     0.01\n 9 DLK1      -0.0998    0.01\n10 KRT81     -0.0949    0.01\n# ‚Ñπ 236 more rows\n\n\nThis can also be shown on a plot. The magnitude and sign of the coefficient (estimate) tell us something about the influence of that gene on the model, with larger (in absolute terms) estimates being more predictive.\n\ntidy(fixed_lasso_fit) %&gt;% \n  filter(term != \"(Intercept)\", estimate != 0) %&gt;% \n  ggplot(aes(y = forcats::fct_reorder(term, estimate),x = estimate, fill = estimate &gt; 0)) + geom_col() + xlab(\"Model Coefficient\") + ylab(\"Gene\")\n\n\n\n\n\n\n\n\nJust to make sure that we understand we can plot the most positive coefficients against ER and see these genes are all higher expressed in ER Positive tumours.\n\nmax_coefs &lt;- tidy(fixed_lasso_fit) %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  slice_max(estimate, n = 5) %&gt;% \n  pull(term)\n\ner_train %&gt;% \n  select(ER, all_of(max_coefs)) %&gt;% \n  pivot_longer(-ER,names_to = \"Gene\", values_to = \"count\") %&gt;% \nggplot(aes(x = ER, y = count)) + geom_boxplot() + facet_wrap(~Gene)\n\n\n\n\n\n\n\n\nConversely, genes where a negative coefficient was predicted are all higher in ER Negative (or equivalently lower in ER Positive)\n\nmin_coefs &lt;- tidy(fixed_lasso_fit) %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  slice_min(estimate, n = 5) %&gt;% \n  pull(term)\n\ner_train %&gt;% \n  select(ER, all_of(min_coefs)) %&gt;% \n  pivot_longer(-ER,names_to = \"Gene\", values_to = \"count\") %&gt;% \nggplot(aes(x = ER, y = count)) + geom_boxplot() + facet_wrap(~Gene)\n\n\n\n\n\n\n\n\nAnd the next step is of course to see how the model performs on un-seen or new data, and we created the er_test dataset for this purpose. The tidymodels eco-system uses yardstick to efficiently compile metrics and we will use accuracy, specificity and sensitivity as defined in the previous section.\n\nclass_metrics &lt;- metric_set(accuracy, specificity, sensitivity)\n\nfixed_lasso_fit |&gt;\n  predict(new_data = er_test) |&gt;\n  bind_cols(er_test) |&gt;\n  class_metrics(truth = ER, estimate = .pred_class)\n\n# A tibble: 3 √ó 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.920\n2 specificity binary         0.939\n3 sensitivity binary         0.857\n\n\nOverall the model is doing quite well even though we didn‚Äôt use ESR1. The specificity is quite high, meaning that it doesn‚Äôt pick too many false positives. On the other hand, the lower sensitivity means that some samples that are truly Positive are being missed.\n\n\n\n\n\n\nClinical context\n\n\n\nIn a clinical context, the high specificity is encouraging as you wouldn‚Äôt progress too many patients to potentially aggressive treatment they wouldn‚Äôt benefit from. On the other hand, some patients may miss the opportunity to take a potentially life-changing or life-extending treatment.\n\n\nThere is clearly an opportunity for some improvement, and possible that could be achieved by better choice of hyper-parameters. For now though lets see how other methods perform starting with the decision tree."
  },
  {
    "objectID": "posts/2025_11_17_tidymodels_TCGA_part3/index.html#decision-trees-revisited",
    "href": "posts/2025_11_17_tidymodels_TCGA_part3/index.html#decision-trees-revisited",
    "title": "Tidymodels for omics data: Part 3a",
    "section": "Decision Trees revisited",
    "text": "Decision Trees revisited\nThanks for the tidymodels philosophy, we can actually reuse quite a lot of the code from before. We first need a specification.\n\ndecision_tree_spec &lt;- decision_tree() %&gt;% \n  set_engine(\"rpart\") %&gt;%       \n  set_mode(\"classification\")\n\nThe workflow then combines the specifiction with the same recipe from above.\n\ndecision_tree_workflow_fixed &lt;- workflow() %&gt;%\n  add_model(decision_tree_spec) %&gt;%\n  add_recipe(er_recipe_multigene)\n\n# Fit the model directly to your training data\ndecision_tree_fit &lt;- decision_tree_workflow_fixed %&gt;%\n  fit(data = er_train)\n\ndecision_tree_fit\n\n‚ïê‚ïê Workflow [trained] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: decision_tree()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n1 Recipe Step\n\n‚Ä¢ step_normalize()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nn= 847 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 847 193 Positive (0.22786305 0.77213695)  \n   2) TFF1&lt; -0.8626308 192  47 Negative (0.75520833 0.24479167)  \n     4) GRPR&lt; -0.6827699 152  15 Negative (0.90131579 0.09868421) *\n     5) GRPR&gt;=-0.6827699 40   8 Positive (0.20000000 0.80000000)  \n      10) KIF1A&gt;=0.300862 11   3 Negative (0.72727273 0.27272727) *\n      11) KIF1A&lt; 0.300862 29   0 Positive (0.00000000 1.00000000) *\n   3) TFF1&gt;=-0.8626308 655  48 Positive (0.07328244 0.92671756)  \n     6) CYP2B7P&lt; -1.140518 49  22 Positive (0.44897959 0.55102041)  \n      12) LINC01087&lt; -0.1876734 26   6 Negative (0.76923077 0.23076923)  \n        24) KCNJ3&lt; -0.4372225 18   1 Negative (0.94444444 0.05555556) *\n        25) KCNJ3&gt;=-0.4372225 8   3 Positive (0.37500000 0.62500000) *\n      13) LINC01087&gt;=-0.1876734 23   2 Positive (0.08695652 0.91304348) *\n     7) CYP2B7P&gt;=-1.140518 606  26 Positive (0.04290429 0.95709571)  \n      14) DLK1&gt;=2.034831 22   9 Positive (0.40909091 0.59090909)  \n        28) HOTAIR&gt;=0.1519768 9   1 Negative (0.88888889 0.11111111) *\n        29) HOTAIR&lt; 0.1519768 13   1 Positive (0.07692308 0.92307692) *\n      15) DLK1&lt; 2.034831 584  17 Positive (0.02910959 0.97089041)  \n        30) STAC2&gt;=1.77434 8   3 Negative (0.62500000 0.37500000) *\n        31) STAC2&lt; 1.77434 576  12 Positive (0.02083333 0.97916667) *\n\n\nThe fit looks quite complex, but let‚Äôs plot it out.\n\nlibrary(rpart.plot)\nrpart.plot(extract_fit_engine(decision_tree_fit))\n\n\n\n\n\n\n\n\nNow that the decision tree doesn‚Äôt have ESR1 to rely on,the model found the next strongest predictor available: TFF1, splitting at a scaled value of \\(-0.86\\). Although TFF1 is strongly correlated with ER status, it is not powerful enough to separate the classes cleanly on its own.\nFurthermore, unlike the initial ESR1-based tree (seen in the previous section), this tree is very deep and involves splits on numerous secondary genes (GRPR, KIF1A, YP2B7P, HOTAIR, etc.). The model is relying on the cumulative effect of many weak, complex, and potentially non-linear interactions among many genes to achieve classification. There is a danger that we might be over-fitting.\nNevertheless, we can see how the model performs:-\n\ndecision_tree_fit |&gt;\n  predict(new_data = er_test) |&gt;\n  bind_cols(er_test) |&gt;\n  class_metrics(truth = ER, estimate = .pred_class)\n\n# A tibble: 3 √ó 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.901\n2 specificity binary         0.927\n3 sensitivity binary         0.816\n\n\nThe specificity is roughly the same as our Lasso from above, but the sensitivity is worse still. It seems like it is about time to introduce a method that is an evolution from the Decision Tree - that of a Random Forest."
  },
  {
    "objectID": "posts/2025_11_17_tidymodels_TCGA_part3/index.html#introducing-the-random-forest",
    "href": "posts/2025_11_17_tidymodels_TCGA_part3/index.html#introducing-the-random-forest",
    "title": "Tidymodels for omics data: Part 3a",
    "section": "Introducing the Random Forest",
    "text": "Introducing the Random Forest\nThe random forest is a natural extension of a decision tree, as it harnesses the power of many different decision tree and uses the predictions from each individual tree to form a consensus or majority vote for each sample to be classified.\nThe method constructs many decision trees (as above), each using a ‚Äúbootstrap sample‚Äù (a random sample with replacement) of the data. Therefore, each tree is a bit different and makes its own predictions without relying on others. Moreover, when tree is built it doesn‚Äôt look at all the features (genes in our case) at once. It picks a few at random to decide how to split the data. Whilst some trees might not work very well individually, the crowd-voting step of taking predictions from a large number of trees should lead to accurate predictions overall and negate concerns over individual trees overfitting data.\nIt‚Äôs specification is similar to models we have used before. We can set ranger as the method used to build the model, and this package is included as part of tidymodels. Setting importance = \"permutation\" is useful for biological interpretation as it will allow us to see which features (genes) are contributing most to the model\n\nrandom_forest_spec &lt;- rand_forest(\n    mode = \"classification\", \n    trees = 1000 # Using 1000 trees for robustness\n  ) %&gt;%\n  set_engine(\"ranger\", importance = \"permutation\")\n\nDue to the random nature of the method, we need to set a seed to ensure reproducibility before we conduct the fit\n\nset.seed(42)\n\nrandom_forest_workflow &lt;- workflow() %&gt;%\n  add_model(random_forest_spec) %&gt;%\n  add_recipe(er_recipe_multigene)\n\nrandom_forest_fit &lt;- random_forest_workflow %&gt;%\n  fit(data = er_train)\n\nrandom_forest_fit\n\n‚ïê‚ïê Workflow [trained] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: rand_forest()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n1 Recipe Step\n\n‚Ä¢ step_normalize()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1000,      importance = ~\"permutation\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  1000 \nSample size:                      847 \nNumber of independent variables:  245 \nMtry:                             15 \nTarget node size:                 10 \nVariable importance mode:         permutation \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.05419088 \n\n\nWhilst the decision tree was quite intuitive to visualise, the random forest is unfortunately less so. But let‚Äôs make some predictions and see how it performs:-\n\nrandom_forest_fit %&gt;% \n  predict(new_data = er_test) |&gt;\n  bind_cols(er_test) |&gt;\n  class_metrics(truth = ER, estimate = .pred_class)\n\n# A tibble: 3 √ó 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.925\n2 specificity binary         0.933\n3 sensitivity binary         0.898\n\n\nSo the sensitivity has improved from the decision tree, and also beats the glmnet model, with no difference in specificity. As with the Lasso output, we can get a sense of what features are most powerful using an ‚Äúimportance‚Äù score that is calculated (setting importance = \"permutation\" in the specification allows us to do this). The ranking of these scores is important, and reflects how much more or how much less this gene contributes compared to all others in your model. A positive score means a model benefits from having that gene includes, whereas a negative score means the gene actually makes a model perform worse.\nExtracting this information can be done as follows\n\nimportance_df &lt;- pull_workflow_fit(random_forest_fit)$fit$variable.importance %&gt;%\n  enframe(name = \"term\", value = \"Importance\") %&gt;%\n  arrange(desc(Importance))\nimportance_df\n\n# A tibble: 245 √ó 2\n   term       Importance\n   &lt;chr&gt;           &lt;dbl&gt;\n 1 TFF1          0.00825\n 2 SERPINA11     0.00821\n 3 POTEKP        0.00801\n 4 CYP2B7P       0.00683\n 5 LINC01087     0.00666\n 6 AC044784.1    0.00600\n 7 HORMAD1       0.00521\n 8 GP2           0.00518\n 9 SRARP         0.00508\n10 LINC01016     0.00505\n# ‚Ñπ 235 more rows\n\n\nThe genes with negative ‚Äúimportance‚Äù are in fact only very slightly so, and there only a handful of them.\n\nimportance_df %&gt;% \n  filter(Importance &lt; 0)\n\n# A tibble: 9 √ó 2\n  term       Importance\n  &lt;chr&gt;           &lt;dbl&gt;\n1 TCN1      -0.00000626\n2 LINC01291 -0.00000743\n3 BRINP2    -0.0000202 \n4 SCGB2A1   -0.0000214 \n5 ARHGAP36  -0.0000214 \n6 LEP       -0.0000225 \n7 RPSAP53   -0.0000341 \n8 ADIPOQ    -0.0000399 \n9 GSTM1     -0.0000572 \n\n\nWe can combine these results with the Lasso output from above to compare how the two methods are considering genes as important. Reassuring, those with negative importance in the random forest, with the exception of GSTM1 all had a coefficient of 0 (i.e.¬†were removed by Lasso).\n\ntidy(fixed_lasso_fit) %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  select(-penalty) %&gt;% \n  left_join(importance_df) %&gt;% \n  filter(Importance &lt; 0 )\n\n# A tibble: 9 √ó 3\n  term      estimate  Importance\n  &lt;chr&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1 GSTM1       -0.199 -0.0000572 \n2 ADIPOQ       0     -0.0000399 \n3 LINC01291    0     -0.00000743\n4 TCN1         0     -0.00000626\n5 SCGB2A1      0     -0.0000214 \n6 RPSAP53      0     -0.0000341 \n7 ARHGAP36     0     -0.0000214 \n8 LEP          0     -0.0000225 \n9 BRINP2       0     -0.0000202 \n\n\nWe can plot both together on the bar plot below. This seems to show there are some genes with high importance from the random forest and higher coefficient in Lasso. Although surprisingly we also get some important genes according to random forest that were removed by Lasso.\nI don‚Äôt think I‚Äôm going to get too concerned about this just yet as these are not our final Lasso and random forest models.\n\n## N.B. Haven't quite figured out the best visualisation for this yet\n\ntidy(fixed_lasso_fit) %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  select(-penalty) %&gt;% \n  left_join(importance_df) %&gt;% \n  filter(Importance &gt; 0 ) %&gt;% \n  ggplot(aes(y = forcats::fct_reorder(term, Importance), x = estimate, fill = estimate)) + geom_col()"
  },
  {
    "objectID": "posts/2025_11_17_tidymodels_TCGA_part3/index.html#summary",
    "href": "posts/2025_11_17_tidymodels_TCGA_part3/index.html#summary",
    "title": "Tidymodels for omics data: Part 3a",
    "section": "Summary",
    "text": "Summary\nIn this section, we built powerful classification models by taking advantage of many gene features while aiming to prevent overfitting. We employed two fundamentally different approaches:\n\nThe Lasso approach (using glmnet), which identifies features with a strong linear relationship to the outcome by driving the coefficients of less useful features to zero.\nThe Random Forest, which builds a consensus prediction from many random decision trees, allowing it to capture complex, non-linear interactions and threshold effects.\n\nWhilst both these methods achieved good performance and provided complementary insights into gene importance, they typically need some tuning to discover the best ‚Äòhyper-parameters.‚Äô This will be discussed in a later part. Before that, we will explore some other classification methods, like K-Nearest Neighbors (KNN) and Support Vector Machines (SVM), and learn how to incorporate other types of predictors (such as clinical variables like age and tumor grade) into our models."
  },
  {
    "objectID": "posts/2025_11_17_tidymodels_TCGA_part3/index.html#pre-amble",
    "href": "posts/2025_11_17_tidymodels_TCGA_part3/index.html#pre-amble",
    "title": "Tidymodels for omics data: Part 3a",
    "section": "",
    "text": "In the first section in this series we described how to download TCGA data for breast cancers and manipulated them using a combination of tidybulk and dplyr to retain a set of expressed, variable genes plus a set of known cancer genes.\nThere is a very extensive set of clinical information recorded for each sample / patient, but to keep things simple we will start with a task for being able to predict Estrogen Receptor status from the expression data, which can be used as an indicator of whether a patient will respond to certain treatments. This is clearly not going to get us a Nature paper or a Nobel prize, but it should work well and introduce some of the key concepts of machine learning. There are a set of packages that we will need:-\n\nif(!require(tidymodels)) install.packages(\"tidymodels\")\n\nif(!require(dplyr)) install.packages(\"dplyr\")\nif(!require(ggplot2)) install.packages(\"ggplot2\")\n\nYou also need the processed data from the first section and the code to download this is:-\n\n### get the saved RDS\ndir.create(\"raw_data\", showWarnings = FALSE)\nif(!file.exists(\"raw_data/brca_train_tidy.rds\")) download.file(\"https://github.com/markdunning/markdunning.github.com/raw/refs/heads/master/posts/2025_11_06_tidymodels_TCGA_part1/brca_train_tidy.rds\", destfile = \"raw_data/brca_train_tidy.rds\")\nif(!file.exists(\"raw_data/brca_test_tidy.rds\")) download.file(\"https://github.com/markdunning/markdunning.github.com/raw/refs/heads/master/posts/2025_11_06_tidymodels_TCGA_part1/brca_test_tidy.rds\", destfile = \"raw_data/brca_test_tidy.rds\")\n\nLoad the pre-prepared tidy data\n\nbrca_train_tidy &lt;- readRDS(\"raw_data/brca_train_tidy.rds\")\nbrca_test_tidy &lt;- readRDS(\"raw_data/brca_test_tidy.rds\")\n\nThe previous section was hopefully a gentle introduction to Machine Learning, and we didn‚Äôt set the bar too high for what for our task. The simple models we built using ESR1 to classify breast cancer patients into Estrogen Receptor Positive or Negative were extremely effective (as measured by accuracy, sensitivity and specificity). In reality, faced with this problem there wouldn‚Äôt be much be much justification in exploring other models when simple models, that are easily explainable perform very well.\nRather than congratulating ourselves lets see how we can cope without ESR1 in our dataset, or any genes that correlate highly with it.\n\nbrca_train_tidy %&gt;% \n  select(CLEC3A:BRINP2) %&gt;% \n  cor() %&gt;% \n  data.frame() %&gt;% \n  tibble::rownames_to_column(\"Cor_with_Gene\") %&gt;% \n  select(ESR1, Cor_with_Gene) %&gt;% \n  slice_max(ESR1, n = 10) \n\n        ESR1 Cor_with_Gene\n1  1.0000000          ESR1\n2  0.8961402    AL078582.2\n3  0.8397965          AGR3\n4  0.7097549          AGR2\n5  0.6985503         GFRA1\n6  0.6948485     SERPINA11\n7  0.6874275       CYP2B7P\n8  0.6854794    AC093838.1\n9  0.6759798     LINC01087\n10 0.6756978          TFF1\n\n\nLet‚Äôs proceed by picking genes with an absolute correlation &lt; 0.7 with ESR1 as our possible predictors.\n\nkept_genes &lt;- brca_train_tidy %&gt;% \n  select(CLEC3A:BRINP2) %&gt;% \n  cor() %&gt;% \n  data.frame() %&gt;% \n  tibble::rownames_to_column(\"Cor_with_Gene\") %&gt;% \n  select(ESR1, Cor_with_Gene) %&gt;% #\n  filter(abs(ESR1) &lt;0.7) %&gt;% \n  pull(Cor_with_Gene)\n\nNow we restrict our data to just these genes, and the Estrogen Receptor status (renamed to ER for convenience).\n\ner_train &lt;- brca_train_tidy %&gt;% \n  select(all_of(kept_genes), ER = er_status_by_ihc) %&gt;% \n  mutate(ER = as.factor(ER))\n\ner_test &lt;- brca_test_tidy %&gt;% \n  select(all_of(kept_genes), ER = er_status_by_ihc) %&gt;% \n  mutate(ER = as.factor(ER))"
  },
  {
    "objectID": "posts/2025_11_19_tidymodels_TCGA_part3b/index.html",
    "href": "posts/2025_11_19_tidymodels_TCGA_part3b/index.html",
    "title": "Tidymodels for omics data: Part 3b",
    "section": "",
    "text": "In the first section we described how to download TCGA data for breast cancers and manipulated them using a combination of tidybulk and dplyr to retain a set of expressed, variable genes plus a set of known cancer genes.\nThere is a very extensive set of clinical information recorded for each sample / patient, but to keep things simple we will start with a task for being able to predict Estrogen Receptor status from the expression data, which can be used as an indicator of whether a patient will respond to certain treatments. This is clearly not going to get us a Nature paper or a Nobel prize, but it should work well and introduce some of the key concepts of machine learning. There are a set of packages that we will need:-\n\nif(!require(tidymodels)) install.packages(\"tidymodels\")\n\n\nif(!require(dplyr)) install.packages(\"dplyr\")\nif(!require(ggplot2)) install.packages(\"ggplot2\")\n\nYou also need the processed data from the first section and the code to download this is:-\n\n### get the saved RDS\ndir.create(\"raw_data\", showWarnings = FALSE)\nif(!file.exists(\"raw_data/brca_train_tidy.rds\")) download.file(\"https://github.com/markdunning/markdunning.github.com/raw/refs/heads/master/posts/2025_11_06_tidymodels_TCGA_part1/brca_train_tidy.rds\", destfile = \"raw_data/brca_train_tidy.rds\")\nif(!file.exists(\"raw_data/brca_test_tidy.rds\")) download.file(\"https://github.com/markdunning/markdunning.github.com/raw/refs/heads/master/posts/2025_11_06_tidymodels_TCGA_part1/brca_test_tidy.rds\", destfile = \"raw_data/brca_test_tidy.rds\")\n\nNow load the pre-prepared tidy data\n\nbrca_train_tidy &lt;- readRDS(\"raw_data/brca_train_tidy.rds\")\nbrca_test_tidy &lt;- readRDS(\"raw_data/brca_test_tidy.rds\")"
  },
  {
    "objectID": "posts/2025_11_19_tidymodels_TCGA_part3b/index.html#pre-amble",
    "href": "posts/2025_11_19_tidymodels_TCGA_part3b/index.html#pre-amble",
    "title": "Tidymodels for omics data: Part 3b",
    "section": "",
    "text": "In the first section we described how to download TCGA data for breast cancers and manipulated them using a combination of tidybulk and dplyr to retain a set of expressed, variable genes plus a set of known cancer genes.\nThere is a very extensive set of clinical information recorded for each sample / patient, but to keep things simple we will start with a task for being able to predict Estrogen Receptor status from the expression data, which can be used as an indicator of whether a patient will respond to certain treatments. This is clearly not going to get us a Nature paper or a Nobel prize, but it should work well and introduce some of the key concepts of machine learning. There are a set of packages that we will need:-\n\nif(!require(tidymodels)) install.packages(\"tidymodels\")\n\n\nif(!require(dplyr)) install.packages(\"dplyr\")\nif(!require(ggplot2)) install.packages(\"ggplot2\")\n\nYou also need the processed data from the first section and the code to download this is:-\n\n### get the saved RDS\ndir.create(\"raw_data\", showWarnings = FALSE)\nif(!file.exists(\"raw_data/brca_train_tidy.rds\")) download.file(\"https://github.com/markdunning/markdunning.github.com/raw/refs/heads/master/posts/2025_11_06_tidymodels_TCGA_part1/brca_train_tidy.rds\", destfile = \"raw_data/brca_train_tidy.rds\")\nif(!file.exists(\"raw_data/brca_test_tidy.rds\")) download.file(\"https://github.com/markdunning/markdunning.github.com/raw/refs/heads/master/posts/2025_11_06_tidymodels_TCGA_part1/brca_test_tidy.rds\", destfile = \"raw_data/brca_test_tidy.rds\")\n\nNow load the pre-prepared tidy data\n\nbrca_train_tidy &lt;- readRDS(\"raw_data/brca_train_tidy.rds\")\nbrca_test_tidy &lt;- readRDS(\"raw_data/brca_test_tidy.rds\")"
  },
  {
    "objectID": "posts/2025_11_19_tidymodels_TCGA_part3b/index.html#k--nearest-neighbours-knn",
    "href": "posts/2025_11_19_tidymodels_TCGA_part3b/index.html#k--nearest-neighbours-knn",
    "title": "Tidymodels for omics data: Part 3b",
    "section": "K- Nearest Neighbours (KNN)",
    "text": "K- Nearest Neighbours (KNN)\nThe KNN method is fairly intuitive to understand in a two-dimensional space (i.e.¬†when using two features for prediction), so we will pick two genes for illustrative purposes to predict ER status. The first is our favourite ESR1 followed by TFF1 which we saw gave good predictions.\n\ner_train_simple &lt;- dplyr::select(brca_train_tidy, ER  = er_status_by_ihc, ESR1, TFF1) |&gt; \n  dplyr::mutate(ER_Numeric = ifelse(ER== \"Positive\", 1,0)) |&gt;\n  dplyr::mutate(ER  = forcats::fct_rev(ER))\n\ner_test_simple &lt;- dplyr::select(brca_test_tidy, ER  = er_status_by_ihc, ESR1, TFF1) |&gt; \n  dplyr::mutate(ER_Numeric = ifelse(ER== \"Positive\", 1,0)) |&gt;\n  dplyr::mutate(ER  = forcats::fct_rev(ER))\n\n\nIntuition\nWe can start by plotting ESR1 against TFF1 and colouring according the ER status of each patient. These genes have been chosen deliberately to give separation into ER classes. i.e.¬†higher ESR1 and TFF1 for a patient generally means and ER positive tumour.\n\nggplot(er_train_simple, aes(x = ESR1, y = TFF1, col = ER)) + geom_point()\n\n\n\n\n\n\n\n\nThe main goal of machine learning is to try and make predictions on unseen data given the information in our training dataset. So let‚Äôs consider the first such sample in our test dataset. We will ignore the existing ER classification for this sample for now.\n\nnew_sample &lt;- er_test_simple |&gt;\n  slice_head() |&gt;\n  select(-ER, -ER_Numeric)\nnew_sample\n\n      ESR1     TFF1\n1 12.53109 7.824957\n\n\nIn order to decide what ER status this sample we might be we can simply plot it on the same axes as our training data\n\nggplot(er_train_simple, aes(x = ESR1, y = TFF1, col = ER)) + geom_point() + geom_point(data=(new_sample), shape = 19,col=\"black\")\n\n\n\n\n\n\n\n\n\nnearest_training_samples &lt;- er_train_simple |&gt;\n  mutate(distance = sqrt((ESR1 - new_sample$ESR1)^2 + (TFF1 - new_sample$TFF1)^2)) |&gt;\n  arrange(distance) |&gt;\n  slice_min(distance, n = 5)\n\n\nggplot(er_train_simple, aes(x = ESR1, y = TFF1, col = ER)) + geom_point() + \n  geom_point(data=(new_sample), shape = 19,col=\"black\") +\n  geom_point(data=(nearest_training_samples), shape =4,col=\"black\", size=2)\n\n\n\n\n\n\n\n\n\nknn_spec &lt;- \n  nearest_neighbor(neighbors = 2) |&gt;\n  set_mode(\"classification\") |&gt;\n  set_engine(\"kknn\")\n\n\ner_recipe_simple &lt;- recipe(ER ~ ESR1, data = er_train_simple) # %&gt;% \n\n\nknn_workflow &lt;- \n  workflow() %&gt;%\n  add_recipe(er_recipe_simple) %&gt;% # Reusing the normalized recipe\n  add_model(knn_spec)\n\n\nknn_fit &lt;- fit(knn_workflow, data = er_train_simple)\nknn_fit\n\n‚ïê‚ïê Workflow [trained] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n0 Recipe Steps\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\nCall:\nkknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(2,     data, 5))\n\nType of response variable: nominal\nMinimal misclassification: 0.09799292\nBest kernel: optimal\nBest k: 2\n\n\n\ner_predictions_knn &lt;- \n  predict(knn_fit, new_data = er_test_simple, type = \"class\") |&gt;\n  bind_cols(predict(knn_fit, new_data = er_test_simple, type = \"prob\")) |&gt;\n  bind_cols(er_test_simple) \n\nggplot(er_predictions_knn, aes(x = ESR1, y = ER_Numeric, col = ER)) + \n  geom_line(aes(y = .pred_Positive),color=\"black\",linewidth = 1.2)"
  },
  {
    "objectID": "posts/2025_11_19_tidymodels_TCGA_part3b/index.html#svm",
    "href": "posts/2025_11_19_tidymodels_TCGA_part3b/index.html#svm",
    "title": "Tidymodels for omics data: Part 3b",
    "section": "SVM",
    "text": "SVM\nThe previous section was hopefully a gentle introduction to Machine Learning, and we didn‚Äôt set the bar too high for what for our task. The simple models we built using ESR1 to classify breast cancer patients into Estrogen Receptor Positive or Negative were extremely effective (as measured by accuracy, sensitivity and specificity). In reality, faced with this problem there wouldn‚Äôt be much be much justification in exploring other models when simple models, that are easily explainable perform very well.\nRather than congratulating ourselves lets see how we can cope without ESR1 in our dataset, or any genes that correlate highly with it.\n\nbrca_train_tidy %&gt;% \n  select(CLEC3A:BRINP2) %&gt;% \n  cor() %&gt;% \n  data.frame() %&gt;% \n  tibble::rownames_to_column(\"Cor_with_Gene\") %&gt;% \n  select(ESR1, Cor_with_Gene) %&gt;% \n  slice_max(ESR1, n = 10) \n\n        ESR1 Cor_with_Gene\n1  1.0000000          ESR1\n2  0.8961402    AL078582.2\n3  0.8397965          AGR3\n4  0.7097549          AGR2\n5  0.6985503         GFRA1\n6  0.6948485     SERPINA11\n7  0.6874275       CYP2B7P\n8  0.6854794    AC093838.1\n9  0.6759798     LINC01087\n10 0.6756978          TFF1\n\n\nLet‚Äôs proceed by picking genes with an absolute correlation &lt; 0.7 with ESR1 as our possible predictors.\n\nkept_genes &lt;- brca_train_tidy %&gt;% \n  select(CLEC3A:BRINP2) %&gt;% \n  cor() %&gt;% \n  data.frame() %&gt;% \n  tibble::rownames_to_column(\"Cor_with_Gene\") %&gt;% \n  select(ESR1, Cor_with_Gene) %&gt;% #\n  filter(abs(ESR1) &lt;0.7) %&gt;% \n  pull(Cor_with_Gene)\n\nNow we restrict our data to just these genes, and the Estrogen Receptor status (renamed to ER for convenience).\n\ner_train &lt;- brca_train_tidy %&gt;% \n  select(all_of(kept_genes), ER = er_status_by_ihc) %&gt;% \n  mutate(ER = as.factor(ER))\n\ner_test &lt;- brca_test_tidy %&gt;% \n  select(all_of(kept_genes), ER = er_status_by_ihc) %&gt;% \n  mutate(ER = as.factor(ER))\n\n\nSetting the specification for SVM\n\nsvm_spec &lt;- \n  svm_linear(cost = 1.0) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"kernlab\") # The implementation of the SVM algorithm\n\ner_recipe_multigene &lt;- recipe(ER ~ ., data = er_train)  %&gt;% \n  step_normalize(all_predictors())\n\n\n\nFitting the SVM\n\nsvm_workflow &lt;- \n  workflow() %&gt;%\n  add_recipe(er_recipe_multigene) %&gt;%\n  add_model(svm_spec)\n\n\nsvm_fit &lt;- fit(svm_workflow, data = er_train)\n\n Setting default kernel parameters  \n\n\n\n\nSVM predictions\n\ner_predictions &lt;- \n  predict(svm_fit, new_data = er_test, type = \"class\") %&gt;%\n  bind_cols(predict(svm_fit, new_data = er_test, type = \"prob\")) %&gt;%\n  bind_cols(er_test %&gt;% select(ER))\n\n\n# Calculate and print performance metrics\nsvm_metrics &lt;- metric_set(accuracy, roc_auc)\ner_predictions %&gt;%\n  svm_metrics(truth = ER, estimate = .pred_class, .pred_Positive)\n\n# A tibble: 2 √ó 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary        0.892 \n2 roc_auc  binary        0.0840"
  },
  {
    "objectID": "posts/2025_11_19_tidymodels_TCGA_part3b/index.html#other-kinds-of-predictor---proof-of-concept",
    "href": "posts/2025_11_19_tidymodels_TCGA_part3b/index.html#other-kinds-of-predictor---proof-of-concept",
    "title": "Tidymodels for omics data: Part 3b",
    "section": "Other kinds of predictor - PROOF OF CONCEPT",
    "text": "Other kinds of predictor - PROOF OF CONCEPT\n\nclin_train &lt;- select(brca_train_tidy, paper_pathologic_stage, \n       paper_age_at_initial_pathologic_diagnosis, \n       pr_status_by_ihc,\n       ER=er_status_by_ihc,\n       her2_status_by_ihc)\nclin_test &lt;- select(brca_test_tidy, paper_pathologic_stage, \n       paper_age_at_initial_pathologic_diagnosis, \n       pr_status_by_ihc,\n       ER=er_status_by_ihc,\n       her2_status_by_ihc)\n\n\nlibrary(tidymodels)\n\nclin_train &lt;- select(brca_train_tidy, paper_pathologic_stage, \n       paper_age_at_initial_pathologic_diagnosis, \n       pr_status_by_ihc,\n       ER=er_status_by_ihc,\n       her2_status_by_ihc) %&gt;% \n  filter(!is.na(paper_age_at_initial_pathologic_diagnosis)) %&gt;% \n  mutate(ER = as.factor(ER))\n\nclin_test &lt;- select(brca_test_tidy, paper_pathologic_stage, \n       paper_age_at_initial_pathologic_diagnosis, \n       pr_status_by_ihc,\n       ER=er_status_by_ihc,\n       her2_status_by_ihc) %&gt;% \n  filter(!is.na(paper_age_at_initial_pathologic_diagnosis)) %&gt;% \n  mutate(ER = as.factor(ER))\n\nlasso_spec_fixed &lt;- logistic_reg(\n  penalty = 0.01,  # Fixed penalty (lambda)\n  mixture = 1 # set alpha = 1 for lasso      \n) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"classification\")\n\n\nclin_recipe &lt;- recipe(ER ~ ., data = clin_train)  %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_unknown() %&gt;% \n  step_dummy(all_nominal_predictors())\n\n\nclin_workflow &lt;- workflow() %&gt;%\n  add_model(lasso_spec_fixed) %&gt;%\n  add_recipe(clin_recipe)\n\n# Fit the model directly to your training data\nclin_lasso_fit &lt;- clin_workflow %&gt;%\n  fit(data = clin_train)\n\ntidy(clin_lasso_fit)\n\n# A tibble: 14 √ó 3\n   term                                      estimate penalty\n   &lt;chr&gt;                                        &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)                                 0.426     0.01\n 2 paper_age_at_initial_pathologic_diagnosis   0.182     0.01\n 3 paper_pathologic_stage_Stage_I              0         0.01\n 4 paper_pathologic_stage_Stage_II            -0.0796    0.01\n 5 paper_pathologic_stage_Stage_III            0         0.01\n 6 paper_pathologic_stage_Stage_IV             0.948     0.01\n 7 pr_status_by_ihc_Indeterminate              1.12      0.01\n 8 pr_status_by_ihc_Negative                  -0.960     0.01\n 9 pr_status_by_ihc_Positive                   2.97      0.01\n10 her2_status_by_ihc_X.Not.Evaluated.         0         0.01\n11 her2_status_by_ihc_Equivocal                0         0.01\n12 her2_status_by_ihc_Indeterminate            0         0.01\n13 her2_status_by_ihc_Negative                 0         0.01\n14 her2_status_by_ihc_Positive                 0         0.01\n\n\n\nclass_metrics &lt;- metric_set(accuracy, specificity, sensitivity)\n\nclin_lasso_fit |&gt;\n  predict(new_data = clin_test) |&gt;\n  bind_cols(clin_test) |&gt;\n  class_metrics(truth = ER, estimate = .pred_class)\n\n# A tibble: 3 √ó 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.835\n2 specificity binary         0.822\n3 sensitivity binary         0.878\n\n\n\nrandom_forest_spec &lt;- rand_forest(\n    mode = \"classification\", \n    trees = 1000 # Using 1000 trees for robustness\n  ) %&gt;%\n  set_engine(\"ranger\", importance = \"permutation\")\n\n\nset.seed(42)\n\nrandom_forest_workflow &lt;- workflow() %&gt;%\n  add_model(random_forest_spec) %&gt;%\n  add_recipe(clin_recipe)\n\nrandom_forest_fit &lt;- random_forest_workflow %&gt;%\n  fit(data = clin_train)\n\nrandom_forest_fit\n\n‚ïê‚ïê Workflow [trained] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: rand_forest()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n3 Recipe Steps\n\n‚Ä¢ step_normalize()\n‚Ä¢ step_unknown()\n‚Ä¢ step_dummy()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1000,      importance = ~\"permutation\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  1000 \nSample size:                      836 \nNumber of independent variables:  13 \nMtry:                             3 \nTarget node size:                 10 \nVariable importance mode:         permutation \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.09213 \n\nrandom_forest_fit |&gt;\n  predict(new_data = clin_test) |&gt;\n  bind_cols(clin_test) |&gt;\n  class_metrics(truth = ER, estimate = .pred_class)\n\n# A tibble: 3 √ó 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.840\n2 specificity binary         0.847\n3 sensitivity binary         0.816\n\nimportance_df &lt;- pull_workflow_fit(random_forest_fit)$fit$variable.importance %&gt;%\n  enframe(name = \"term\", value = \"Importance\") %&gt;%\n  arrange(desc(Importance))\nimportance_df\n\n# A tibble: 13 √ó 2\n   term                                      Importance\n   &lt;chr&gt;                                          &lt;dbl&gt;\n 1 pr_status_by_ihc_Positive                  0.0879   \n 2 pr_status_by_ihc_Negative                  0.0794   \n 3 paper_age_at_initial_pathologic_diagnosis  0.00451  \n 4 paper_pathologic_stage_Stage_I             0.00159  \n 5 paper_pathologic_stage_Stage_III           0.00107  \n 6 paper_pathologic_stage_Stage_IV            0.00104  \n 7 her2_status_by_ihc_Equivocal               0.000747 \n 8 her2_status_by_ihc_Negative                0.000736 \n 9 paper_pathologic_stage_Stage_II            0.000545 \n10 her2_status_by_ihc_Positive                0.000303 \n11 pr_status_by_ihc_Indeterminate             0.000254 \n12 her2_status_by_ihc_Indeterminate           0.0000554\n13 her2_status_by_ihc_X.Not.Evaluated.       -0.0000725"
  }
]