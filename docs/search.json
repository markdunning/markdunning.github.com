[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dr. Mark Dunning",
    "section": "",
    "text": "I’m a Bioinformatician with a background in Mathematics and Computer Science with over 20 years of experience. I specialise in the visualisation and interpretation of complex data and have a wealth of experience in making results accessible and easy to interrogate for wet-lab biologists. I also have experience in developing and delivering training courses and workships, along with formal teaching.\nI obtained my PhD in the Statistics and Computational Biology group of Simon Tavaré at the University of Cambridge / Cancer Research Uk. During this time, I developed the beadarray Bioconductor package for the analysis of Illumina microarray data. My PhD thesis is available online, should you be interested.\nAfter my PhD, I worked as a Bioinformatics Analyst within the Bioinformatics Core; consulting on, and assisting in, the analysis on all types of high-throughput datasets. During this time I worked on studies to define subtypes of Breast and Prostate cancer and established pipelines for analysis whole-genome and exome resequencing data.\nI held the role of “Bioinformatics Training Coordinator” in the Bioinformatics Core of Cancer Research Uk Cambridge Institute. I organised, developed and facilitated Bioinformatics training courses to teach computational and analytical skills to wet-lab biologists - along with a series of “Bioinformatics Summer Schools” for Cancer Research funded staff nationwide.\nFrom October 2017 I established the Bioinformatics Core at The University of Sheffield which ran until August 2025. Along with supporting clinicians and researchers across campus, I also contributed to various MSC and undergraduate programs as well as obtaining training / teaching qualifications from thecarpentries and AdvanceHE (FHEA)"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "training/r/index.html",
    "href": "training/r/index.html",
    "title": "Introduction to R",
    "section": "",
    "text": "Disclaimer\n\nAlthough R is well-regarded as a tool for performing statistical analysis, this workshop will not explicitly teach stats. Instead we give introduce the tools that we allow you to manipulate and interrogate your data into a form with which you can execute statistical tests.\n\n\n\nSetup\nIf you are following these notes independently (outside one of our workshops)\nFrom the RStudio menus, Choose the File -&gt; New Project option and select New Directory from the new window\n\nThen for the Project Type pick New Project.\n\nIt will ask you to pick a new Directory name, and where to create that directory (e.g. your Home directory or directory where you usually save your work)\n\nRStudio should now refresh itself. You can now download the data required for the workshop by copying and pasting the following into the R console (as shown in the screenshot)\n\ndownload.file(\"https://github.com/sheffield-bioinformatics-core/r-online/raw/master/CourseData.zip\", destfile = \"CourseData.zip\")\n\n The files from the zip file can be extracted using the command:-\n\nunzip(\"CourseData.zip\")\n\nYour RStudio screen should look like:-\n\nYou will need to install some R packages and download some data before you start. You can install the packages by copying and pasting the following into an R console and pressing ENTER\n\ninstall.packages(\"dplyr\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"readr\")\ninstall.packages(\"rmarkdown\")\ninstall.packages(\"tidyr\")\n\nYou can check that this worked by copying and pasting the following:-\n\nsource(\"https://raw.githubusercontent.com/sheffield-bioinformatics-core/r-online/master/check_packages.R\")\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nThe dplyr package has been installed\n\n\nThe ggplot2 package has been installed\n\n\nThe readr package has been installed\n\n\nThe rmarkdown package has been installed\n\n\nThe tidyr package has been installed\n\n\nYou have successfully installed all the packages required for the course\n\n\nIf you want to follow along with the R code on this webpage, you can open the file part1.Rmd from the bottom-right corner of RStudio\n\nThere are equivalent markdown files (part2.Rmd, part3.Rmd) for the other sections of the course. Enjoy!"
  },
  {
    "objectID": "training.html",
    "href": "training.html",
    "title": "Training Materials",
    "section": "",
    "text": "Here is a collection of Bioinformatics and Data Analysis materials that I have created and taught over the years. Please feel free to browse, and get in touch if you would like them to be taught at your institute.\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to R - Part 1\n\n\n\n\n\n\nMark Dunning\n\n\nOct 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to R - Part 2\n\n\n\n\n\n\nMark Dunning\n\n\nOct 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to R - Part 3\n\n\n\n\n\n\nMark Dunning\n\n\nOct 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to RNA-Seq - Part 1\n\n\n\n\n\n\nMark Dunning\n\n\nOct 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to RNA-Seq - Part 2\n\n\n\n\n\n\nMark Dunning\n\n\nOct 23, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "training/r_part2/index.html",
    "href": "training/r_part2/index.html",
    "title": "Introduction to R - Part 2",
    "section": "",
    "text": "Starting to play with data with the dplyr and ggplot2 packages\n\n\n\nChoosing which columns to show from the data\nChoosing what rows to keep in the data\nAdding / altering columns\nSorting the rows in our data\nIntroduction to plotting"
  },
  {
    "objectID": "training/bulk-rnaseq_1/index.html",
    "href": "training/bulk-rnaseq_1/index.html",
    "title": "Introduction to RNA-Seq - Part 1",
    "section": "",
    "text": "Importing “raw” RNA-seq counts into R and performing quality assessment\n\n\nHigh-throughput sequencing is now established as a standard technique for many functional genomics studies; allowing the researcher to compare and contrast the transcriptomes of many individuals to obtain biological insight. A high-volume of data are generated from these experimental techniques and thus require robust and reproducible tools to be employed in the analysis.\nIn this workshop, you will be learning how to analyse RNA-seq count data, using R. This will include reading the data into R, quality control and performing differential expression analysis and gene set testing, with a focus on the well-respected DESEq2 analysis workflow. You will learn how to generate common plots for analysis and visualisation of gene expression data, such as boxplots and heatmaps.\n\n\n\n\n\n\nNote\n\n\n\nWe will be discussing Bulk RNA-seq only, although some of the methods and techniques will be applicable to single-cell RNA-seq. I am planning some materials on single-cell RNA-seq in the future. In the meantime, the homepage for Seurat (a popular R package for single-cell analysis) has lots of useful tutorials.\n\nSeurat homepage\n\n\n\n\n\n\nI will go through the setup in quite a bit of detail. If you are already fairly confident with R, you can probably skim this and proceed to the start of the pre-processing section.\n\nCreate a new RStudio project that you want to work in\nCreate folders called meta_data and raw_counts\nDownload the meta data and raw counts\n\nmeta data\nraw counts\n\nPlace the sampleInfo.csv and raw_counts_matrix.tsv files into meta_data and raw_counts folders respectively\nInstall some R packages using this command\n\n\nsource(\"https://raw.githubusercontent.com/markdunning/markdunning.github.com/refs/heads/master/files/training/bulk_rnaseq/install_bioc_packages.R\")\n\n\n\n\nYou will need to install some R packages before you start, which I usually do this at RStudio’s console. See the below screenshot.\n\nRather than typing the location of the install script I suggest copying from here:-\n\nsource(\"https://raw.githubusercontent.com/markdunning/markdunning.github.com/refs/heads/master/files/training/bulk_rnaseq/install_bioc_packages.R\")\n\nIt may take a few minutes, but you will only have to do this once for a specific version of R. To check that everything worked, now copy and paste the following command. It should print messages to the screen to say that all the packages were installed\n\nsource(\"https://raw.githubusercontent.com/markdunning/markdunning.github.com/refs/heads/master/files/training/bulk_rnaseq/check_packages.R\")\n\nI also recommend creating a new project to work through the tutorial. You can do this via the file menu in Rstudio and it will ask you to choose a directory on your hard drive that you want the project to be located in. Briefly, using “projects” is a convenient way of keeping all the input data, R code, and outputs for a particular analysis together in the same place.\nFile -&gt; New Project -&gt; New Directory\n\nIn the screenshot, I am using a directory (which doesn’t exist at this point) called bulkrnaseq_tutorial in c:\\work\\personal_development. RStudio should now refresh and your working directory will be the location that you specified. Now we will use some code to download the example data. Again, I suggest you copy from below rather than typing manually.\n\n## Create two folders for the meta data and counts\ndir.create(\"meta_data\", showWarnings = FALSE)\ndir.create(\"raw_counts\",showWarnings = FALSE)\n\n\n## Download the raw data files\ndownload.file(\"https://raw.githubusercontent.com/markdunning/markdunning.github.com/refs/heads/master/files/training/bulk_rnaseq/meta_data/sampleInfo.csv\", destfile = \"meta_data/sampleInfo.csv\")\ndownload.file(\"https://raw.githubusercontent.com/markdunning/markdunning.github.com/refs/heads/master/files/training/bulk_rnaseq/raw_counts/raw_counts_matrix.tsv\", destfile = \"raw_counts/raw_counts_matrix.tsv\")\n\nHopefully your screen should look a bit like this:-\n\nFinally, create a new “Quarto Document” from the File menu\nFile -&gt; New File -&gt; Quarto Document\n\nClicking the “Create Empty Document” button on the pop-up that appears will create a blank quarto document that you can use to type the code from this tutorial, and any comments you may wish to make along the way. The Title and Author boxes can be filled with anything you like. They are used when you want to create a report document from your code. A new panel should appear in RStudio which is your bare bones analysis. The title and author lines should correspond to the text you entered (if any) when creating the document\nR code can be added to this document by clicking the “Insert Code Chunk” toolbar option\n\nThis will give you space to type or paste R code from the tutorial. In the below screenshot, R code can be entered between lines 8 and 10. Pressing ENTER between these lines will allow more code to be written. Pressing CTRL + ENTER causes the code to be run.\n\nLines outside of a code chunk can be used to write any explanations or interpretations of the plots, stats that you produce along the way.\nAt this point you are working on an untitled document that is not saved to disk. Go through the menu File -&gt; Save and choose a file name\n\n\n\nIn this short section, we will briefly describe the pre-processing of RNA-seq data which is not typically done in R. If you are only interested in using R, you may skip to the next section.\nThere are many steps involved in analysing an RNA-Seq experiment.\n\n(Workflow image from Harvard Bioinformatics Core)\nAnalysing an RNAseq experiment begins with sequencing reads. These are large (typically several Gb) that contain information on the sequences that have been generated for each biological sample; one fastq (or pair of fastqs) for each sample. Each set of four lines describe one sequence (called a “read”).\nA typical RNA-seq experiment will have 10 - 30 million reads in a fastq file, with each read about 100 bases long. e.g.\n@D0UW5ACXX120511:8:1204:6261:40047/1\nAATGTTTATGTTCTTAAATTTTAGTTGTATATGTGAATCTTTGTAGTTTTTGCTAAAATACTAAGTAATTTATATAAAAGTGAGTTAAGAGATTTTTCTGA\n+\nCCCFFFFFHHHHHJJJJJIJJJJJIJJHIIJIJIJJIJJJIJJHIIHIJJJJJJBEGIHIJICGIDICFGIJJJIIJJGJ&gt;F&gt;GAGCGEEHEHHEEFFFD&gt;\nAs the fastq files are large, we tend to analyse them using command-line software and a computing cluster. The traditional workflow for RNA-seq compares the sequences to a reference genome to see which genomic region each read matches the best.\n\nAgain, this requires more memory than a typical laptop or desktop machine so is performed on a remote computer with large memory. The resulting file is called a bam and records the best genomic match for each read. However, as we are interested in gene expression we want to relate these mappings to the positions of genes.\nA variety of different counting methods can determine how many reads overlap each known gene region. These are know as the raw counts and are the kind of data we will start with today.\n\nRecent tools for RNA-seq analysis (e.g. salmon, kallisto) do not require the time-consuming step of whole-genome alignment to be performed, and can therefore produce gene-level counts in a much faster time frame. They not require the creation of large bam files, which is useful if constrained by file space (e.g. if using Galaxy).\nMy strong recommendation would be to use the nextflow workflow system in conjunction with the nf.core pipeline to align and quantify your RNA-seq reads. My former colleague Dr. Lewis Quayle has a really good write-up of using nf.core on his pages\n\nhttps://www.lewisdoesdata.com/2024/05/01/bulk-rnaseq-end-to-end-part-1.html\n\n\n\n\n\n\n\nImportant\n\n\n\nUnless you are doing something extremely novel and bespoke I don’t believe it would be worth writing your own pipeline for processing RNA-seq, or any other sequencing data, from scratch rather than using what is available in nf.core.\n\n\n\n\n\nThe data for this tutorial comes from the paper, Induction of fibroblast senescence generates a non-fibrogenic myofibroblast phenotype that differentially impacts on cancer prognosis..\n\nCancer associated fibroblasts characterized by an myofibroblastic phenotype play a major role in the tumour microenvironment, being associated with poor prognosis. We found that this cell population is made in part by senescent fibroblasts in vivo. As senescent fibroblasts and myofibroblasts have been shown to share similar tumour promoting functions in vitro we compared the transcriptosomes of these two fibroblast types and performed RNA-seq of human foetal foreskin fibroblasts 2 (HFFF2) treated with 2ng/ml TGF-beta-1 to induce myofibroblast differentiation or 10Gy gamma irradiation to induce senescence. We isolated RNA 7 days upon this treatments changing the medium 3 days before the RNA extraction.\n\n\n\n\n\n\n\nNote\n\n\n\nA really useful resource for obtaining raw sequencing reads is sraexplorer. Given a GEO, ArrayExpress of SRA dataset name it will give you links to download the raw fastq files\n\nSRA Explorer\n\nAgain, Lewis’ page has some commands for downloading these data in fastq form.\n\nDownloading the example data"
  },
  {
    "objectID": "training/r_part1/index.html",
    "href": "training/r_part1/index.html",
    "title": "Introduction to R - Part 1",
    "section": "",
    "text": "In these materials, we explore fundamental operations of R and load some example data\n\n\n\n\nBasic calculations in R\nUsing functions\nGetting help\nSaving data using variables\nReading a spreadsheet into R"
  },
  {
    "objectID": "training/r_part3/index.html",
    "href": "training/r_part3/index.html",
    "title": "Introduction to R - Part 3",
    "section": "",
    "text": "Further data exploration and manipulation with ggplot2 and dplyr\n\n\n\nCustomising ggplot2 plots\nSummarising data\nGroup-based summaries\nJoining data\nData Cleaning\n\nLets make sure we have read the gapminder data into R and have the relevant packages loaded.\n\n## Checks if the required file is present, and downloads if not\n\nif(!file.exists(\"raw_data/gapminder.csv\")) {\n  dir.create(\"raw_data/\",showWarnings = FALSE)\ndownload.file(\"https://raw.githubusercontent.com/markdunning/markdunning.github.com/refs/heads/master/files/training/r/raw_data/gapminder.csv\", destfile = \"raw_data/gapminder.csv\")\n}\n\nWe also discussed in the previous part(s) how to read the example dataset into R. We will also load the libraries needed.\n\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\ngapminder &lt;- read_csv(\"raw_data/gapminder.csv\")\n\n\n\n\nNow make a scatter plot of gdp versus life expectancy as we did in the previous session. One of the last topics we covered was how to add colour to a plot. This can make the plot more appealing, but also help with data interpretation. In this case, we can use different colours to indicate countries belonging to different continents. For example, we can see a cluster of Asia data points with unusually large GDP. At some point we might want to adjust the scale on the x-axis to make the trend between the two axes easier to visualise.\n\nggplot(gapminder, aes(x = gdpPercap, y=lifeExp,col=continent)) + geom_point()\n\n\n\n\n\n\n\n\nThe shape and size of points can also be mapped from the data. However, it is easy to get carried away!\n\nggplot(gapminder, aes(x = gdpPercap, y=lifeExp,shape=continent,size=pop)) + geom_point()\n\n\n\n\n\n\n\n\nScales and their legends have so far been handled using ggplot2 defaults. ggplot2 offers functionality to have finer control over scales and legends using the scale methods.\nScale methods are divided into functions by combinations of\n\nthe aesthetics they control.\nthe type of data mapped to scale.\n\nscale_aesthetic_type\nTry typing in scale_ then tab to autocomplete. This will provide some examples of the scale functions available in ggplot2.\nAlthough different scale functions accept some variety in their arguments, common arguments to scale functions include -\n\nname - The axis or legend title\nlimits - Minimum and maximum of the scale\nbreaks - Label/tick positions along an axis\nlabels - Label names at each break\nvalues - the set of aesthetic values to map data values\n\nWe can choose specific colour palettes, such as those provided by the RColorBrewer package. This package is included with R (so you don’t need to install it) and provides palettes for different types of scale (sequential, diverging, qualitative).\n\nlibrary(RColorBrewer)\ndisplay.brewer.all(colorblindFriendly = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen creating a plot, always check that the colour scheme is appropriate for people with various forms of colour-blindness\n\n\nWhen experimenting with colour palettes and labels, it is useful to save the plot as an object. This saves quite a bit of typing! Notice how nothing get shown on the screen.\n\np &lt;- ggplot(gapminder, aes(x = gdpPercap, y=lifeExp,col=continent)) + geom_point()\n\nRunning the line of code with just p now shows the plot on the screen\n\np \n\n\n\n\n\n\n\n\nBut we can also make modifications to the plot with the + symbol. Here, we change the colours to those defined as Set2 in RColorBrewer.\n\n## Here we pick 6 colours from the palette\np + scale_color_manual(values=brewer.pal(6,\"Set2\"))\n\n\n\n\n\n\n\n\nVarious labels can be modified using the labs function.\n\np + labs(x=\"Wealth\",y=\"Life Expectancy\",title=\"Relationship between Wealth and Life Expectancy\")\n\n\n\n\n\n\n\n\nWe can also modify the x- and y- limits of the plot so that any outliers are not shown. ggplot2 will give a warning that some points are excluded.\n\np + xlim(0,60000)\n\nWarning: Removed 5 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nSaving is supported by the ggsave function and automatically saves the last plot that was displayed in RStudio. A variety of file formats are supported (.png, .pdf, .tiff, etc) and the format used is determined from the extension given in the file argument. The height, width and resolution can also be configured. See the help on ggsave (?ggsave) for more information.\n\nggsave(file=\"my_ggplot.png\")\n\nSaving 7 x 5 in image\n\n\nWarning: Removed 5 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nMost aspects of the plot can be modified from the background colour to the grid sizes and font. Several pre-defined “themes” exist and we can modify the appearance of the whole plot using a theme_.. function.\n\np + theme_bw()\n\n\n\n\n\n\n\n\nMore themes are supported by the ggthemes package. You can make your plots look like the Economist, Wall Street Journal or Excel (but please don’t do this!)\n\n## this will check if ggthemes is already installed, and will only install if it is not found\n\nif(!require(\"ggthemes\")) install.packages(\"ggthemes\")\n\nLoading required package: ggthemes\n\nlibrary(ggthemes)\np + theme_excel()\n\n\n\n\n\n\n\n\n\n\n\n\nUse a boxplot to compare the life expectancy values of Australia and New Zealand. Use a Set2 palette from RColorBrewer to colour the boxplots and apply a “minimal” theme to the plot.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ngapminder %&gt;% \n  filter(continent == \"Oceania\") %&gt;% \n  ggplot(aes(x = country, y = lifeExp,fill=country)) + geom_boxplot() + scale_fill_manual(values=brewer.pal(2,\"Set2\")) + theme_bw()\n\n\n\n\nAnother transformation that is useful in this case is to display the x-axis on a log\\(_10\\) scale. This compresses the values on the x-axis (reducing the impact of the high outliers) and makes trends easier to spot\n\np + scale_x_log10()\n\n\n\n\n\n\n\n\nIt now seems that lifeExp is increasing in a roughly linear fashion with the GDP (on a log\\(_10\\) scale).\n\n\n\n\n\n\nAbout the log transformation\n\n\n\n\n\nThe logarithm of 10 (log10) is the exponent to which the base 10 must be “raised” to obtain the number 10. For example, log10(10) = 1, as 10 raised to the power of 1 equals 10.\n\nlog10(10)\n\n[1] 1\n\n10^1\n\n[1] 10\n\nlog10(100)\n\n[1] 2\n\n10^2\n\n[1] 100\n\n\nThis transformation helps in simplifying visualisation involving large numbers. The range of our gdpPercap values is extremely large. summary is a quick way to get various summary statistics from our data\n\n## we will use the $ notation for now\n\nsummary(gapminder$gdpPercap)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n   241.2   1202.1   3531.8   7215.3   9325.5 113523.1 \n\n\nAfter a log10 transformation the data are much more compressed.\n\nsummary(log10(gapminder$gdpPercap))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.382   3.080   3.548   3.543   3.970   5.055 \n\n\nThe largest value after the log\\(_10\\) transformation is around 5\n\n10^5.055\n\n[1] 113501.1\n\n\n\n\n\n\n\n\nOne very useful feature of ggplot2 is faceting. This allows you to produce plots for subsets and groupings in your data (aka “facets”). In the scatter plot above, it was quite difficult to determine if the relationship between gdp and life expectancy was the same for each continent. To overcome this, we would like a see a separate plot for each continent.\nIn we attempted such a task manually we might start off by plotting Africa\n\nafr_plot&lt;- gapminder %&gt;% \n  filter(continent == \"Africa\") %&gt;% \n  ggplot(aes(x = gdpPercap, y = lifeExp)) + geom_point() + scale_x_log10()\nafr_plot\n\n\n\n\n\n\n\n\nAnd then the same for Americas:-\n\namr_plot &lt;- gapminder %&gt;% \n  filter(continent == \"Americas\") %&gt;% \n  ggplot(aes(x = gdpPercap, y = lifeExp)) + geom_point() + scale_x_log10()\namr_plot\n\n\n\n\n\n\n\n\nAt some point we will have to stitch the plots together (which is possible, but we will cover this later) and make sure we have equivalent scales for all plots. In this setup we are manually specifying the name of the continent, which is prone to error. Again, we could use something like a for loop to make the plots for each continent. However, we aren’t covering such techniques in these materials as dplyr and ggplot2 don’t tend to require them.\nAs we said before, dplyr, and ggplot2 are built with the analyst in mind and have many useful features for automating some common tasks. To achieve the plot we want is surprisingly simple. To “facet” our data into multiple plots we can use the facet_wrap (1 variable) or facet_grid (2 variables) functions and specify the variable(s) we split by.\n\np + facet_wrap(~continent) + scale_x_log10() + xlab(\"GDP (log10)\") + ylab(\"Life Expectancy\")\n\n\n\n\n\n\n\n\nThe facet_grid function will create a grid-like plot with one variable on the x-axis and another on the y-axis.\n\np + facet_grid(continent~year)\n\n\n\n\n\n\n\n\nThe previous plot was a bit messy as it contained all combinations of year and continent. Let’s suppose we want our analysis to be a bit more focused and disregard countries in Oceania (as there are only 2 in our dataset) and maybe years between 1997 and 2002. However, we can only “add” more information from our plots and not take anything away. Therefore the suggested approach is to pre-filter and manipulate the data into the form you want for plotting.\nWeknow how to restrict the rows from the gapminder dataset using the filter function. Instead of filtering the data, creating a new data frame, and constructing the data frame from these new data we can use the%&gt;% operator to create the data frame “on the fly” and pass directly to ggplot. Thus we don’t have to save a new data frame or alter the original data.\n\nfilter(gapminder, continent!=\"Oceania\", year %in% c(1997,2002,2007)) %&gt;% \n  ggplot(aes(x = gdpPercap, y=lifeExp,col=continent)) + geom_point() + facet_grid(continent~year)\n\n\n\n\n\n\n\n\nThere is lots more to cover on ggplot2 and quickly we can start to understand our data without too much in the way of coding. When it comes to reporting and justifying our findings we will need to produce some numerical summaries. We tackle this in the next section."
  },
  {
    "objectID": "training/r_part1/index.html#topics-covered",
    "href": "training/r_part1/index.html#topics-covered",
    "title": "Introduction to R - Part 1",
    "section": "",
    "text": "Basic calculations in R\nUsing functions\nGetting help\nSaving data using variables\nReading a spreadsheet into R"
  },
  {
    "objectID": "training/r_part1/index.html#variables",
    "href": "training/r_part1/index.html#variables",
    "title": "Introduction to R - Part 1",
    "section": "Variables",
    "text": "Variables\nA variable is a letter or word which takes (or contains) a value. We use the assignment ‘operator’, &lt;- to create a variable and store some value in it.\n\nx &lt;- 10\nx\n\n[1] 10\n\nmyNumber &lt;- 25\nmyNumber\n\n[1] 25\n\n\nWe also can perform arithmetic on variables using functions:\n\nsqrt(myNumber)\n\n[1] 5\n\n\nWe can add variables together:\n\nx + myNumber\n\n[1] 35\n\n\nWe can change the value of an existing variable:\n\nx &lt;- 21\nx\n\n[1] 21\n\n\nWe can set one variable to equal the value of another variable:\n\nx &lt;- myNumber\nx\n\n[1] 25\n\n\nWhen we are feeling lazy we might give our variables short names (x, y, i…etc), but a better practice would be to give them meaningful names. There are some restrictions on creating variable names. They cannot start with a number or contain characters such as . and ‘-’. Naming variables the same as in-built functions in R, such as c, T, mean should also be avoided.\nNaming variables is a matter of taste. Some conventions exist such as a separating words with - or using camelCaps. Whatever convention you decided, stick with it!"
  },
  {
    "objectID": "training/r_part1/index.html#functions",
    "href": "training/r_part1/index.html#functions",
    "title": "Introduction to R - Part 1",
    "section": "Functions",
    "text": "Functions\nFunctions in R perform operations on arguments (the inputs(s) to the function). We have already used:\n\nsin(x)\n\n[1] -0.1323518\n\n\nthis returns the sine of x. In this case the function has one argument: x. Arguments are always contained in parentheses – curved brackets, () – separated by commas.\nArguments can be named or unnamed, but if they are unnamed they must be ordered (we will see later how to find the right order). The names of the arguments are determined by the author of the function and can be found in the help page for the function. When testing code, it is easier and safer to name the arguments. seq is a function for generating a numeric sequence from and to particular numbers. Type ?seq to get the help page for this function.\n\nseq(from = 3, to = 20, by = 4)\n\n[1]  3  7 11 15 19\n\nseq(3, 20, 4)\n\n[1]  3  7 11 15 19\n\n\nArguments can have default values, meaning we do not need to specify values for these in order to run the function.\nrnorm is a function that will generate a series of values from a normal distribution. In order to use the function, we need to tell R how many values we want\n\n## this will produce a random set of numbers, so everyone will get a different set of numbers\nrnorm(n=10)\n\n [1]  0.5510309 -0.6719926  1.2815910  1.0240086  0.6964364  2.1482310\n [7]  0.1152513  1.9619542  0.6834065 -0.2606619\n\n\nThe normal distribution is defined by a mean (average) and standard deviation (spread). However, in the above example we didn’t tell R what mean and standard deviation we wanted. So how does R know what to do? All arguments to a function and their default values are listed in the help page\n(N.B sometimes help pages can describe more than one function)\n\n?rnorm\n\nIn this case, we see that the defaults for mean and standard deviation are 0 and 1. We can change the function to generate values from a distribution with a different mean and standard deviation using the mean and sd arguments. It is important that we get the spelling of these arguments exactly right, otherwise R will an error message, or (worse?) do something unexpected.\n\nrnorm(n=10, mean=2,sd=3)\n\n [1]  2.745833  1.847071  3.122237 -1.403847  4.265786  7.831045  2.130496\n [8]  4.907590 -1.796182  3.140468\n\nrnorm(10, 2, 3)\n\n [1]  3.32607058  1.64872910 10.03706783  0.03362361  3.10942727  5.96641609\n [7]  2.88051763  0.14591071  2.50020153  2.54308355\n\n\nIn the examples above, seq and rnorm were both outputting a series of numbers, which is called a vector in R and is the most-fundamental data-type.\nJust as we can save single numbers as a variable, we can also save a vector. In fact a single number is still a vector.\n\nmy_seq &lt;- seq(from = 3, to = 20, by = 4)\n\nThe arithmetic operations we have seen can be applied to these vectors; exactly the same as a single number.\n\nmy_seq + 2\n\n[1]  5  9 13 17 21\n\n\n\nmy_seq * 2\n\n[1]  6 14 22 30 38\n\n\nThese so-called “vectorised operations” are a really nice feature of R and will come in useful when dealing with more complex data.\n\n\n\n\nExercise\n\n\nWhat is the value of pi to 3 decimal places?\n\nsee the help for round ?round\n\nHow can we a create a sequence from 2 to 20 comprised of 5 equally-spaced numbers?\n\ni.e. not specifying the by argument and getting R to work-out the intervals\ncheck the help page for seq ?seq\n\nCreate a variable containing 1000 random numbers with a mean of 2 and a standard deviation of 3\n\nwhat is the maximum and minimum of these numbers?\nwhat is the average?\nHINT: see the help pages for functions min, max and mean\n\n\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\n## The digits argument needs to be changed\nround(pi,digits = 3)\n\n[1] 3.142\n\n## Use the length.out argument\nseq(from = 2, to = 20, length.out = 5)\n\n[1]  2.0  6.5 11.0 15.5 20.0\n\n## Make sure you create a variable\n\nmy_numbers &lt;- rnorm(n = 1000, mean = 2, sd = 3)\n\nmax(my_numbers)\n\n[1] 12.06418\n\nmin(my_numbers)\n\n[1] -7.217665\n\nmean(my_numbers)\n\n[1] 2.10007\n\n\n\n\n\nSo far we have only used functions that come with every version of R. To do something more specialised we will need to install some add-on packages.\n\n\n\n\n\n\nAbout random numbers…\n\n\n\nSometimes we just want to create some numbers or data that we can play around with. However, most likely we will be concerned about the reproducibility of our R code. In circumstances where randomness is involved it is common to set a “seed” which ensures the same random numbers are generated each time.\n\nset.seed(123)\nrnorm(10)\n\n [1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499\n [7]  0.46091621 -1.26506123 -0.68685285 -0.44566197"
  },
  {
    "objectID": "training/r_part1/index.html#saving-your-notebook",
    "href": "training/r_part1/index.html#saving-your-notebook",
    "title": "Introduction to R - Part 1",
    "section": "Saving your notebook",
    "text": "Saving your notebook\nIf you want to re-visit your code at any point, you will need to save a copy.\n\nFile &gt; Save &gt;"
  },
  {
    "objectID": "training/r_part1/index.html#packages-in-r",
    "href": "training/r_part1/index.html#packages-in-r",
    "title": "Introduction to R - Part 1",
    "section": "Packages in R",
    "text": "Packages in R\nSo far we have used functions that are available with the base distribution of R; the functions you get with a clean install of R. The open-source nature of R encourages others to write their own functions for their particular data-type or analyses.\nPackages are distributed through repositories. The most-common ones are CRAN and Bioconductor. CRAN alone has many thousands of packages.\n\n\nThe meta cran website can be used to browse packages available in CRAN\nBioconductor packages can be browsed here\n\n\nCRAN and Bioconductor have some level of curation so should be the first place to look. Researchers sometimes make their packages available on github. However, there is no straightforward way of searching github for a particular package and no guarentee of quality.\nThe Packages tab in the bottom-right panel of RStudio lists all packages that you currently have installed. Clicking on a package name will show a list of functions that available once that package has been loaded.\nThere are functions for installing packages within R. If your package is part of the main CRAN repository, you can use install.packages.\nWe will be using a set of tidyverse R packages in this practical. To install them, we would do.\n\n## You should already have installed these as part of the course setup\n\ninstall.packages(\"readr\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"dplyr\")\n# to install the entire set of tidyverse packages, we can do install.packages(\"tidyverse\"). But this will take some time\n\nA package may have several dependencies; other R packages from which it uses functions or data types (re-using code from other packages is strongly-encouraged). If this is the case, the other R packages will be located and installed too.\n\n\n\n\n\n\nInstalling packages can sometimes take a long time. Fortunately we will only have to do it once **as long as we stick to the same version of R.\nNote that you can install newer versions of RStudio without having to re-install R and any packages.\n\n\n\nOnce a package is installed, the library function is used to load a package and make it’s functions / data available in your current R session. You need to do this every time you load a new RStudio session. Let’s go ahead and load the readr so we can import some data.\n\n## readr is a packages to import spreadsheets into R\nlibrary(readr)"
  },
  {
    "objectID": "training/r_part1/index.html#reading-in-data",
    "href": "training/r_part1/index.html#reading-in-data",
    "title": "Introduction to R - Part 1",
    "section": "Reading in data",
    "text": "Reading in data\nAny .csv file can be imported into R by supplying the path to the file to readr function read_csv and assigning it to a new object to store the result. A useful sanity check is the file.exists function which will print TRUE is the file can be found in the working directory.\n\n## This will print the current location of our working directory\ngetwd()\n\n[1] \"C:/work/personal_development/markdunning.github.com/training/r_part1\"\n\ngapminder_path &lt;- \"raw_data/gapminder.csv\"\nfile.exists(gapminder_path)\n\n[1] TRUE\n\n\n\n\n\n\n\n\nThe getwd(), and file.exists(...) functions have been used here as you may find them useful in your own work. If we are confident that we know where a file is located we can use read_csv as below.\n\n\n\nAssuming the file can be found, we can use read_csv to import. Other functions can be used to read tab-delimited files (read_delim) or a generic read.table function. A data frame object is created.\n\nlibrary(readr)\ngapminder_path &lt;- \"raw_data/gapminder.csv\"\ngapminder &lt;- read_csv(gapminder_path)\n\nRows: 1704 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): country, continent\ndbl (4): year, lifeExp, pop, gdpPercap\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\nFile paths\n\n\n\n🤔\nWhy would specifying gapminder_path as Users/mark/Documents/workflows/workshops/r-crash-course/raw_data/gapminder.csv be a bad idea? Would you be able to re-run the analysis on another machine?\n\n\n\n\n\n\n\n\nReading from Excel (xls/xlsx) files\n\n\n\nYou can also read excel (.xls or .xlsx) files into R, but you will have to use the readxl package instead.\n\ninstall.packages(\"readxl\")\nlibrary(readxl)\n## Replace PATH_TO_MY_XLS with the name of the file you want to read\ndata &lt;- read_xls(PATH_TO_MY_XLS)\n## Replace PATH_TO_MY_XLSX with the name of the file you want to read\ndata &lt;- read_xlsx(PATH_TO_MY_XLSX)\n\n\n\n\n\n\n\n\n\nIf you get really stuck importing data, there is a File -&gt; Import Dataset option that should guide you through the process. It will also show the corresponding R code that you can use in future.\n\n\n\nThe data frame object in R allows us to work with “tabular” data, like we might be used to dealing with in Excel, where our data can be thought of having rows and columns. The values in each column have to all be of the same type (i.e. all numbers or all text).\nIn Rstudio, you can view the contents of the data frame we have just created using function View(). This is useful for interactive exploration of the data, but not so useful for automation, scripting and analyses.\n\n## Make sure that you use a capital letter V\n\nView(gapminder)\n\n\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nWe should always check the data frame that we have created. Sometimes R will happily read data using an inappropriate function and create an object without raising an error. However, the data might be unusable. Consider:-\n\ntest &lt;- read_table(gapminder_path)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  `\"country\",\"continent\",\"year\",\"lifeExp\",\"pop\",\"gdpPercap\"` = col_character()\n)\n\n\nWarning: 324 parsing failures.\nrow col  expected    actual                     file\n145  -- 1 columns 3 columns 'raw_data/gapminder.csv'\n146  -- 1 columns 3 columns 'raw_data/gapminder.csv'\n147  -- 1 columns 3 columns 'raw_data/gapminder.csv'\n148  -- 1 columns 3 columns 'raw_data/gapminder.csv'\n149  -- 1 columns 3 columns 'raw_data/gapminder.csv'\n... ... ......... ......... ........................\nSee problems(...) for more details.\n\n\n\nView(test)\n\n\n\n# A tibble: 1,704 × 1\n   `\"country\",\"continent\",\"year\",\"lifeExp\",\"pop\",\"gdpPercap\"` \n   &lt;chr&gt;                                                      \n 1 \"\\\"Afghanistan\\\",\\\"Asia\\\",1952,28.801,8425333,779.4453145\" \n 2 \"\\\"Afghanistan\\\",\\\"Asia\\\",1957,30.332,9240934,820.8530296\" \n 3 \"\\\"Afghanistan\\\",\\\"Asia\\\",1962,31.997,10267083,853.10071\"  \n 4 \"\\\"Afghanistan\\\",\\\"Asia\\\",1967,34.02,11537966,836.1971382\" \n 5 \"\\\"Afghanistan\\\",\\\"Asia\\\",1972,36.088,13079460,739.9811058\"\n 6 \"\\\"Afghanistan\\\",\\\"Asia\\\",1977,38.438,14880372,786.11336\"  \n 7 \"\\\"Afghanistan\\\",\\\"Asia\\\",1982,39.854,12881816,978.0114388\"\n 8 \"\\\"Afghanistan\\\",\\\"Asia\\\",1987,40.822,13867957,852.3959448\"\n 9 \"\\\"Afghanistan\\\",\\\"Asia\\\",1992,41.674,16317921,649.3413952\"\n10 \"\\\"Afghanistan\\\",\\\"Asia\\\",1997,41.763,22227415,635.341351\" \n# ℹ 1,694 more rows\n\n\n😬\n\n\n\n\n\n\nThe problem here is that we incorrectly told R that our file was “tab-delimited” rather than “comma-separated”. Tab-delimited means that a “tab” (four spaces) is used to distinguish the columns in the file. Therefore R cannot tell where the columns are, and the resulting data frame has a single column. R will not automatically use the appropriate read_csv or read_delim etc function, so you need to be careful\n\n\n\nQuick sanity checks can also be performed by inspecting details in the environment tab. A useful check in RStudio is to use the head function, which prints the first 6 rows of the data frame to the screen.\n\nhead(gapminder)\n\n# A tibble: 6 × 6\n  country     continent  year lifeExp      pop gdpPercap\n  &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 Afghanistan Asia       1952    28.8  8425333      779.\n2 Afghanistan Asia       1957    30.3  9240934      821.\n3 Afghanistan Asia       1962    32.0 10267083      853.\n4 Afghanistan Asia       1967    34.0 11537966      836.\n5 Afghanistan Asia       1972    36.1 13079460      740.\n6 Afghanistan Asia       1977    38.4 14880372      786.\n\n\n\n\n\n\n\n\nWe have used a nice, clean, dataset as our example for the workshop. Other datasets out in the wild might not be so ameanable for analysis in R. If your data look like this, you might have problems:-\n\nWe recommend the Data Carpentry materials on spreadsheet organisation for an overview of common pitfalls - and how to address them\n\nhttps://datacarpentry.org/spreadsheet-ecology-lesson/\n\nAlthough R has many functions for data cleaning, if you are new to the language the best approach to such data would be to clean them before attempting to read into R."
  },
  {
    "objectID": "training/r_part1/index.html#accessing-data-in-columns",
    "href": "training/r_part1/index.html#accessing-data-in-columns",
    "title": "Introduction to R - Part 1",
    "section": "Accessing data in columns",
    "text": "Accessing data in columns\nIn the next section we will explore in more detail how to control the columns and rows from a data frame that are displayed in RStudio. For now, accessing all the observations from a particular column can be achieved by typing the $ symbol after the name of the data frame followed by the name of a column you are interested in.\nRStudio is able to “tab-complete” the column name, so typing the following and pressing the TAB key will bring-up a list of possible columns. The contents of the column that you select are then printed to the screen.\n\ngapminder$c\n\nRather than merely printing to the screen we can also create a variable\n\nyears &lt;- gapminder$year\n\nWe can then use some of the functions we have seen before\n\nmin(years)\n\n[1] 1952\n\nmax(years)\n\n[1] 2007\n\nmedian(years)\n\n[1] 1979.5\n\n\nAlthough we don’t have to save the values in the column as a variable first\n\nmin(gapminder$year)\n\n[1] 1952"
  },
  {
    "objectID": "training/r_part1/index.html#creating-a-new-r-notebook",
    "href": "training/r_part1/index.html#creating-a-new-r-notebook",
    "title": "Introduction to R - Part 1",
    "section": "Creating a new R notebook",
    "text": "Creating a new R notebook\nYou will probably want to create a new R notebook file to perform your analysis. This can be done by following the menus:-\n\nFile -&gt; New File -&gt; R notebook\n\nA new pane should open that includes some example code. You can delete everything apart from lines 1 to 4\n\nYou can now insert R code chunks using the insert menu.\nBefore generating a report you will need to save the file with the menu File -&gt; Save. You will then be able to create a report using the Preview button. N.B. you may need to install extra software before doing this.\n\n\n\n\nExercise before the next session\n\n\nCreate a new R notebook using the instructions above and create a code chunk to read the gapminder.csv file. Answer the following questions and generate a report\n\nThe function tail is similar to head except it prints the last lines in a file. Use this function to print the last 10 lines in the data frame (you will have to consult the help on tail to see how to change the default arguments.)\nWhat is the largest observed population?\nWhat is the lowest life expectancy"
  },
  {
    "objectID": "training/r_part2/index.html#topics-covered",
    "href": "training/r_part2/index.html#topics-covered",
    "title": "Introduction to R - Part 2",
    "section": "",
    "text": "Choosing which columns to show from the data\nChoosing what rows to keep in the data\nAdding / altering columns\nSorting the rows in our data\nIntroduction to plotting"
  },
  {
    "objectID": "training/r_part2/index.html#manipulating-columns",
    "href": "training/r_part2/index.html#manipulating-columns",
    "title": "Introduction to R - Part 2",
    "section": "Manipulating Columns",
    "text": "Manipulating Columns\nWe are going to use functions from the dplyr package to manipulate the data frame we have just created. It is perfectly possible to work with data frames using the functions provided as part of “base R”. However, many find it easy to read and write code using dplyr.\nThere are many more functions available in dplyr than we will cover today. An overview of all functions is given in a cheatsheet.\n\n\n\n\n\n\nHelp with dplyr functions\n\n\n\n\ndplyr cheatsheet. The “cheatsheet” is also available through the RStudio Help menu. However, I don’t think of this as cheating to have such information to hand. There are far too many functions to remember all of them!\n\n\n\nBefore using any of these functions, we need to load the library:-\n\nlibrary(dplyr)\n\n\nselecting columns\nWe can access the columns of a data frame using the select function. This lets us have control over what is printed to the screen. Admitedly the dataset we are using here is rather small (being only six columns), but these useful functions really shine when faced with 10s or 100s of columns\n\nby name\nFirstly, we can select column by name, by adding bare column names (i.e. not requiring quote marks around the name) after the name of the data frame, separated by a , .\n\nselect(gapminder, country, continent)\n\n# A tibble: 862 × 2\n   country     continent\n   &lt;chr&gt;       &lt;chr&gt;    \n 1 Afghanistan Asia     \n 2 Afghanistan Asia     \n 3 Afghanistan Asia     \n 4 Afghanistan Asia     \n 5 Afghanistan Asia     \n 6 Afghanistan Asia     \n 7 Afghanistan Asia     \n 8 Afghanistan Asia     \n 9 Afghanistan Asia     \n10 Afghanistan Asia     \n# ℹ 852 more rows\n\n\nNow lets imagine that we want to see all the columns apart from country. It would quickly become tedious, not to mention and prone to error, if we had to type every column name we wanted to keep by-hand.\nThankfully, we can also omit columns from the ouput by putting a minus (-) in front of the column name. Note that this is not the same as removing the column from the data permanently.\n\nselect(gapminder, -country)\n\n# A tibble: 862 × 5\n   continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Asia       1952    28.8  8425333      779.\n 2 Asia       1957    30.3  9240934      821.\n 3 Asia       1962    32.0 10267083      853.\n 4 Asia       1967    34.0 11537966      836.\n 5 Asia       1972    36.1 13079460      740.\n 6 Asia       1977    38.4 14880372      786.\n 7 Asia       1982    39.9 12881816      978.\n 8 Asia       1987    40.8 13867957      852.\n 9 Asia       1992    41.7 16317921      649.\n10 Asia       1997    41.8 22227415      635.\n# ℹ 852 more rows\n\n\n\n\n\n\n\n\nThe dplyr package has been carefully developed over the years with the needs of the data analyst in mind. Ideally we would rather be spending our time exploring and understanding data than writing reams of code. For this reason, you will often find a helpful function for a common task.\nIf you find yourself having to write lots of code to achieve a data manipulation task, the chances are the a convenient function already exists.\n\n\n\n\n\nrange of columns\nA range of columns can be selected by the : operator.\n\nselect(gapminder, lifeExp:gdpPercap)\n\n# A tibble: 862 × 3\n   lifeExp      pop gdpPercap\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1    28.8  8425333      779.\n 2    30.3  9240934      821.\n 3    32.0 10267083      853.\n 4    34.0 11537966      836.\n 5    36.1 13079460      740.\n 6    38.4 14880372      786.\n 7    39.9 12881816      978.\n 8    40.8 13867957      852.\n 9    41.7 16317921      649.\n10    41.8 22227415      635.\n# ℹ 852 more rows\n\n\n\n\nhelper functions\nThere are a number of helper functions can be employed if we are unsure about the exact name of the column.\n\nselect(gapminder, starts_with(\"co\"))\n\n# A tibble: 862 × 2\n   country     continent\n   &lt;chr&gt;       &lt;chr&gt;    \n 1 Afghanistan Asia     \n 2 Afghanistan Asia     \n 3 Afghanistan Asia     \n 4 Afghanistan Asia     \n 5 Afghanistan Asia     \n 6 Afghanistan Asia     \n 7 Afghanistan Asia     \n 8 Afghanistan Asia     \n 9 Afghanistan Asia     \n10 Afghanistan Asia     \n# ℹ 852 more rows\n\nselect(gapminder, contains(\"life\"))\n\n# A tibble: 862 × 1\n   lifeExp\n     &lt;dbl&gt;\n 1    28.8\n 2    30.3\n 3    32.0\n 4    34.0\n 5    36.1\n 6    38.4\n 7    39.9\n 8    40.8\n 9    41.7\n10    41.8\n# ℹ 852 more rows\n\n# selecting the last and penultimate columns\nselect(gapminder, last_col(1),last_col())\n\n# A tibble: 862 × 2\n        pop gdpPercap\n      &lt;dbl&gt;     &lt;dbl&gt;\n 1  8425333      779.\n 2  9240934      821.\n 3 10267083      853.\n 4 11537966      836.\n 5 13079460      740.\n 6 14880372      786.\n 7 12881816      978.\n 8 13867957      852.\n 9 16317921      649.\n10 22227415      635.\n# ℹ 852 more rows\n\n\nIt is also possible to use the column number in the selection.\n\nselect(gapminder, 4:6)\n\n# A tibble: 862 × 3\n   lifeExp      pop gdpPercap\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1    28.8  8425333      779.\n 2    30.3  9240934      821.\n 3    32.0 10267083      853.\n 4    34.0 11537966      836.\n 5    36.1 13079460      740.\n 6    38.4 14880372      786.\n 7    39.9 12881816      978.\n 8    40.8 13867957      852.\n 9    41.7 16317921      649.\n10    41.8 22227415      635.\n# ℹ 852 more rows\n\n\nThe select function can be used with just a single column name - in a similar manner to the $ operation we saw in Part 1. However, select always returns a data frame whereas $ gives a vector. Compare the output of the following code chunks\n\nselect(gapminder, pop)\n\n# A tibble: 862 × 1\n        pop\n      &lt;dbl&gt;\n 1  8425333\n 2  9240934\n 3 10267083\n 4 11537966\n 5 13079460\n 6 14880372\n 7 12881816\n 8 13867957\n 9 16317921\n10 22227415\n# ℹ 852 more rows\n\n\n\ngapminder$pop\n\nThe consequence of this is that you cannot use functions such as mean in combination with select\n\npops &lt;- select(gapminder, pop)\nmean(pops)\n\nIn the next session we will see how to calculate summary statistics on particular columns in our data. For now, a useful function is pull that will return the correct type of data required for a function such as mean.\n\npops &lt;- pull(gapminder,pop)\nmean(pops)\n\n[1] 40918865"
  },
  {
    "objectID": "training/r_part2/index.html#restricting-rows-with-filter",
    "href": "training/r_part2/index.html#restricting-rows-with-filter",
    "title": "Introduction to R - Part 2",
    "section": "Restricting rows with filter",
    "text": "Restricting rows with filter\nSo far we have been returning all the rows in the output. We can use what we call a logical test to filter the rows in a data frame. This logical test will be applied to each row and give either a TRUE or FALSE result. When filtering, only rows with a TRUE result get returned.\nFor example we filter for rows where the lifeExp variable is less than 40. You can think of R looking at each row of the data frame in turn and deciding whether the lifeExp value in that row is less than 40. If so, that row will be shown on the screen.\n\nfilter(gapminder, lifeExp &lt; 40)\n\n# A tibble: 65 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Angola      Africa     1952    30.0  4232095     3521.\n 9 Angola      Africa     1957    32.0  4561361     3828.\n10 Angola      Africa     1962    34    4826015     4269.\n# ℹ 55 more rows\n\n\nTesting for equality can be done using ==. This will only give TRUE for entries that are exactly the same as the test string.\n\nfilter(gapminder, country == \"Zambia\")\n\n# A tibble: 0 × 6\n# ℹ 6 variables: country &lt;chr&gt;, continent &lt;chr&gt;, year &lt;dbl&gt;, lifeExp &lt;dbl&gt;,\n#   pop &lt;dbl&gt;, gdpPercap &lt;dbl&gt;\n\n\nN.B. For partial matches, the grepl function and / or regular expressions (if you know them) can be used.\n\nfilter(gapminder, grepl(\"land\", country))\n\n# A tibble: 36 × 6\n   country continent  year lifeExp     pop gdpPercap\n   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 Finland Europe     1952    66.6 4090500     6425.\n 2 Finland Europe     1957    67.5 4324000     7545.\n 3 Finland Europe     1962    68.8 4491443     9372.\n 4 Finland Europe     1967    69.8 4605744    10922.\n 5 Finland Europe     1972    70.9 4639657    14359.\n 6 Finland Europe     1977    72.5 4738902    15605.\n 7 Finland Europe     1982    74.6 4826933    18533.\n 8 Finland Europe     1987    74.8 4931729    21141.\n 9 Finland Europe     1992    75.7 5041039    20647.\n10 Finland Europe     1997    77.1 5134406    23724.\n# ℹ 26 more rows\n\n\nWe can also test if rows are not equal to a value using !=\n\nfilter(gapminder, continent != \"Europe\")\n\n# A tibble: 670 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 660 more rows\n\n\n\ntesting more than one condition\nThere are a couple of ways of testing for more than one pattern. The first uses an or | statement. i.e. testing if the value of country is Zambia or the value is Zimbabwe. Remember to use double = sign to test for string equality; ==.\n\nfilter(gapminder, country == \"Zambia\" | country == \"Zimbabwe\")\n\n# A tibble: 0 × 6\n# ℹ 6 variables: country &lt;chr&gt;, continent &lt;chr&gt;, year &lt;dbl&gt;, lifeExp &lt;dbl&gt;,\n#   pop &lt;dbl&gt;, gdpPercap &lt;dbl&gt;\n\n\n\n\n\n\n\n\nThe difference between “and” and “or”\n\n\n\nConsider the following code. Is the output as you expect? 🤔\n\nfilter(gapminder, country == \"Zambia\", country == \"Zimbabwe\")\n\n# A tibble: 0 × 6\n# ℹ 6 variables: country &lt;chr&gt;, continent &lt;chr&gt;, year &lt;dbl&gt;, lifeExp &lt;dbl&gt;,\n#   pop &lt;dbl&gt;, gdpPercap &lt;dbl&gt;\n\n\n\n\nThe %in% function is a convenient function for testing which items in a vector correspond to a defined set of values.\n\nfilter(gapminder, country %in% c(\"Zambia\", \"Zimbabwe\"))\n\n# A tibble: 0 × 6\n# ℹ 6 variables: country &lt;chr&gt;, continent &lt;chr&gt;, year &lt;dbl&gt;, lifeExp &lt;dbl&gt;,\n#   pop &lt;dbl&gt;, gdpPercap &lt;dbl&gt;\n\n\nWe can require that two or more tests are TRUE, e.g. which years in Zambia had a life expectancy less than 40, by separating conditional statements by a ,. This performs an AND test so only rows that meet both conditions are returned.\n\nfilter(gapminder, country == \"Zambia\", lifeExp &lt; 40)\n\n# A tibble: 0 × 6\n# ℹ 6 variables: country &lt;chr&gt;, continent &lt;chr&gt;, year &lt;dbl&gt;, lifeExp &lt;dbl&gt;,\n#   pop &lt;dbl&gt;, gdpPercap &lt;dbl&gt;\n\n\n\n\n\n\n\n\nYou may have noticed that filter will always output the same number of columns as the input data frame. filter never changes the columns that are displayed. There are ways of using filter in conjunction with select as we will see later."
  },
  {
    "objectID": "training/r_part2/index.html#exercise",
    "href": "training/r_part2/index.html#exercise",
    "title": "Introduction to R - Part 2",
    "section": "Exercise",
    "text": "Exercise\n\n\nCreate a subset of the data where the population less than a million in the year 2002\nCreate a subset of the data where the life expectancy is greater than 75 in the years prior to 1987\nCreate a subset of the European data where the life expectancy is between 75 and 80 in the years 2002 or 2007.\nIf you are finished with these, try to explore alternative ways of performing the same filtering\n\n\n\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\n\n# Create a subset of the data where the population less than a million in the year 2002\nfilter(gapminder, pop &lt; 1e6, year == 2002)\n\n# A tibble: 5 × 6\n  country           continent  year lifeExp    pop gdpPercap\n  &lt;chr&gt;             &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1 Bahrain           Asia       2002    74.8 656397    23404.\n2 Comoros           Africa     2002    63.0 614382     1076.\n3 Djibouti          Africa     2002    53.4 447416     1908.\n4 Equatorial Guinea Africa     2002    49.3 495627     7703.\n5 Iceland           Europe     2002    80.5 288030    31163.\n\n# Create a subset of the data where the life expectancy is greater than 75 in the years prior to 1987\n\nfilter(gapminder, lifeExp &gt; 75, year &lt; 1987)\n\n# A tibble: 7 × 6\n  country          continent  year lifeExp       pop gdpPercap\n  &lt;chr&gt;            &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Canada           Americas   1982    75.8  25201900    22899.\n2 Greece           Europe     1982    75.2   9786480    15268.\n3 Hong Kong, China Asia       1982    75.4   5264500    14561.\n4 Iceland          Europe     1977    76.1    221823    19655.\n5 Iceland          Europe     1982    77.0    233997    23270.\n6 Japan            Asia       1977    75.4 113872473    16610.\n7 Japan            Asia       1982    77.1 118454974    19384.\n\n# Create a subset of the European data where the life expectancy is between 75 and 80 in the years 2002 or 2007\n\nfilter(gapminder, continent == \"Europe\", lifeExp &gt; 75, lifeExp &lt; 80 , year == 2002 | year == 2007)\n\n# A tibble: 20 × 6\n   country        continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;          &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Albania        Europe     2002    75.7  3508512     4604.\n 2 Albania        Europe     2007    76.4  3600523     5937.\n 3 Austria        Europe     2002    79.0  8148312    32418.\n 4 Austria        Europe     2007    79.8  8199783    36126.\n 5 Belgium        Europe     2002    78.3 10311970    30486.\n 6 Belgium        Europe     2007    79.4 10392226    33693.\n 7 Croatia        Europe     2007    75.7  4493312    14619.\n 8 Czech Republic Europe     2002    75.5 10256295    17596.\n 9 Czech Republic Europe     2007    76.5 10228744    22833.\n10 Denmark        Europe     2002    77.2  5374693    32167.\n11 Denmark        Europe     2007    78.3  5468120    35278.\n12 Finland        Europe     2002    78.4  5193039    28205.\n13 Finland        Europe     2007    79.3  5238460    33207.\n14 France         Europe     2002    79.6 59925035    28926.\n15 Germany        Europe     2002    78.7 82350671    30036.\n16 Germany        Europe     2007    79.4 82400996    32170.\n17 Greece         Europe     2002    78.3 10603863    22514.\n18 Greece         Europe     2007    79.5 10706290    27538.\n19 Ireland        Europe     2002    77.8  3879155    34077.\n20 Ireland        Europe     2007    78.9  4109086    40676.\n\n# A different version using a built-in dplyr function called between\n\nfilter(gapminder, continent == \"Europe\", \n       between(lifeExp, 75,80), \n       year %in% c(2002,2007))\n\n# A tibble: 20 × 6\n   country        continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;          &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Albania        Europe     2002    75.7  3508512     4604.\n 2 Albania        Europe     2007    76.4  3600523     5937.\n 3 Austria        Europe     2002    79.0  8148312    32418.\n 4 Austria        Europe     2007    79.8  8199783    36126.\n 5 Belgium        Europe     2002    78.3 10311970    30486.\n 6 Belgium        Europe     2007    79.4 10392226    33693.\n 7 Croatia        Europe     2007    75.7  4493312    14619.\n 8 Czech Republic Europe     2002    75.5 10256295    17596.\n 9 Czech Republic Europe     2007    76.5 10228744    22833.\n10 Denmark        Europe     2002    77.2  5374693    32167.\n11 Denmark        Europe     2007    78.3  5468120    35278.\n12 Finland        Europe     2002    78.4  5193039    28205.\n13 Finland        Europe     2007    79.3  5238460    33207.\n14 France         Europe     2002    79.6 59925035    28926.\n15 Germany        Europe     2002    78.7 82350671    30036.\n16 Germany        Europe     2007    79.4 82400996    32170.\n17 Greece         Europe     2002    78.3 10603863    22514.\n18 Greece         Europe     2007    79.5 10706290    27538.\n19 Ireland        Europe     2002    77.8  3879155    34077.\n20 Ireland        Europe     2007    78.9  4109086    40676."
  },
  {
    "objectID": "training/r_part2/index.html#manipulating-the-values-in-a-column-creating-new-columns",
    "href": "training/r_part2/index.html#manipulating-the-values-in-a-column-creating-new-columns",
    "title": "Introduction to R - Part 2",
    "section": "Manipulating the values in a column / creating new columns",
    "text": "Manipulating the values in a column / creating new columns\nAs well as selecting existing columns in the data frame, new columns can be created and existing ones manipulated using the mutate function. Typically a function or mathematical expression is applied to data in existing columns by row, and the result either stored in a new column or reassigned to an existing one. In other words, the number of values returned by the function must be the same as the number of input values. Multiple mutations can be performed in one line of code.\nHere, we create a new column of population in millions (PopInMillions) and round lifeExp to the nearest integer.\n\nmutate(gapminder, PopInMillions = pop / 1e6,\n       lifeExp = round(lifeExp))\n\n# A tibble: 862 × 7\n   country     continent  year lifeExp      pop gdpPercap PopInMillions\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n 1 Afghanistan Asia       1952      29  8425333      779.          8.43\n 2 Afghanistan Asia       1957      30  9240934      821.          9.24\n 3 Afghanistan Asia       1962      32 10267083      853.         10.3 \n 4 Afghanistan Asia       1967      34 11537966      836.         11.5 \n 5 Afghanistan Asia       1972      36 13079460      740.         13.1 \n 6 Afghanistan Asia       1977      38 14880372      786.         14.9 \n 7 Afghanistan Asia       1982      40 12881816      978.         12.9 \n 8 Afghanistan Asia       1987      41 13867957      852.         13.9 \n 9 Afghanistan Asia       1992      42 16317921      649.         16.3 \n10 Afghanistan Asia       1997      42 22227415      635.         22.2 \n# ℹ 852 more rows\n\n\n\n\n\n\n\n\nSomething to think about\n\n\n\nIn the previous code we created a new column called PopInMillions. Why does the following code now produce an error?\n\nselect(gapminder, PopInMillions)\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nThis code shows what a data frame looks like with a new column called PopInMillions.\n\nmutate(gapminder, PopInMillions = pop / 1e6,\n       lifeExp = round(lifeExp))\n\n# A tibble: 862 × 7\n   country     continent  year lifeExp      pop gdpPercap PopInMillions\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n 1 Afghanistan Asia       1952      29  8425333      779.          8.43\n 2 Afghanistan Asia       1957      30  9240934      821.          9.24\n 3 Afghanistan Asia       1962      32 10267083      853.         10.3 \n 4 Afghanistan Asia       1967      34 11537966      836.         11.5 \n 5 Afghanistan Asia       1972      36 13079460      740.         13.1 \n 6 Afghanistan Asia       1977      38 14880372      786.         14.9 \n 7 Afghanistan Asia       1982      40 12881816      978.         12.9 \n 8 Afghanistan Asia       1987      41 13867957      852.         13.9 \n 9 Afghanistan Asia       1992      42 16317921      649.         16.3 \n10 Afghanistan Asia       1997      42 22227415      635.         22.2 \n# ℹ 852 more rows\n\n\nIt does not alter the gapminder dataset itself. If we wanted to continue to work with PopInMillions, we would either need to create a new variable or overwrite the original gapminder dataset (not recommended)\n\ngapminder2 &lt;- mutate(gapminder, PopInMillions = pop / 1e6,\n       lifeExp = round(lifeExp))\n\nselect(gapminder2, PopInMillions)\n\n# A tibble: 862 × 1\n   PopInMillions\n           &lt;dbl&gt;\n 1          8.43\n 2          9.24\n 3         10.3 \n 4         11.5 \n 5         13.1 \n 6         14.9 \n 7         12.9 \n 8         13.9 \n 9         16.3 \n10         22.2 \n# ℹ 852 more rows\n\n\n\n\n\n\n\nSimilar to mutate, if we want to rename existing columns, and not create any extra columns, we can use the rename function.\n\nrename(gapminder, GDP=gdpPercap)\n\n# A tibble: 862 × 6\n   country     continent  year lifeExp      pop   GDP\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333  779.\n 2 Afghanistan Asia       1957    30.3  9240934  821.\n 3 Afghanistan Asia       1962    32.0 10267083  853.\n 4 Afghanistan Asia       1967    34.0 11537966  836.\n 5 Afghanistan Asia       1972    36.1 13079460  740.\n 6 Afghanistan Asia       1977    38.4 14880372  786.\n 7 Afghanistan Asia       1982    39.9 12881816  978.\n 8 Afghanistan Asia       1987    40.8 13867957  852.\n 9 Afghanistan Asia       1992    41.7 16317921  649.\n10 Afghanistan Asia       1997    41.8 22227415  635.\n# ℹ 852 more rows"
  },
  {
    "objectID": "training/r_part2/index.html#ordering-sorting",
    "href": "training/r_part2/index.html#ordering-sorting",
    "title": "Introduction to R - Part 2",
    "section": "Ordering / sorting",
    "text": "Ordering / sorting\nThe whole data frame can be re-ordered according to the values in one column using the arrange function. So to order the table according to population size:-\n\narrange(gapminder, pop)\n\n# A tibble: 862 × 6\n   country  continent  year lifeExp    pop gdpPercap\n   &lt;chr&gt;    &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1 Djibouti Africa     1952    34.8  63149     2670.\n 2 Djibouti Africa     1957    37.3  71851     2865.\n 3 Djibouti Africa     1962    39.7  89898     3021.\n 4 Bahrain  Asia       1952    50.9 120447     9867.\n 5 Djibouti Africa     1967    42.1 127617     3020.\n 6 Bahrain  Asia       1957    53.8 138655    11636.\n 7 Iceland  Europe     1952    72.5 147962     7268.\n 8 Comoros  Africa     1952    40.7 153936     1103.\n 9 Kuwait   Asia       1952    55.6 160000   108382.\n10 Iceland  Europe     1957    73.5 165110     9244.\n# ℹ 852 more rows\n\n\nThe default is smallest --&gt; largest but we can change this using the desc function\n\narrange(gapminder, desc(pop))\n\n# A tibble: 862 × 6\n   country continent  year lifeExp        pop gdpPercap\n   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 China   Asia       2007    73.0 1318683096     4959.\n 2 China   Asia       2002    72.0 1280400000     3119.\n 3 China   Asia       1997    70.4 1230075000     2289.\n 4 China   Asia       1992    68.7 1164970000     1656.\n 5 India   Asia       2007    64.7 1110396331     2452.\n 6 China   Asia       1987    67.3 1084035000     1379.\n 7 India   Asia       2002    62.9 1034172547     1747.\n 8 China   Asia       1982    65.5 1000281000      962.\n 9 India   Asia       1997    61.8  959000000     1459.\n10 China   Asia       1977    64.0  943455000      741.\n# ℹ 852 more rows\n\n\narrange also works on character vectors, arrange them alpha-numerically.\n\narrange(gapminder, desc(country))\n\n# A tibble: 862 × 6\n   country continent  year lifeExp     pop gdpPercap\n   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 Kuwait  Asia       1952    55.6  160000   108382.\n 2 Kuwait  Asia       1957    58.0  212846   113523.\n 3 Kuwait  Asia       1962    60.5  358266    95458.\n 4 Kuwait  Asia       1967    64.6  575003    80895.\n 5 Kuwait  Asia       1972    67.7  841934   109348.\n 6 Kuwait  Asia       1977    69.3 1140357    59265.\n 7 Kuwait  Asia       1982    71.3 1497494    31354.\n 8 Kuwait  Asia       1987    74.2 1891487    28118.\n 9 Kuwait  Asia       1992    75.2 1418095    34933.\n10 Kuwait  Asia       1997    76.2 1765345    40301.\n# ℹ 852 more rows\n\n\nWe can even order by more than one condition\n\narrange(gapminder, year, pop)\n\n# A tibble: 862 × 6\n   country           continent  year lifeExp    pop gdpPercap\n   &lt;chr&gt;             &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1 Djibouti          Africa     1952    34.8  63149     2670.\n 2 Bahrain           Asia       1952    50.9 120447     9867.\n 3 Iceland           Europe     1952    72.5 147962     7268.\n 4 Comoros           Africa     1952    40.7 153936     1103.\n 5 Kuwait            Asia       1952    55.6 160000   108382.\n 6 Equatorial Guinea Africa     1952    34.5 216964      376.\n 7 Gambia            Africa     1952    30   284320      485.\n 8 Gabon             Africa     1952    37.0 420702     4293.\n 9 Botswana          Africa     1952    47.6 442308      851.\n10 Guinea-Bissau     Africa     1952    32.5 580653      300.\n# ℹ 852 more rows\n\n\n\narrange(gapminder, year, continent, pop)\n\n# A tibble: 862 × 6\n   country                  continent  year lifeExp     pop gdpPercap\n   &lt;chr&gt;                    &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 Djibouti                 Africa     1952    34.8   63149     2670.\n 2 Comoros                  Africa     1952    40.7  153936     1103.\n 3 Equatorial Guinea        Africa     1952    34.5  216964      376.\n 4 Gambia                   Africa     1952    30    284320      485.\n 5 Gabon                    Africa     1952    37.0  420702     4293.\n 6 Botswana                 Africa     1952    47.6  442308      851.\n 7 Guinea-Bissau            Africa     1952    32.5  580653      300.\n 8 Congo, Rep.              Africa     1952    42.1  854885     2126.\n 9 Central African Republic Africa     1952    35.5 1291695     1071.\n10 Eritrea                  Africa     1952    35.9 1438760      329.\n# ℹ 852 more rows"
  },
  {
    "objectID": "training/r_part2/index.html#saving-data-frames",
    "href": "training/r_part2/index.html#saving-data-frames",
    "title": "Introduction to R - Part 2",
    "section": "Saving data frames",
    "text": "Saving data frames\nA final point on data frames is that we can write them to disk once we have done our data processing.\nLet’s create a folder in which to store such processed, “analysis-ready” data for sharing\n\ndir.create(\"out_data\",showWarnings = FALSE)\n## showWarnings will stop a message from appearing if the directory already exists\n\n\nbyWealth &lt;- arrange(gapminder, desc(gdpPercap))\n# check the output before writing\nhead(byWealth)\n\n# A tibble: 6 × 6\n  country continent  year lifeExp     pop gdpPercap\n  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1 Kuwait  Asia       1957    58.0  212846   113523.\n2 Kuwait  Asia       1972    67.7  841934   109348.\n3 Kuwait  Asia       1952    55.6  160000   108382.\n4 Kuwait  Asia       1962    60.5  358266    95458.\n5 Kuwait  Asia       1967    64.6  575003    80895.\n6 Kuwait  Asia       1977    69.3 1140357    59265.\n\nwrite_csv(byWealth, file = \"out_data/by_wealth.csv\")\n\nWe will now try an exercise that involves using several steps of these operations"
  },
  {
    "objectID": "training/r_part2/index.html#exercise-1",
    "href": "training/r_part2/index.html#exercise-1",
    "title": "Introduction to R - Part 2",
    "section": "Exercise",
    "text": "Exercise\n\n\nFilter the data to include just observations from the year 2002\nRe-arrange the table so that the countries from each continent are ordered according to decreasing wealth. i.e. the wealthiest countries first\nSelect all the columns apart from year\nWrite the data frame out to a file in out_data/ folder\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ngapminder2 &lt;- filter(gapminder, year == 2002)\ngapminder3 &lt;- arrange(gapminder2, continent, country, desc(gdpPercap))\ngapminder4 &lt;- select(gapminder3, -year)\nwrite_csv(gapminder4, \"out_data/gapminder_2002.csv\")"
  },
  {
    "objectID": "training/r_part2/index.html#piping",
    "href": "training/r_part2/index.html#piping",
    "title": "Introduction to R - Part 2",
    "section": "“Piping”",
    "text": "“Piping”\nAs have have just seen, we will often need to perform an analysis, or clean a dataset, using several dplyr functions in sequence. e.g. filtering, mutating, then selecting columns of interest (possibly followed by plotting - see shortly).\nAs a small example; if we wanted to filter our results to just Europe the continent column becomes redundant so we might as well remove it.\nThe following is perfectly valid R code, but invites the user to make mistakes and copy-and-paste errors when writing it. We also have to create multiple copies of the same data frame, which would not be desirable for large datasets.\n\ntmp &lt;- filter(gapminder, continent == \"Europe\")\ntmp2 &lt;- select(tmp, -continent)\ntmp2\n\n# A tibble: 192 × 5\n   country  year lifeExp     pop gdpPercap\n   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 Albania  1952    55.2 1282697     1601.\n 2 Albania  1957    59.3 1476505     1942.\n 3 Albania  1962    64.8 1728137     2313.\n 4 Albania  1967    66.2 1984060     2760.\n 5 Albania  1972    67.7 2263554     3313.\n 6 Albania  1977    68.9 2509048     3533.\n 7 Albania  1982    70.4 2780097     3631.\n 8 Albania  1987    72   3075321     3739.\n 9 Albania  1992    71.6 3326498     2497.\n10 Albania  1997    73.0 3428038     3193.\n# ℹ 182 more rows\n\n\nIn R, dplyr commands to be linked together and form a workflow. The symbol %&gt;% is pronounced then. With a %&gt;% the input to a function is assumed to be the output of the previous line. All the dplyr functions that we have seen so far take a data frame as an input and return an altered data frame as an output, so are amenable to this type of programming.\nThe example we gave of filtering just the European countries and removing the continent column becomes:-\n\nfilter(gapminder, continent==\"Europe\") %&gt;% \n  select(-continent)\n\n# A tibble: 192 × 5\n   country  year lifeExp     pop gdpPercap\n   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 Albania  1952    55.2 1282697     1601.\n 2 Albania  1957    59.3 1476505     1942.\n 3 Albania  1962    64.8 1728137     2313.\n 4 Albania  1967    66.2 1984060     2760.\n 5 Albania  1972    67.7 2263554     3313.\n 6 Albania  1977    68.9 2509048     3533.\n 7 Albania  1982    70.4 2780097     3631.\n 8 Albania  1987    72   3075321     3739.\n 9 Albania  1992    71.6 3326498     2497.\n10 Albania  1997    73.0 3428038     3193.\n# ℹ 182 more rows\n\n\nHopefully you will agree that the code is much cleaner and easier to read and write.\n\n\n\n\nExercise\n\n\nRe-write your solution to the previous exercise, but using the %&gt;% symbol\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nfilter(gapminder, year == 2002) %&gt;% \n  arrange(continent, desc(gdpPercap)) %&gt;% \n  select(-year) %&gt;% \nwrite_csv(\"out_data/gapminder_piped_2002.csv\")\n\n\n\n\nWe will leave dplyr for the moment (although it will never be far away from us, since it is such a fundamental tool…) and start to look at making some nice graphs to understand our data.\n\n\n\n\n\n\nCeci n’est pas une pipe\n\n\n\nThe %&gt;% operation was introduced as part of the magrittr package, which gets loaded automatically as part of dplyr. However, the dplyr package is quite large and involves a lot of dependencies. If you only wanted to use the %&gt;% and not any other part of dplyr it would be quite inefficient to load the entire dplyr package as part of your code.\nAn equivalent |&gt; operation is available as part of base R. This means you can use piping without having to load the whole of dplyr. This is not an issue for these materials since we are working with dplyr quite a lot, but worth mentioning for completeness as you may see |&gt; used elsewhere. The code is exactly the same.\n\nfilter(gapminder, year == 2002) |&gt;\n  arrange(continent, desc(gdpPercap)) |&gt;\n  select(-year)\n\n# A tibble: 71 × 5\n   country           continent lifeExp      pop gdpPercap\n   &lt;chr&gt;             &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Gabon             Africa       56.8  1299304    12522.\n 2 Botswana          Africa       46.6  1630347    11004.\n 3 Equatorial Guinea Africa       49.3   495627     7703.\n 4 Algeria           Africa       71.0 31287142     5288.\n 5 Egypt             Africa       69.8 73312559     4755.\n 6 Congo, Rep.       Africa       53.0  3328795     3484.\n 7 Angola            Africa       41.0 10866106     2773.\n 8 Cameroon          Africa       49.9 15929988     1934.\n 9 Djibouti          Africa       53.4   447416     1908.\n10 Cote d'Ivoire     Africa       46.8 16252726     1649.\n# ℹ 61 more rows"
  },
  {
    "objectID": "training/r_part2/index.html#why-use-ggplot2",
    "href": "training/r_part2/index.html#why-use-ggplot2",
    "title": "Introduction to R - Part 2",
    "section": "Why use ggplot2?",
    "text": "Why use ggplot2?\nThe structured syntax and high level of abstraction used by ggplot2 should allow for the user to concentrate on the visualisations instead of creating the underlying code.\nOn top of this central philosophy ggplot2 has:\n\nIncreased flexibility over many plotting systems.\nAn advanced theme system for professional/publication level graphics.\nLarge developer base – Many libraries extending its flexibility.\nLarge user base – Great documentation and active mailing list.\n\n\n\n\n\n\n\nTop tip\n\n\n\nIt is always useful to think about the message you want to convey and the appropriate plot before writing any R code. Resources like data-to-viz.com should help. Don’t be afraid to even sketch out the plot on paper or a whiteboard!\n\n\nWith some practice, ggplot2 makes it easier to go from the figure you are imagining in our head (or on paper) to a publication-ready image in R.\n\n\n\n\n\n\nAnother “cheatsheet”\n\n\n\nAs with dplyr, we won’t have time to cover all details of ggplot2. This is however a useful cheatsheet that can be printed as a reference. The cheatsheet is also available through the RStudio Help menu."
  },
  {
    "objectID": "training/r_part2/index.html#basic-plot-types",
    "href": "training/r_part2/index.html#basic-plot-types",
    "title": "Introduction to R - Part 2",
    "section": "Basic plot types",
    "text": "Basic plot types\nA plot in ggplot2 is created with the following type of command. N.B. please dont try and run this code, it just an overall sketch of what our ggplot2 code will look like.\nggplot(data = &lt;DATA&gt;, mapping = aes(&lt;MAPPINGS&gt;)) +  &lt;GEOM_FUNCTION&gt;()\nSo we need to specify\n\nThe data to be used in graph\nMappings of data to the graph (aesthetic mapping)\nWhat type of graph we want to use (The geom to use).\n\nLets say that we want to explore the relationship between GDP and Life Expectancy. We might start with the hypothesis that richer countries have higher life expectancy. A sensible choice of plot would be a scatter plot with gdp on the x-axis and life expectancy on the y-axis.\nThe first stage is to specify our dataset using the data argument. ggplot2 is great, but not clever enough to know what kind of plot we might want. It just creates a blank canvas.\n\nlibrary(ggplot2)\nggplot(data = gapminder)\n\n\n\n\n\n\n\n\nFor the aesthetics, as a bare minimum we will map the gdpPercap and lifeExp to the x- and y-axis of the plot. Some progress is made; we at least get axes\n\nggplot(data = gapminder,aes(x=gdpPercap, y=lifeExp))\n\n\n\n\n\n\n\n\nThat created the axes, but we still need to define how to display our points on the plot. As we have continuous data for both the x- and y-axis, geom_point is a good choice.\n\nggplot(data = gapminder,aes(x=gdpPercap, y=lifeExp)) + geom_point()\n\n\n\n\n\n\n\n\nThe geom we use will depend on what kind of data we have (continuous, categorical etc)\n\ngeom_point() - Scatter plots\ngeom_line() - Line plots\ngeom_smooth() - Fitted line plots\ngeom_bar() - Bar plots\ngeom_boxplot() - Boxplots\ngeom_jitter() - Jitter to plots\ngeom_histogram() - Histogram plots\ngeom_density() - Density plots\ngeom_text() - Text to plots\ngeom_errorbar() - Errorbars to plots\ngeom_violin() - Violin plots\ngeom_tile() - for “heatmap”-like plots\n\nBoxplots are commonly used to visualise the distributions of continuous data. We have to use a categorical variable on the x-axis such as continent or country (not advisable in this case as there are too many different values).\nThe order of the boxes along the x-axis is dictated by the order of categories in the factor; with the default for names being alphabetical order.\n\nggplot(gapminder, aes(x = continent, y=gdpPercap)) + geom_boxplot()\n\n\n\n\n\n\n\n\nA histogram is a common method for visualising a distribution of numeric values. Your data are split into a number of bins (which can be altered in the code) across the whole data range, and the number of observations in each bin is shown on the y-axis. Thus you can see where the majority of your data points are\n\nggplot(gapminder, aes(x = gdpPercap)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nFor categorical data, e.g. the country of continent columns in our case, a barplot will show the number of times each category is observed. The geom_bar will do the job of counting and plotting.\n\nggplot(gapminder, aes(x=continent)) + geom_bar()\n\n\n\n\n\n\n\n\nIf you have particular numeric values you want to display in a barplot you can use geom_col. To give an example we will first filter the data to a particular year and continent. The gdpPercap values for each country can then be plotted. In the below plot the axis labels will be messy and difficult to read. This is something that can be customised with some of the ggplot2 options we will explore later.\n\ngapminder2002 &lt;- filter(gapminder, year==2002,continent==\"Americas\")\n\n## Notice that we plot the variable we have just created and not gapminder\n## You could also do this in one step using the piping technique for earlier\n\nggplot(gapminder2002, aes(x=country,y=gdpPercap)) + geom_col()\n\n\n\n\n\n\n\n\nWhere appropriate, we can add multiple layers of geoms to the plot. For instance, a criticism of the boxplot is that it does not show all the data. We can rectify this by overlaying the individual points. This can give a representation of how many data points there are.\n\nggplot(gapminder, aes(x = continent, y=gdpPercap)) + geom_boxplot() + geom_point()\n\n\n\n\n\n\n\n\nHowever, the default x-coordinate is always the same for each category. Adding some random “noise” to the x-axis can help using geom_jitter.\n\nggplot(gapminder, aes(x = continent, y=gdpPercap)) + geom_boxplot() + geom_jitter(width=0.1)\n\n\n\n\n\n\n\n\n\n\n\n\nExercises\n\n\nThe violin plot is a popular alternative to the boxplot. Create a violin plot with geom_violin to visualise the differences in GDP between different continents.\nCreate a subset of the gapminder data frame containing just the rows for your country of birth\nHas there been an increase in life expectancy over time?\n\nvisualise the trend using a scatter plot (geom_point), line graph (geom_line) or smoothed line (geom_smooth).\n\nWhat happens when you modify the geom_boxplot example to compare the gdp distributions for different years?\n\nLook at the message ggplot2 prints above the plot and try to modify the code to give a separate boxplot for each year\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nggplot(gapminder, aes(x = continent, y = gdpPercap)) + geom_violin()\n\n## I choose United Kingdom here, but pick a different one if you like\n\nuk_data &lt;- filter(gapminder, country == \"United Kingdom\")\n\n## As a scatter plot\nggplot(uk_data, aes(x = year, y = lifeExp)) + geom_point()\n\n## As a line plot\nggplot(uk_data, aes(x = year, y = lifeExp)) + geom_line()\n\n## With a smoothed line\nggplot(uk_data, aes(x = year, y = lifeExp)) + geom_smooth()\n\n\n## more than one of the above\n## You can also fit a straight line (via a linear model) by changing method\n\nggplot(uk_data, aes(x = year, y = lifeExp)) + geom_point() + geom_smooth(method = \"lm\")\n\n## this exercise could also make use of the piping technique\n\nfilter(gapminder, country == \"United Kingdom\") %&gt;% \n  ggplot(aes(x = year, y = lifeExp)) + geom_point() + geom_smooth()\n\n# this is how we might expect the code to look like\nggplot(gapminder, aes(x = year, y = gdpPercap)) + geom_boxplot()\n\n# The previous output hints that you might want to group by year - otherwise it thinks that year is a numerical variable\n\n\nggplot(gapminder, aes(x = year, y = gdpPercap, group=year)) + geom_boxplot()\n\n# You may sometimes see this as a possible solution which fixes the year to be a categorical variable\nggplot(gapminder, aes(x = as.factor(year), y = gdpPercap)) + geom_boxplot()\n\n\n\n\n\n\n\nAs we have seen already, ggplot offers an interface to create many popular plot types. It is up to the user to decide what the best way to visualise the data."
  },
  {
    "objectID": "training/r_part2/index.html#customising-the-plot-appearance",
    "href": "training/r_part2/index.html#customising-the-plot-appearance",
    "title": "Introduction to R - Part 2",
    "section": "Customising the plot appearance",
    "text": "Customising the plot appearance\nOur plots are a bit dreary at the moment, but one way to add colour is to add a col argument to the geom_point function. The value can be any of the pre-defined colour names in R. These are displayed in this handy online reference. Red, Green, Blue of Hex values can also be given.\n\nggplot(gapminder, aes(x = gdpPercap, y=lifeExp)) + geom_point(col=\"red\")\n\n\n\n\n\n\n\n\n\n# Use the Hex codes from Farrow and Ball: https://convertingcolors.com/list/farrow-ball.html\n# (cook's blue)\n\nggplot(gapminder, aes(x = gdpPercap, y=lifeExp)) + geom_point(col=\"#6A90B4\")\n\n\n\n\n\n\n\n\nHowever, whilst looking nicer this doesn’t really tell us anything about the data. For example, what are the points to the far right? Do they belong to a particular country or continent? A powerful feature of ggplot2 is that colours are treated as aesthetics of the plot. In other words we can use a column in our dataset.\nLet’s say that we want points on our plot to be coloured according to continent. We add an extra argument to the definition of aesthetics to define the mapping. ggplot2 will even decide on colours and create a legend for us. Don’t worry if you don’t like the colours chosen, all of this can be customised.\n\nggplot(gapminder, aes(x = gdpPercap, y=lifeExp,col=continent)) + geom_point()\n\n\n\n\n\n\n\n\nIt will even choose a continuous or discrete colour scale based on the data type. We have already seen that ggplot2 is treating our year column as numerical data; which is probably not very useful for visualisation.\n\nggplot(gapminder, aes(x = gdpPercap, y=lifeExp,col=year)) + geom_point()\n\n\n\n\n\n\n\n\nWe can force ggplot2 to treat year as categorical data by using as.factor when creating the aesthetics.\n\nggplot(gapminder, aes(x = gdpPercap, y=lifeExp,col=as.factor(year))) + geom_point()\n\n\n\n\n\n\n\n\nWhen used in the construction of a boxplot, the col argument will change the colour of the lines. To change the colour of the boxes we have to use fill.\n\nggplot(gapminder, aes(x = continent, y=gdpPercap,fill=continent)) + geom_boxplot()"
  },
  {
    "objectID": "training/r_part2/index.html#help-with-dplyr-functions",
    "href": "training/r_part2/index.html#help-with-dplyr-functions",
    "title": "Introduction to R - Part 2",
    "section": "Help with dplyr functions",
    "text": "Help with dplyr functions\n\ndplyr cheatsheet. The “cheatsheet” is also available through the RStudio Help menu. I personally tend to think of it as cheating to have such information to hand. There are far too many functions to remember all of them!\n\n:::\nBefore using any of these functions, we need to load the library:-\n\nlibrary(dplyr)\n\n\nselecting columns\nWe can access the columns of a data frame using the select function. This lets us have control over what is printed to the screen. Admitedly the dataset we are using here is rather small (being only six columns), but these useful functions really shine when faced with 10s or 100s of columns\n\nby name\nFirstly, we can select column by name, by adding bare column names (i.e. not requiring quote marks around the name) after the name of the data frame, separated by a , .\n\nselect(gapminder, country, continent)\n\n# A tibble: 1,704 × 2\n   country     continent\n   &lt;chr&gt;       &lt;chr&gt;    \n 1 Afghanistan Asia     \n 2 Afghanistan Asia     \n 3 Afghanistan Asia     \n 4 Afghanistan Asia     \n 5 Afghanistan Asia     \n 6 Afghanistan Asia     \n 7 Afghanistan Asia     \n 8 Afghanistan Asia     \n 9 Afghanistan Asia     \n10 Afghanistan Asia     \n# ℹ 1,694 more rows\n\n\nNow lets imagine that we want to see all the columns apart from country. It would quickly become tedious, not to mention and prone to error, if we had to type every column name we wanted to keep by-hand.\nThankfully, we can also omit columns from the ouput by putting a minus (-) in front of the column name. Note that this is not the same as removing the column from the data permanently.\n\nselect(gapminder, -country)\n\n# A tibble: 1,704 × 5\n   continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Asia       1952    28.8  8425333      779.\n 2 Asia       1957    30.3  9240934      821.\n 3 Asia       1962    32.0 10267083      853.\n 4 Asia       1967    34.0 11537966      836.\n 5 Asia       1972    36.1 13079460      740.\n 6 Asia       1977    38.4 14880372      786.\n 7 Asia       1982    39.9 12881816      978.\n 8 Asia       1987    40.8 13867957      852.\n 9 Asia       1992    41.7 16317921      649.\n10 Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\n\n\n\n\n\n\nThe dplyr package has been carefully developed over the years with the needs of the data analyst in mind. Ideally we would rather be spending our time exploring and understanding data than writing reams of code. For this reason, you will often find a helpful function for a common task.\nIf you find yourself having to write lots of code to achieve a data manipulation task, the chances are the a convenient function already exists.\n\n\n\n\n\nrange of columns\nA range of columns can be selected by the : operator.\n\nselect(gapminder, lifeExp:gdpPercap)\n\n# A tibble: 1,704 × 3\n   lifeExp      pop gdpPercap\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1    28.8  8425333      779.\n 2    30.3  9240934      821.\n 3    32.0 10267083      853.\n 4    34.0 11537966      836.\n 5    36.1 13079460      740.\n 6    38.4 14880372      786.\n 7    39.9 12881816      978.\n 8    40.8 13867957      852.\n 9    41.7 16317921      649.\n10    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\n\n\nhelper functions\nThere are a number of helper functions can be employed if we are unsure about the exact name of the column.\n\nselect(gapminder, starts_with(\"co\"))\n\n# A tibble: 1,704 × 2\n   country     continent\n   &lt;chr&gt;       &lt;chr&gt;    \n 1 Afghanistan Asia     \n 2 Afghanistan Asia     \n 3 Afghanistan Asia     \n 4 Afghanistan Asia     \n 5 Afghanistan Asia     \n 6 Afghanistan Asia     \n 7 Afghanistan Asia     \n 8 Afghanistan Asia     \n 9 Afghanistan Asia     \n10 Afghanistan Asia     \n# ℹ 1,694 more rows\n\nselect(gapminder, contains(\"life\"))\n\n# A tibble: 1,704 × 1\n   lifeExp\n     &lt;dbl&gt;\n 1    28.8\n 2    30.3\n 3    32.0\n 4    34.0\n 5    36.1\n 6    38.4\n 7    39.9\n 8    40.8\n 9    41.7\n10    41.8\n# ℹ 1,694 more rows\n\n# selecting the last and penultimate columns\nselect(gapminder, last_col(1),last_col())\n\n# A tibble: 1,704 × 2\n        pop gdpPercap\n      &lt;dbl&gt;     &lt;dbl&gt;\n 1  8425333      779.\n 2  9240934      821.\n 3 10267083      853.\n 4 11537966      836.\n 5 13079460      740.\n 6 14880372      786.\n 7 12881816      978.\n 8 13867957      852.\n 9 16317921      649.\n10 22227415      635.\n# ℹ 1,694 more rows\n\n\nIt is also possible to use the column number in the selection.\n\nselect(gapminder, 4:6)\n\n# A tibble: 1,704 × 3\n   lifeExp      pop gdpPercap\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1    28.8  8425333      779.\n 2    30.3  9240934      821.\n 3    32.0 10267083      853.\n 4    34.0 11537966      836.\n 5    36.1 13079460      740.\n 6    38.4 14880372      786.\n 7    39.9 12881816      978.\n 8    40.8 13867957      852.\n 9    41.7 16317921      649.\n10    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nThe select function can be used with just a single column name - in a similar manner to the $ operation we saw in Part 1. However, select always returns a data frame whereas $ gives a vector. Compare the output of the following code chunks\n\nselect(gapminder, pop)\n\n# A tibble: 1,704 × 1\n        pop\n      &lt;dbl&gt;\n 1  8425333\n 2  9240934\n 3 10267083\n 4 11537966\n 5 13079460\n 6 14880372\n 7 12881816\n 8 13867957\n 9 16317921\n10 22227415\n# ℹ 1,694 more rows\n\n\n\ngapminder$pop\n\nThe consequence of this is that you cannot use functions such as mean in combination with select\n\npops &lt;- select(gapminder, pop)\nmean(pops)\n\nIn the next session we will see how to calculate summary statistics on particular columns in our data. For now, a useful function is pull that will return the correct type of data required for a function such as mean.\n\npops &lt;- pull(gapminder,pop)\nmean(pops)\n\n[1] 29601212"
  },
  {
    "objectID": "training/r_part1/index.html#solutions",
    "href": "training/r_part1/index.html#solutions",
    "title": "Introduction to R - Part 1",
    "section": "Solutions",
    "text": "Solutions\n\n## The digits argument needs to be changed\nround(pi,digits = 3)\n\n[1] 3.142\n\n## Use the length.out argument\nseq(from = 2, to = 20, length.out = 5)\n\n[1]  2.0  6.5 11.0 15.5 20.0\n\n## Make sure you create a variable\n\nmy_numbers &lt;- rnorm(n = 1000, mean = 2, sd = 3)\n\nmax(my_numbers)\n\n[1] 11.39027\n\nmin(my_numbers)\n\n[1] -7.072349\n\nmean(my_numbers)\n\n[1] 2.080459\n\n\n:::\n\n\n\n\n\n\nAbout random numbers…\n\n\n\nSometimes we just want to create some numbers or data that we can play around with. However, most likely we will be concerned about the reproducibility of our R code. In circumstances where randomness is involved it is common to set a “seed” which ensures the same random numbers are generated each time.\n\nset.seed(123)\nrnorm(10)\n\n [1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499\n [7]  0.46091621 -1.26506123 -0.68685285 -0.44566197"
  },
  {
    "objectID": "training/r_part1/index.html#coming-next",
    "href": "training/r_part1/index.html#coming-next",
    "title": "Introduction to R - Part 1",
    "section": "Coming next…",
    "text": "Coming next…\nIn Part 2 we will start to interrogate and visualise the data we have just imported\n\nChoosing which columns to show from the data\nChoosing what rows to keep in the data\nAdding / altering columns\nSorting the rows in our data\nIntroduction to plotting"
  },
  {
    "objectID": "training/r_part3/index.html#topics-covered",
    "href": "training/r_part3/index.html#topics-covered",
    "title": "Introduction to R - Part 3",
    "section": "",
    "text": "Customising ggplot2 plots\nSummarising data\nGroup-based summaries\nJoining data\nData Cleaning\n\nLets make sure we have read the gapminder data into R and have the relevant packages loaded.\n\n## Checks if the required file is present, and downloads if not\n\nif(!file.exists(\"raw_data/gapminder.csv\")) {\n  dir.create(\"raw_data/\",showWarnings = FALSE)\ndownload.file(\"https://raw.githubusercontent.com/markdunning/markdunning.github.com/refs/heads/master/files/training/r/raw_data/gapminder.csv\", destfile = \"raw_data/gapminder.csv\")\n}\n\nWe also discussed in the previous part(s) how to read the example dataset into R. We will also load the libraries needed.\n\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\ngapminder &lt;- read_csv(\"raw_data/gapminder.csv\")"
  },
  {
    "objectID": "training/r_part3/index.html#customising-a-plot",
    "href": "training/r_part3/index.html#customising-a-plot",
    "title": "Introduction to R - Part 3",
    "section": "",
    "text": "Now make a scatter plot of gdp versus life expectancy as we did in the previous session. One of the last topics we covered was how to add colour to a plot. This can make the plot more appealing, but also help with data interpretation. In this case, we can use different colours to indicate countries belonging to different continents. For example, we can see a cluster of Asia data points with unusually large GDP. At some point we might want to adjust the scale on the x-axis to make the trend between the two axes easier to visualise.\n\nggplot(gapminder, aes(x = gdpPercap, y=lifeExp,col=continent)) + geom_point()\n\n\n\n\n\n\n\n\nThe shape and size of points can also be mapped from the data. However, it is easy to get carried away!\n\nggplot(gapminder, aes(x = gdpPercap, y=lifeExp,shape=continent,size=pop)) + geom_point()\n\n\n\n\n\n\n\n\nScales and their legends have so far been handled using ggplot2 defaults. ggplot2 offers functionality to have finer control over scales and legends using the scale methods.\nScale methods are divided into functions by combinations of\n\nthe aesthetics they control.\nthe type of data mapped to scale.\n\nscale_aesthetic_type\nTry typing in scale_ then tab to autocomplete. This will provide some examples of the scale functions available in ggplot2.\nAlthough different scale functions accept some variety in their arguments, common arguments to scale functions include -\n\nname - The axis or legend title\nlimits - Minimum and maximum of the scale\nbreaks - Label/tick positions along an axis\nlabels - Label names at each break\nvalues - the set of aesthetic values to map data values\n\nWe can choose specific colour palettes, such as those provided by the RColorBrewer package. This package is included with R (so you don’t need to install it) and provides palettes for different types of scale (sequential, diverging, qualitative).\n\nlibrary(RColorBrewer)\ndisplay.brewer.all(colorblindFriendly = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen creating a plot, always check that the colour scheme is appropriate for people with various forms of colour-blindness\n\n\nWhen experimenting with colour palettes and labels, it is useful to save the plot as an object. This saves quite a bit of typing! Notice how nothing get shown on the screen.\n\np &lt;- ggplot(gapminder, aes(x = gdpPercap, y=lifeExp,col=continent)) + geom_point()\n\nRunning the line of code with just p now shows the plot on the screen\n\np \n\n\n\n\n\n\n\n\nBut we can also make modifications to the plot with the + symbol. Here, we change the colours to those defined as Set2 in RColorBrewer.\n\n## Here we pick 6 colours from the palette\np + scale_color_manual(values=brewer.pal(6,\"Set2\"))\n\n\n\n\n\n\n\n\nVarious labels can be modified using the labs function.\n\np + labs(x=\"Wealth\",y=\"Life Expectancy\",title=\"Relationship between Wealth and Life Expectancy\")\n\n\n\n\n\n\n\n\nWe can also modify the x- and y- limits of the plot so that any outliers are not shown. ggplot2 will give a warning that some points are excluded.\n\np + xlim(0,60000)\n\nWarning: Removed 5 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nSaving is supported by the ggsave function and automatically saves the last plot that was displayed in RStudio. A variety of file formats are supported (.png, .pdf, .tiff, etc) and the format used is determined from the extension given in the file argument. The height, width and resolution can also be configured. See the help on ggsave (?ggsave) for more information.\n\nggsave(file=\"my_ggplot.png\")\n\nSaving 7 x 5 in image\n\n\nWarning: Removed 5 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nMost aspects of the plot can be modified from the background colour to the grid sizes and font. Several pre-defined “themes” exist and we can modify the appearance of the whole plot using a theme_.. function.\n\np + theme_bw()\n\n\n\n\n\n\n\n\nMore themes are supported by the ggthemes package. You can make your plots look like the Economist, Wall Street Journal or Excel (but please don’t do this!)\n\n## this will check if ggthemes is already installed, and will only install if it is not found\n\nif(!require(\"ggthemes\")) install.packages(\"ggthemes\")\n\nLoading required package: ggthemes\n\nlibrary(ggthemes)\np + theme_excel()"
  },
  {
    "objectID": "training/r_part3/index.html#exercise",
    "href": "training/r_part3/index.html#exercise",
    "title": "Introduction to R - Part 3",
    "section": "",
    "text": "Use a boxplot to compare the life expectancy values of Australia and New Zealand. Use a Set2 palette from RColorBrewer to colour the boxplots and apply a “minimal” theme to the plot.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ngapminder %&gt;% \n  filter(continent == \"Oceania\") %&gt;% \n  ggplot(aes(x = country, y = lifeExp,fill=country)) + geom_boxplot() + scale_fill_manual(values=brewer.pal(2,\"Set2\")) + theme_bw()\n\n\n\n\nAnother transformation that is useful in this case is to display the x-axis on a log\\(_10\\) scale. This compresses the values on the x-axis (reducing the impact of the high outliers) and makes trends easier to spot\n\np + scale_x_log10()\n\n\n\n\n\n\n\n\nIt now seems that lifeExp is increasing in a roughly linear fashion with the GDP (on a log\\(_10\\) scale).\n\n\n\n\n\n\nAbout the log transformation\n\n\n\n\n\nThe logarithm of 10 (log10) is the exponent to which the base 10 must be “raised” to obtain the number 10. For example, log10(10) = 1, as 10 raised to the power of 1 equals 10.\n\nlog10(10)\n\n[1] 1\n\n10^1\n\n[1] 10\n\nlog10(100)\n\n[1] 2\n\n10^2\n\n[1] 100\n\n\nThis transformation helps in simplifying visualisation involving large numbers. The range of our gdpPercap values is extremely large. summary is a quick way to get various summary statistics from our data\n\n## we will use the $ notation for now\n\nsummary(gapminder$gdpPercap)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n   241.2   1202.1   3531.8   7215.3   9325.5 113523.1 \n\n\nAfter a log10 transformation the data are much more compressed.\n\nsummary(log10(gapminder$gdpPercap))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.382   3.080   3.548   3.543   3.970   5.055 \n\n\nThe largest value after the log\\(_10\\) transformation is around 5\n\n10^5.055\n\n[1] 113501.1"
  },
  {
    "objectID": "training/r_part3/index.html#facets",
    "href": "training/r_part3/index.html#facets",
    "title": "Introduction to R - Part 3",
    "section": "",
    "text": "One very useful feature of ggplot2 is faceting. This allows you to produce plots for subsets and groupings in your data (aka “facets”). In the scatter plot above, it was quite difficult to determine if the relationship between gdp and life expectancy was the same for each continent. To overcome this, we would like a see a separate plot for each continent.\nIn we attempted such a task manually we might start off by plotting Africa\n\nafr_plot&lt;- gapminder %&gt;% \n  filter(continent == \"Africa\") %&gt;% \n  ggplot(aes(x = gdpPercap, y = lifeExp)) + geom_point() + scale_x_log10()\nafr_plot\n\n\n\n\n\n\n\n\nAnd then the same for Americas:-\n\namr_plot &lt;- gapminder %&gt;% \n  filter(continent == \"Americas\") %&gt;% \n  ggplot(aes(x = gdpPercap, y = lifeExp)) + geom_point() + scale_x_log10()\namr_plot\n\n\n\n\n\n\n\n\nAt some point we will have to stitch the plots together (which is possible, but we will cover this later) and make sure we have equivalent scales for all plots. In this setup we are manually specifying the name of the continent, which is prone to error. Again, we could use something like a for loop to make the plots for each continent. However, we aren’t covering such techniques in these materials as dplyr and ggplot2 don’t tend to require them.\nAs we said before, dplyr, and ggplot2 are built with the analyst in mind and have many useful features for automating some common tasks. To achieve the plot we want is surprisingly simple. To “facet” our data into multiple plots we can use the facet_wrap (1 variable) or facet_grid (2 variables) functions and specify the variable(s) we split by.\n\np + facet_wrap(~continent) + scale_x_log10() + xlab(\"GDP (log10)\") + ylab(\"Life Expectancy\")\n\n\n\n\n\n\n\n\nThe facet_grid function will create a grid-like plot with one variable on the x-axis and another on the y-axis.\n\np + facet_grid(continent~year)\n\n\n\n\n\n\n\n\nThe previous plot was a bit messy as it contained all combinations of year and continent. Let’s suppose we want our analysis to be a bit more focused and disregard countries in Oceania (as there are only 2 in our dataset) and maybe years between 1997 and 2002. However, we can only “add” more information from our plots and not take anything away. Therefore the suggested approach is to pre-filter and manipulate the data into the form you want for plotting.\nWeknow how to restrict the rows from the gapminder dataset using the filter function. Instead of filtering the data, creating a new data frame, and constructing the data frame from these new data we can use the%&gt;% operator to create the data frame “on the fly” and pass directly to ggplot. Thus we don’t have to save a new data frame or alter the original data.\n\nfilter(gapminder, continent!=\"Oceania\", year %in% c(1997,2002,2007)) %&gt;% \n  ggplot(aes(x = gdpPercap, y=lifeExp,col=continent)) + geom_point() + facet_grid(continent~year)\n\n\n\n\n\n\n\n\nThere is lots more to cover on ggplot2 and quickly we can start to understand our data without too much in the way of coding. When it comes to reporting and justifying our findings we will need to produce some numerical summaries. We tackle this in the next section."
  },
  {
    "objectID": "training/r_part3/index.html#introducing-the-covid-19-data",
    "href": "training/r_part3/index.html#introducing-the-covid-19-data",
    "title": "Introduction to R - Part 3",
    "section": "Introducing the COVID-19 data",
    "text": "Introducing the COVID-19 data\nData for global COVID-19 cases are available online from CSSE at Johns Hopkins University on their github repository.\n\ngithub is an excellent way of making your code and analysis available for others to reuse and share. Private repositories with restricted access are also available. Here is a useful beginners guide.\n-Friendly github intro\n\nR is capable of downloading files to our own machine so we can analyse them. We need to know the URL (for the COVID data we can find this from github, or use the address below) and can specify what to call the file when it is downloaded.\n\ndownload.file(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\",destfile = \"raw_data/time_series_covid19_confirmed_global.csv\")\n\nWe can use the read_csv function as before to import the data and take a look. We can see the basic structure of the data is one row for each country / region and columns for cases on each day.\n\ncovid &lt;- read_csv(\"raw_data/time_series_covid19_confirmed_global.csv\")\n\nRows: 289 Columns: 1147\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr    (2): Province/State, Country/Region\ndbl (1145): Lat, Long, 1/22/20, 1/23/20, 1/24/20, 1/25/20, 1/26/20, 1/27/20,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncovid\n\n# A tibble: 289 × 1,147\n   `Province/State`  `Country/Region`   Lat   Long `1/22/20` `1/23/20` `1/24/20`\n   &lt;chr&gt;             &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 &lt;NA&gt;              Afghanistan       33.9  67.7          0         0         0\n 2 &lt;NA&gt;              Albania           41.2  20.2          0         0         0\n 3 &lt;NA&gt;              Algeria           28.0   1.66         0         0         0\n 4 &lt;NA&gt;              Andorra           42.5   1.52         0         0         0\n 5 &lt;NA&gt;              Angola           -11.2  17.9          0         0         0\n 6 &lt;NA&gt;              Antarctica       -71.9  23.3          0         0         0\n 7 &lt;NA&gt;              Antigua and Bar…  17.1 -61.8          0         0         0\n 8 &lt;NA&gt;              Argentina        -38.4 -63.6          0         0         0\n 9 &lt;NA&gt;              Armenia           40.1  45.0          0         0         0\n10 Australian Capit… Australia        -35.5 149.           0         0         0\n# ℹ 279 more rows\n# ℹ 1,140 more variables: `1/25/20` &lt;dbl&gt;, `1/26/20` &lt;dbl&gt;, `1/27/20` &lt;dbl&gt;,\n#   `1/28/20` &lt;dbl&gt;, `1/29/20` &lt;dbl&gt;, `1/30/20` &lt;dbl&gt;, `1/31/20` &lt;dbl&gt;,\n#   `2/1/20` &lt;dbl&gt;, `2/2/20` &lt;dbl&gt;, `2/3/20` &lt;dbl&gt;, `2/4/20` &lt;dbl&gt;,\n#   `2/5/20` &lt;dbl&gt;, `2/6/20` &lt;dbl&gt;, `2/7/20` &lt;dbl&gt;, `2/8/20` &lt;dbl&gt;,\n#   `2/9/20` &lt;dbl&gt;, `2/10/20` &lt;dbl&gt;, `2/11/20` &lt;dbl&gt;, `2/12/20` &lt;dbl&gt;,\n#   `2/13/20` &lt;dbl&gt;, `2/14/20` &lt;dbl&gt;, `2/15/20` &lt;dbl&gt;, `2/16/20` &lt;dbl&gt;, …\n\n\nWe can potentially join these data to gapminder, and it would be beneficial to have one column name in common between both files. We can rename the Country/Region column of our new data frame to match gapminder.\n\ncovid &lt;- read_csv(\"raw_data/time_series_covid19_confirmed_global.csv\") %&gt;% \n  rename(country = `Country/Region`) \n\nRows: 289 Columns: 1147\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr    (2): Province/State, Country/Region\ndbl (1145): Lat, Long, 1/22/20, 1/23/20, 1/24/20, 1/25/20, 1/26/20, 1/27/20,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncovid\n\n# A tibble: 289 × 1,147\n   `Province/State` country   Lat   Long `1/22/20` `1/23/20` `1/24/20` `1/25/20`\n   &lt;chr&gt;            &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 &lt;NA&gt;             Afghan…  33.9  67.7          0         0         0         0\n 2 &lt;NA&gt;             Albania  41.2  20.2          0         0         0         0\n 3 &lt;NA&gt;             Algeria  28.0   1.66         0         0         0         0\n 4 &lt;NA&gt;             Andorra  42.5   1.52         0         0         0         0\n 5 &lt;NA&gt;             Angola  -11.2  17.9          0         0         0         0\n 6 &lt;NA&gt;             Antarc… -71.9  23.3          0         0         0         0\n 7 &lt;NA&gt;             Antigu…  17.1 -61.8          0         0         0         0\n 8 &lt;NA&gt;             Argent… -38.4 -63.6          0         0         0         0\n 9 &lt;NA&gt;             Armenia  40.1  45.0          0         0         0         0\n10 Australian Capi… Austra… -35.5 149.           0         0         0         0\n# ℹ 279 more rows\n# ℹ 1,139 more variables: `1/26/20` &lt;dbl&gt;, `1/27/20` &lt;dbl&gt;, `1/28/20` &lt;dbl&gt;,\n#   `1/29/20` &lt;dbl&gt;, `1/30/20` &lt;dbl&gt;, `1/31/20` &lt;dbl&gt;, `2/1/20` &lt;dbl&gt;,\n#   `2/2/20` &lt;dbl&gt;, `2/3/20` &lt;dbl&gt;, `2/4/20` &lt;dbl&gt;, `2/5/20` &lt;dbl&gt;,\n#   `2/6/20` &lt;dbl&gt;, `2/7/20` &lt;dbl&gt;, `2/8/20` &lt;dbl&gt;, `2/9/20` &lt;dbl&gt;,\n#   `2/10/20` &lt;dbl&gt;, `2/11/20` &lt;dbl&gt;, `2/12/20` &lt;dbl&gt;, `2/13/20` &lt;dbl&gt;,\n#   `2/14/20` &lt;dbl&gt;, `2/15/20` &lt;dbl&gt;, `2/16/20` &lt;dbl&gt;, `2/17/20` &lt;dbl&gt;, …\n\n\nMuch of the analysis of this dataset has looked at trends over time (e.g. increasing /decreasing case numbers, comparing trajectories). As we know by now, the ggplot2 package allows us to map columns (variables) in our dataset to aspects of the plot.\nIn other words, we would expect to create plots by writing code such as:-\nggplot(covid, aes(x = Date, y =...)) + ...\nUnfortunately such plots are not possible with the data in it’s current format. Counts for each date are containing in a different column. What we require is a column to indicate the date, and the corresponding count in the next column. Such data arrangements are known as long data; whereas we have wide data. Fortunately we can convert between the two using the tidyr package (also part of tidyverse).\n\n## install tidyr if you don't already have it\ninstall.packages(\"tidyr\")\n\n\nFor more information on tidy data, and how to convert between long and wide data, see\nhttps://r4ds.had.co.nz/tidy-data.html\n\n\nlibrary(tidyr)\ncovid &lt;- read_csv(\"raw_data/time_series_covid19_confirmed_global.csv\") %&gt;% \n  rename(country = `Country/Region`) %&gt;% \n  pivot_longer(5:last_col(),names_to=\"Date\", values_to=\"Cases\")\n\nRows: 289 Columns: 1147\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr    (2): Province/State, Country/Region\ndbl (1145): Lat, Long, 1/22/20, 1/23/20, 1/24/20, 1/25/20, 1/26/20, 1/27/20,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncovid\n\n# A tibble: 330,327 × 6\n   `Province/State` country       Lat  Long Date    Cases\n   &lt;chr&gt;            &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1 &lt;NA&gt;             Afghanistan  33.9  67.7 1/22/20     0\n 2 &lt;NA&gt;             Afghanistan  33.9  67.7 1/23/20     0\n 3 &lt;NA&gt;             Afghanistan  33.9  67.7 1/24/20     0\n 4 &lt;NA&gt;             Afghanistan  33.9  67.7 1/25/20     0\n 5 &lt;NA&gt;             Afghanistan  33.9  67.7 1/26/20     0\n 6 &lt;NA&gt;             Afghanistan  33.9  67.7 1/27/20     0\n 7 &lt;NA&gt;             Afghanistan  33.9  67.7 1/28/20     0\n 8 &lt;NA&gt;             Afghanistan  33.9  67.7 1/29/20     0\n 9 &lt;NA&gt;             Afghanistan  33.9  67.7 1/30/20     0\n10 &lt;NA&gt;             Afghanistan  33.9  67.7 1/31/20     0\n# ℹ 330,317 more rows\n\n\nAnother point to note is that the dates are not in an internationally recognised format, which could cause a problem for some visualisations that rely on date order. We can fix by explicitly converting to YYYY-MM-DD format.\n\nFor more ways of dealing with dates in R see the lubridate package.\n\n\ncovid &lt;- read_csv(\"raw_data/time_series_covid19_confirmed_global.csv\") %&gt;% \n  rename(country = `Country/Region`) %&gt;% \n  pivot_longer(5:last_col(),names_to=\"Date\", values_to=\"Cases\") %&gt;% \n    mutate(Date=as.Date(Date,\"%m/%d/%y\"))\n\nRows: 289 Columns: 1147\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr    (2): Province/State, Country/Region\ndbl (1145): Lat, Long, 1/22/20, 1/23/20, 1/24/20, 1/25/20, 1/26/20, 1/27/20,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncovid\n\n# A tibble: 330,327 × 6\n   `Province/State` country       Lat  Long Date       Cases\n   &lt;chr&gt;            &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;\n 1 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-22     0\n 2 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-23     0\n 3 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-24     0\n 4 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-25     0\n 5 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-26     0\n 6 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-27     0\n 7 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-28     0\n 8 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-29     0\n 9 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-30     0\n10 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-31     0\n# ℹ 330,317 more rows\n\n\nAnother useful modification is to make sure only one row exists for each country. If we look at the data for some countries (e.g. China and UK) there are different entries for provinces and oversees territories.\n\n## the count function tabulates the number of observations in a particular column\n\nfilter(covid, country == \"China\") %&gt;% \n  count(`Province/State`)\n\n# A tibble: 34 × 2\n   `Province/State`     n\n   &lt;chr&gt;            &lt;int&gt;\n 1 Anhui             1143\n 2 Beijing           1143\n 3 Chongqing         1143\n 4 Fujian            1143\n 5 Gansu             1143\n 6 Guangdong         1143\n 7 Guangxi           1143\n 8 Guizhou           1143\n 9 Hainan            1143\n10 Hebei             1143\n# ℹ 24 more rows\n\n\nSo we can change the Cases to be the sum of all cases for that country on a particular day. We can do this using the group_by and summarise functions from above\n\ncovid &lt;- read_csv(\"raw_data/time_series_covid19_confirmed_global.csv\") %&gt;% \n  rename(country = `Country/Region`) %&gt;% \n  pivot_longer(5:last_col(),names_to=\"Date\", values_to=\"Cases\") %&gt;% \n  mutate(Date=as.Date(Date,\"%m/%d/%y\")) %&gt;% \n  group_by(country,Date) %&gt;% \n  summarise(Cases = sum(Cases))\n\nRows: 289 Columns: 1147\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr    (2): Province/State, Country/Region\ndbl (1145): Lat, Long, 1/22/20, 1/23/20, 1/24/20, 1/25/20, 1/26/20, 1/27/20,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n`summarise()` has grouped output by 'country'. You can override using the `.groups` argument.\n\ncovid\n\n# A tibble: 229,743 × 3\n# Groups:   country [201]\n   country     Date       Cases\n   &lt;chr&gt;       &lt;date&gt;     &lt;dbl&gt;\n 1 Afghanistan 2020-01-22     0\n 2 Afghanistan 2020-01-23     0\n 3 Afghanistan 2020-01-24     0\n 4 Afghanistan 2020-01-25     0\n 5 Afghanistan 2020-01-26     0\n 6 Afghanistan 2020-01-27     0\n 7 Afghanistan 2020-01-28     0\n 8 Afghanistan 2020-01-29     0\n 9 Afghanistan 2020-01-30     0\n10 Afghanistan 2020-01-31     0\n# ℹ 229,733 more rows\n\n\n\nExercise\n\nWhat plots and summaries can you make from these data?\n\nPlotting the number of cases over time for certain countries\nWhich country in each continent currently has the highest number of cases?\nNormalise the number of cases for population size (using 2007 population figures as a population estimate)?\n\ne.g. cases per 100,000\n\nWhich European countries have the highest number of cases per 100,000 population\n\ne.g. https://www.statista.com/statistics/1110187/coronavirus-incidence-europe-by-country/"
  },
  {
    "objectID": "training/r_part2/index.html#bonus-exercise",
    "href": "training/r_part2/index.html#bonus-exercise",
    "title": "Introduction to R - Part 2",
    "section": "Bonus Exercise",
    "text": "Bonus Exercise\nThese are a bit more challenging, but please feel free to have a go\n\nUsing the filter function, find all countries that start with the letter Z\n\nHint: You can find the first letter of each country using the substr function. The mutate function can then be used to add a new column to the data.\n\nUse geom_tile to create a heatmap visualising life expectancy over time for European countries. You will need to work out what aesthetics to specify for a geom_tile plot\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nPart 1\n\n## month.name is a built-in vector of the months of the year\nmonth.name\n\n\n## substr can be used to extract substrings from a character vector betwen a start and end position\n## e.g. print the first three letters of each moth\n\n\nsubstr(month.name, 1, 3)\n\n# Using mutate, add an extra column; the first letter of each country name. \n\ngapminder2 &lt;- mutate(gapminder, FirstLetter = substr(country, 1,1))\n\n# Now filter using the new FirstLetter column\n\ngapminder3 &lt;- filter(gapminder2, FirstLetter == \"Z\")\ngapminder3\n\n\n## Get the European countries\nfilter(gapminder, continent == \"Europe\") %&gt;% \n## make heatmap. See the fill aesthetic to be life expectancy\nggplot(aes(x=year,y=country,fill=lifeExp)) + geom_tile()"
  },
  {
    "objectID": "training/r_part2/index.html#wrap-up",
    "href": "training/r_part2/index.html#wrap-up",
    "title": "Introduction to R - Part 2",
    "section": "Wrap-up",
    "text": "Wrap-up\nWe have covered a lot about manipulating and visualising data, but have only just scratched the surface. In the next part we will conclude with\n\nChoosing colour palettes\n(Some ways to) customise our plots\nAutomatically plot different subsets / categories in our data using “faceting”\nProducing summary statistics from our data, and for different subsets / categories\nJoining two data frames\n(Briefly) how to clean “messy” data"
  },
  {
    "objectID": "training/r_part3/index.html#data-cleaning-a-covid-19-data-example",
    "href": "training/r_part3/index.html#data-cleaning-a-covid-19-data-example",
    "title": "Introduction to R - Part 3",
    "section": "Data Cleaning: A COVID-19 data example",
    "text": "Data Cleaning: A COVID-19 data example\nData for global COVID-19 cases are available online from CSSE at Johns Hopkins University on their github repository.\n\n\n\n\n\n\nNote\n\n\n\ngithub is an excellent way of making your code and analysis available for others to reuse and share. Private repositories with restricted access are also available. Here is a useful beginners guide.\n-Friendly github intro\n\n\nR is capable of downloading files to our own machine so we can analyse them. We need to know the URL (for the COVID data we can find this from github, or use the address below) and can specify what to call the file when it is downloaded.\n\nif(!file.exists(\"raw_data/time_series_covid19_confirmed_global.csv\")){\n  download.file(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\",destfile = \"raw_data/time_series_covid19_confirmed_global.csv\")\n}\n\nWe can use the read_csv function as before to import the data and take a look. We can see the basic structure of the data is one row for each country / region and columns for cases on each day.\n\ncovid &lt;- read_csv(\"raw_data/time_series_covid19_confirmed_global.csv\")\n\nRows: 289 Columns: 1147\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr    (2): Province/State, Country/Region\ndbl (1145): Lat, Long, 1/22/20, 1/23/20, 1/24/20, 1/25/20, 1/26/20, 1/27/20,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncovid\n\n# A tibble: 289 × 1,147\n   `Province/State`  `Country/Region`   Lat   Long `1/22/20` `1/23/20` `1/24/20`\n   &lt;chr&gt;             &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 &lt;NA&gt;              Afghanistan       33.9  67.7          0         0         0\n 2 &lt;NA&gt;              Albania           41.2  20.2          0         0         0\n 3 &lt;NA&gt;              Algeria           28.0   1.66         0         0         0\n 4 &lt;NA&gt;              Andorra           42.5   1.52         0         0         0\n 5 &lt;NA&gt;              Angola           -11.2  17.9          0         0         0\n 6 &lt;NA&gt;              Antarctica       -71.9  23.3          0         0         0\n 7 &lt;NA&gt;              Antigua and Bar…  17.1 -61.8          0         0         0\n 8 &lt;NA&gt;              Argentina        -38.4 -63.6          0         0         0\n 9 &lt;NA&gt;              Armenia           40.1  45.0          0         0         0\n10 Australian Capit… Australia        -35.5 149.           0         0         0\n# ℹ 279 more rows\n# ℹ 1,140 more variables: `1/25/20` &lt;dbl&gt;, `1/26/20` &lt;dbl&gt;, `1/27/20` &lt;dbl&gt;,\n#   `1/28/20` &lt;dbl&gt;, `1/29/20` &lt;dbl&gt;, `1/30/20` &lt;dbl&gt;, `1/31/20` &lt;dbl&gt;,\n#   `2/1/20` &lt;dbl&gt;, `2/2/20` &lt;dbl&gt;, `2/3/20` &lt;dbl&gt;, `2/4/20` &lt;dbl&gt;,\n#   `2/5/20` &lt;dbl&gt;, `2/6/20` &lt;dbl&gt;, `2/7/20` &lt;dbl&gt;, `2/8/20` &lt;dbl&gt;,\n#   `2/9/20` &lt;dbl&gt;, `2/10/20` &lt;dbl&gt;, `2/11/20` &lt;dbl&gt;, `2/12/20` &lt;dbl&gt;,\n#   `2/13/20` &lt;dbl&gt;, `2/14/20` &lt;dbl&gt;, `2/15/20` &lt;dbl&gt;, `2/16/20` &lt;dbl&gt;, …\n\n\nMuch of the analysis of this dataset has looked at trends over time (e.g. increasing /decreasing case numbers, comparing trajectories). As we know by now, the ggplot2 package allows us to map columns (variables) in our dataset to aspects of the plot.\nIn other words, we would expect to create plots by writing code such as:-\nggplot(covid, aes(x = Date, y =...)) + ...\nUnfortunately such plots are not possible with the data in it’s current format. Counts for each date are containing in a different column. What we require is a column to indicate the date, and the corresponding count in the next column. Such data arrangements are known as long data; whereas we have wide data. Fortunately we can convert between the two using the tidyr package (also part of tidyverse).\n\n## install tidyr if you don't already have it\ninstall.packages(\"tidyr\")\n\n\n\n\n\n\n\nAbout “tidy data”\n\n\n\nFor more information on tidy data, and how to convert between long and wide data, see\nhttps://r4ds.had.co.nz/tidy-data.html\n\n\nFor convenience we will also rename the column containing country names\n\n## set the show_col_types argument to FALSE to suppress message about column types\n\nlibrary(tidyr)\ncovid &lt;- read_csv(\"raw_data/time_series_covid19_confirmed_global.csv\",show_col_types = FALSE) %&gt;% \n    rename(country = `Country/Region`) %&gt;% \n  pivot_longer(5:last_col(),names_to=\"Date\", values_to=\"Cases\")\ncovid\n\n# A tibble: 330,327 × 6\n   `Province/State` country       Lat  Long Date    Cases\n   &lt;chr&gt;            &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1 &lt;NA&gt;             Afghanistan  33.9  67.7 1/22/20     0\n 2 &lt;NA&gt;             Afghanistan  33.9  67.7 1/23/20     0\n 3 &lt;NA&gt;             Afghanistan  33.9  67.7 1/24/20     0\n 4 &lt;NA&gt;             Afghanistan  33.9  67.7 1/25/20     0\n 5 &lt;NA&gt;             Afghanistan  33.9  67.7 1/26/20     0\n 6 &lt;NA&gt;             Afghanistan  33.9  67.7 1/27/20     0\n 7 &lt;NA&gt;             Afghanistan  33.9  67.7 1/28/20     0\n 8 &lt;NA&gt;             Afghanistan  33.9  67.7 1/29/20     0\n 9 &lt;NA&gt;             Afghanistan  33.9  67.7 1/30/20     0\n10 &lt;NA&gt;             Afghanistan  33.9  67.7 1/31/20     0\n# ℹ 330,317 more rows\n\n\nThe number of rows and columns has changed dramatically, but this is a much more usable form for dplyr and ggplot2.\nAnother point to note is that the dates are not in an internationally recognised format, which could cause a problem for some visualisations that rely on date order.\n\nWe can fix by explicitly converting to YYYY-MM-DD format. The as.Date function can be used to convert an existing column into standardised dates. It needs to know how the months, days and years are being specified which might look a bit obtuse. The specification needed for these data is %m/%d/%y. This means months(%m) separated by a / followed by a day (%d) followed by another / followed by the year represented by two digits (%y). Other conversions are possible including if you have dates with month names (Jan, Feb…) or four digit years. See the link below for more information.\n\n\n\n\n\n\nDealing with dates\n\n\n\nSee this website for more about representing and converting dates in R.\n\nhttps://www.statology.org/r-date-format/\n\nFor more ways of dealing with dates in R see the lubridate package which can handle tasks such as calculating intervals between dates and much more\n\n\n\ncovid &lt;- read_csv(\"raw_data/time_series_covid19_confirmed_global.csv\",show_col_types = FALSE) %&gt;% \n    rename(country = `Country/Region`) %&gt;% \n  pivot_longer(5:last_col(),names_to=\"Date\", values_to=\"Cases\") %&gt;% \n  mutate(Date=as.Date(Date,\"%m/%d/%y\"))\ncovid\n\n# A tibble: 330,327 × 6\n   `Province/State` country       Lat  Long Date       Cases\n   &lt;chr&gt;            &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;\n 1 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-22     0\n 2 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-23     0\n 3 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-24     0\n 4 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-25     0\n 5 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-26     0\n 6 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-27     0\n 7 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-28     0\n 8 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-29     0\n 9 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-30     0\n10 &lt;NA&gt;             Afghanistan  33.9  67.7 2020-01-31     0\n# ℹ 330,317 more rows\n\n\nAnother useful modification is to make sure only one row exists for each country. If we look at the data for some countries (e.g. China and UK) there are different entries for provinces and oversees territories. So we can change the Cases to be the sum of all cases for that country on a particular day. We can do this using the group_by and summarise functions from above.\n\ncovid &lt;- read_csv(\"raw_data/time_series_covid19_confirmed_global.csv\", show_col_types = FALSE) %&gt;% \n  rename(country = `Country/Region`) %&gt;% \n  pivot_longer(5:last_col(),names_to=\"Date\", values_to=\"Cases\") %&gt;% \n  mutate(Date=as.Date(Date,\"%m/%d/%y\")) %&gt;% \n  group_by(country,Date) %&gt;% \n  summarise(Cases = sum(Cases))\n\n`summarise()` has grouped output by 'country'. You can override using the\n`.groups` argument.\n\ncovid\n\n# A tibble: 229,743 × 3\n# Groups:   country [201]\n   country     Date       Cases\n   &lt;chr&gt;       &lt;date&gt;     &lt;dbl&gt;\n 1 Afghanistan 2020-01-22     0\n 2 Afghanistan 2020-01-23     0\n 3 Afghanistan 2020-01-24     0\n 4 Afghanistan 2020-01-25     0\n 5 Afghanistan 2020-01-26     0\n 6 Afghanistan 2020-01-27     0\n 7 Afghanistan 2020-01-28     0\n 8 Afghanistan 2020-01-29     0\n 9 Afghanistan 2020-01-30     0\n10 Afghanistan 2020-01-31     0\n# ℹ 229,733 more rows\n\n\n\ncovid\n\n# A tibble: 229,743 × 3\n# Groups:   country [201]\n   country     Date       Cases\n   &lt;chr&gt;       &lt;date&gt;     &lt;dbl&gt;\n 1 Afghanistan 2020-01-22     0\n 2 Afghanistan 2020-01-23     0\n 3 Afghanistan 2020-01-24     0\n 4 Afghanistan 2020-01-25     0\n 5 Afghanistan 2020-01-26     0\n 6 Afghanistan 2020-01-27     0\n 7 Afghanistan 2020-01-28     0\n 8 Afghanistan 2020-01-29     0\n 9 Afghanistan 2020-01-30     0\n10 Afghanistan 2020-01-31     0\n# ℹ 229,733 more rows\n\n\nSince we previously renamed the country column in covid and both data frames now have a column called country we can use a left_join.\n\nleft_join(gapminder, covid)\n\nJoining with `by = join_by(country)`\n\n\nWarning in left_join(gapminder, covid): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n# A tibble: 1,755,816 × 8\n   country     continent  year lifeExp     pop gdpPercap Date       Cases\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8 8425333      779. 2020-01-22     0\n 2 Afghanistan Asia       1952    28.8 8425333      779. 2020-01-23     0\n 3 Afghanistan Asia       1952    28.8 8425333      779. 2020-01-24     0\n 4 Afghanistan Asia       1952    28.8 8425333      779. 2020-01-25     0\n 5 Afghanistan Asia       1952    28.8 8425333      779. 2020-01-26     0\n 6 Afghanistan Asia       1952    28.8 8425333      779. 2020-01-27     0\n 7 Afghanistan Asia       1952    28.8 8425333      779. 2020-01-28     0\n 8 Afghanistan Asia       1952    28.8 8425333      779. 2020-01-29     0\n 9 Afghanistan Asia       1952    28.8 8425333      779. 2020-01-30     0\n10 Afghanistan Asia       1952    28.8 8425333      779. 2020-01-31     0\n# ℹ 1,755,806 more rows\n\n\n\nIf we hadn’t used rename previously, the joining code would look like this\n\nleft_join(gapminder, covid, by = c(\"county\" = \"Country/Region\"))\n\n\nFurthermore, we might also want to just use the 2007 rows from gapminder.\n\nfilter(gapminder, year == 2007) %&gt;% \n  left_join(covid)\n\nJoining with `by = join_by(country)`\n\n\n# A tibble: 146,318 × 8\n   country     continent  year lifeExp      pop gdpPercap Date       Cases\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       2007    43.8 31889923      975. 2020-01-22     0\n 2 Afghanistan Asia       2007    43.8 31889923      975. 2020-01-23     0\n 3 Afghanistan Asia       2007    43.8 31889923      975. 2020-01-24     0\n 4 Afghanistan Asia       2007    43.8 31889923      975. 2020-01-25     0\n 5 Afghanistan Asia       2007    43.8 31889923      975. 2020-01-26     0\n 6 Afghanistan Asia       2007    43.8 31889923      975. 2020-01-27     0\n 7 Afghanistan Asia       2007    43.8 31889923      975. 2020-01-28     0\n 8 Afghanistan Asia       2007    43.8 31889923      975. 2020-01-29     0\n 9 Afghanistan Asia       2007    43.8 31889923      975. 2020-01-30     0\n10 Afghanistan Asia       2007    43.8 31889923      975. 2020-01-31     0\n# ℹ 146,308 more rows\n\n\n\nExercise\n\nWhat plots and summaries can you make from these data?\n\nPlotting the number of cases over time for certain countries\nWhich country in each continent currently has the highest number of cases?\nNormalise the number of cases for population size (using 2007 population figures as a population estimate)?\n\ne.g. cases per 100,000\n\nWhich European countries have the highest number of cases per 100,000 population\n\ne.g. https://www.statista.com/statistics/1110187/coronavirus-incidence-europe-by-country/\n\n\n\n\n\n\n\n\n\nSome solutions\n\n\n\n\n\nCompare trajectories of different countries. The %in% operator is an alternative to or (|) to find a country name that can have a number of possibilities.\n\ncovid &lt;- read_csv(\"raw_data/time_series_covid19_confirmed_global.csv\") %&gt;% \n  rename(country = `Country/Region`) %&gt;% \n  pivot_longer(5:last_col(),names_to=\"Date\", values_to=\"Cases\") %&gt;% \n  mutate(Date=as.Date(Date,\"%m/%d/%y\")) %&gt;% \n  group_by(country,Date) %&gt;% \n  summarise(Cases = sum(Cases))\n\nRows: 289 Columns: 1147\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr    (2): Province/State, Country/Region\ndbl (1145): Lat, Long, 1/22/20, 1/23/20, 1/24/20, 1/25/20, 1/26/20, 1/27/20,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n`summarise()` has grouped output by 'country'. You can override using the `.groups` argument.\n\nfilter(covid, country %in% c(\"United Kingdom\",\"France\",\"Spain\")) %&gt;%\n  ggplot(aes(x = Date, y = Cases,col=country)) + geom_line()\n\n\n\n\n\n\n\n\nTo explore the european data on a particular date, we first filter gapminder appropriately\n\nfilter(gapminder, year == 2007,continent==\"Europe\") %&gt;%\n  left_join(covid) %&gt;% \n  filter(Date == \"2023-01-13\") %&gt;% \n  mutate(Cases = round(Cases / (pop / 1e5)))\n\nJoining with `by = join_by(country)`\n\n\n# A tibble: 28 × 8\n   country             continent  year lifeExp    pop gdpPercap Date       Cases\n   &lt;chr&gt;               &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;\n 1 Albania             Europe     2007    76.4 3.60e6     5937. 2023-01-13  9277\n 2 Austria             Europe     2007    79.8 8.20e6    36126. 2023-01-13 69987\n 3 Belgium             Europe     2007    79.4 1.04e7    33693. 2023-01-13 45093\n 4 Bosnia and Herzego… Europe     2007    74.9 4.55e6     7446. 2023-01-13  8813\n 5 Bulgaria            Europe     2007    73.0 7.32e6    10681. 2023-01-13 17672\n 6 Croatia             Europe     2007    75.7 4.49e6    14619. 2023-01-13 28181\n 7 Denmark             Europe     2007    78.3 5.47e6    35278. 2023-01-13 62970\n 8 Finland             Europe     2007    79.3 5.24e6    33207. 2023-01-13 27654\n 9 France              Europe     2007    80.7 6.11e7    30470. 2023-01-13 64906\n10 Germany             Europe     2007    79.4 8.24e7    32170. 2023-01-13 45637\n# ℹ 18 more rows\n\n\nTry to make example similar to https://www.statista.com/statistics/1110187/coronavirus-incidence-europe-by-country/. We’re not using up-to-date figures for population so there will be some differences. The two data frames can be joined because they have a column name in common (country). Most of the country names that appear in gapminder also appear in the covid dataset, so not much data is lost in the join.\nSince we are interested in European countries for this dataset, we pre-filter the gapminder data. Also, we only need the population values from 2007.\n\nfilter(gapminder, year == 2007,continent==\"Europe\") %&gt;% \n  left_join(covid) %&gt;% \n  filter(Date == \"2022-01-06\") %&gt;% \n  mutate(Cases = round(Cases / (pop / 1e5))) %&gt;% \n  ggplot(aes(x = Cases, y = country)) + geom_col()\n\nJoining with `by = join_by(country)`\n\n\n\n\n\n\n\n\n\nThe default ordering for the bars is alphabetical, which is probably not a natural choice. The forcats package, which is part of tidyverse allows factors in a data frame to be re-ordered and re-labeled.\n\nforcats package\n\nIn particular, we can use the fct_reorder function to reorder the country names according to the number of cases. This can be done inside the ggplot2 function itself.\n\n## this will install the package if it is not already installed\nif(!require(forcats)) install.packages(forcats)\n\nLoading required package: forcats\n\nfilter(gapminder, year == 2007,continent==\"Europe\") %&gt;% \n  left_join(covid) %&gt;% \n  filter(Date == \"2023-01-13\") %&gt;% \n  mutate(Cases = round(Cases / (pop / 1e5))) %&gt;% \n  ggplot(aes(x = Cases, y = forcats::fct_reorder(country,Cases))) + geom_col()\n\nJoining with `by = join_by(country)`\n\n\n\n\n\n\n\n\n\nA heatmap of number of cases over time (similar to that reported by the BBC) can be achieved using a geom_tile\n\n### Get the 2007 gapminder data to avoid repeating data\nfilter(gapminder, year ==  2007, continent==\"Europe\") %&gt;% \n  left_join(covid) %&gt;% \n  filter(!is.na(Cases)) %&gt;% ## remove countries with missing values%&gt;% \n  mutate(Cases = round(Cases / (pop / 1e5))) %&gt;% \n  ggplot(aes(x = Date, y = country,fill=Cases)) + geom_tile() + scale_fill_viridis_c()\n\nJoining with `by = join_by(country)`"
  },
  {
    "objectID": "posts/2025_10_16_check_data/index.html",
    "href": "posts/2025_10_16_check_data/index.html",
    "title": "Always check your input data",
    "section": "",
    "text": "Why you always check your input data before starting"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "You will find some comprehensive training materials on this site, and here I will add some shorter musings on other aspects of data analysis and Bioinformatics that I have found useful\n\n\n\n\n\n\n\n\n\n\n\n\nAlways check your input data\n\n\n\n\n\n\nMark Dunning\n\n\nOct 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nReading 10X Spatial Data from GEO into R\n\n\n\n\n\n\nMark Dunning\n\n\nOct 24, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025_10_16_check_data/index.html#overview-of-the-code",
    "href": "posts/2025_10_16_check_data/index.html#overview-of-the-code",
    "title": "Always check your input data",
    "section": "Overview of the code",
    "text": "Overview of the code\n\n\nWarning in dir.create(\"raw_data\"): 'raw_data' already exists\n\n\nThe code chunk below takes a time series dataset of covid cases worldwide and applies some essential data cleaning transformations. These ensure that the data are in a “tidy” format expected by ggplot2 and convert the dates into an international standards. Furthermore, some countries are represented by multiple regions and for simplicity we add these case numbers together.\n\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(tidyr)\n\ncovid &lt;- read_csv(\"raw_data/time_series_covid19_confirmed_global.csv\") %&gt;% \n  rename(country = `Country/Region`) %&gt;% \n  pivot_longer(5:last_col(),names_to=\"Date\", values_to=\"Cases\") %&gt;% \n  mutate(Date=as.Date(Date,\"%m/%d/%y\")) %&gt;% \n  group_by(country,Date) %&gt;% \n  summarise(Cases = sum(Cases))\n\nRows: 289 Columns: 1147\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr    (2): Province/State, Country/Region\ndbl (1145): Lat, Long, 1/22/20, 1/23/20, 1/24/20, 1/25/20, 1/26/20, 1/27/20,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n`summarise()` has grouped output by 'country'. You can override using the `.groups` argument.\n\nhead(covid)\n\n# A tibble: 6 × 3\n# Groups:   country [1]\n  country     Date       Cases\n  &lt;chr&gt;       &lt;date&gt;     &lt;dbl&gt;\n1 Afghanistan 2020-01-22     0\n2 Afghanistan 2020-01-23     0\n3 Afghanistan 2020-01-24     0\n4 Afghanistan 2020-01-25     0\n5 Afghanistan 2020-01-26     0\n6 Afghanistan 2020-01-27     0\n\n\nThe plot is now a standard application of the ggplot function\n\nfilter(covid, country %in% c(\"United Kingdom\",\"France\",\"Spain\")) %&gt;%\n  ggplot(aes(x = Date, y = Cases,col=country)) + geom_line()\n\n\n\n\n\n\n\n\nBut the plot is now looking as expected - with United Kingdom showing high numbers of cases. So what could have happened to produce the top of the page? I neglected to explain that the source data come from a github page are were downloaded as part of my code. In the training materials this was intended to show a workflow that started from data located at a remote source. The code below first creates a raw_data folder (without complaining if such a folder already exists - showWarnings=FALSE) and then checks via file.exists if time_series_covid19_confirmed_global.csv is already present. If not, the code will download from github.\n\ndir.create(\"raw_data\", showWarnings = FALSE)\nif(!file.exists(\"raw_data/time_series_covid19_confirmed_global.csv\")){\n  download.file(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\",destfile = \"raw_data/time_series_covid19_confirmed_global.csv\")\n}\n\nWhat should happen, and missing from the workshop materials, is to perform some basic checks on the dimensions of the data once imported into R and stored as a tibble. 🤦\n\ncovid &lt;- read_csv(\"raw_data/time_series_covid19_confirmed_global.csv\")\n\nRows: 289 Columns: 1147\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr    (2): Province/State, Country/Region\ndbl (1145): Lat, Long, 1/22/20, 1/23/20, 1/24/20, 1/25/20, 1/26/20, 1/27/20,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nChecking the dimensions will print the number of rows and columns\n\ndim(covid)\n\n[1]  289 1147\n\n\nThe head function is a classic function for printing the first six rows (adjustable using the n argument)\n\nhead(covid)\n\n# A tibble: 6 × 1,147\n  `Province/State` `Country/Region`   Lat  Long `1/22/20` `1/23/20` `1/24/20`\n  &lt;chr&gt;            &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 &lt;NA&gt;             Afghanistan       33.9 67.7          0         0         0\n2 &lt;NA&gt;             Albania           41.2 20.2          0         0         0\n3 &lt;NA&gt;             Algeria           28.0  1.66         0         0         0\n4 &lt;NA&gt;             Andorra           42.5  1.52         0         0         0\n5 &lt;NA&gt;             Angola           -11.2 17.9          0         0         0\n6 &lt;NA&gt;             Antarctica       -71.9 23.3          0         0         0\n# ℹ 1,140 more variables: `1/25/20` &lt;dbl&gt;, `1/26/20` &lt;dbl&gt;, `1/27/20` &lt;dbl&gt;,\n#   `1/28/20` &lt;dbl&gt;, `1/29/20` &lt;dbl&gt;, `1/30/20` &lt;dbl&gt;, `1/31/20` &lt;dbl&gt;,\n#   `2/1/20` &lt;dbl&gt;, `2/2/20` &lt;dbl&gt;, `2/3/20` &lt;dbl&gt;, `2/4/20` &lt;dbl&gt;,\n#   `2/5/20` &lt;dbl&gt;, `2/6/20` &lt;dbl&gt;, `2/7/20` &lt;dbl&gt;, `2/8/20` &lt;dbl&gt;,\n#   `2/9/20` &lt;dbl&gt;, `2/10/20` &lt;dbl&gt;, `2/11/20` &lt;dbl&gt;, `2/12/20` &lt;dbl&gt;,\n#   `2/13/20` &lt;dbl&gt;, `2/14/20` &lt;dbl&gt;, `2/15/20` &lt;dbl&gt;, `2/16/20` &lt;dbl&gt;,\n#   `2/17/20` &lt;dbl&gt;, `2/18/20` &lt;dbl&gt;, `2/19/20` &lt;dbl&gt;, `2/20/20` &lt;dbl&gt;, …\n\n\nSimilarly, tail will show the last rows in the data\n\ntail(covid)\n\n# A tibble: 6 × 1,147\n  `Province/State` `Country/Region`      Lat  Long `1/22/20` `1/23/20` `1/24/20`\n  &lt;chr&gt;            &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 &lt;NA&gt;             Vietnam              14.1 108.          0         2         2\n2 &lt;NA&gt;             West Bank and Gaza   32.0  35.2         0         0         0\n3 &lt;NA&gt;             Winter Olympics 20…  39.9 116.          0         0         0\n4 &lt;NA&gt;             Yemen                15.6  48.5         0         0         0\n5 &lt;NA&gt;             Zambia              -13.1  27.8         0         0         0\n6 &lt;NA&gt;             Zimbabwe            -19.0  29.2         0         0         0\n# ℹ 1,140 more variables: `1/25/20` &lt;dbl&gt;, `1/26/20` &lt;dbl&gt;, `1/27/20` &lt;dbl&gt;,\n#   `1/28/20` &lt;dbl&gt;, `1/29/20` &lt;dbl&gt;, `1/30/20` &lt;dbl&gt;, `1/31/20` &lt;dbl&gt;,\n#   `2/1/20` &lt;dbl&gt;, `2/2/20` &lt;dbl&gt;, `2/3/20` &lt;dbl&gt;, `2/4/20` &lt;dbl&gt;,\n#   `2/5/20` &lt;dbl&gt;, `2/6/20` &lt;dbl&gt;, `2/7/20` &lt;dbl&gt;, `2/8/20` &lt;dbl&gt;,\n#   `2/9/20` &lt;dbl&gt;, `2/10/20` &lt;dbl&gt;, `2/11/20` &lt;dbl&gt;, `2/12/20` &lt;dbl&gt;,\n#   `2/13/20` &lt;dbl&gt;, `2/14/20` &lt;dbl&gt;, `2/15/20` &lt;dbl&gt;, `2/16/20` &lt;dbl&gt;,\n#   `2/17/20` &lt;dbl&gt;, `2/18/20` &lt;dbl&gt;, `2/19/20` &lt;dbl&gt;, `2/20/20` &lt;dbl&gt;, …\n\n\nI saved a copy of the csv file that was downloaded during the same session that created the erroneous covid line plot.\n\ncovid_bad &lt;- read_csv(\"time_series_covid19_confirmed_global_BAD.csv\")\n\nRows: 270 Columns: 1147\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr    (2): Province/State, Country/Region\ndbl (1145): Lat, Long, 1/22/20, 1/23/20, 1/24/20, 1/25/20, 1/26/20, 1/27/20,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe first rows of the tibble look to be the same\n\nhead(covid)\n\n# A tibble: 6 × 1,147\n  `Province/State` `Country/Region`   Lat  Long `1/22/20` `1/23/20` `1/24/20`\n  &lt;chr&gt;            &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 &lt;NA&gt;             Afghanistan       33.9 67.7          0         0         0\n2 &lt;NA&gt;             Albania           41.2 20.2          0         0         0\n3 &lt;NA&gt;             Algeria           28.0  1.66         0         0         0\n4 &lt;NA&gt;             Andorra           42.5  1.52         0         0         0\n5 &lt;NA&gt;             Angola           -11.2 17.9          0         0         0\n6 &lt;NA&gt;             Antarctica       -71.9 23.3          0         0         0\n# ℹ 1,140 more variables: `1/25/20` &lt;dbl&gt;, `1/26/20` &lt;dbl&gt;, `1/27/20` &lt;dbl&gt;,\n#   `1/28/20` &lt;dbl&gt;, `1/29/20` &lt;dbl&gt;, `1/30/20` &lt;dbl&gt;, `1/31/20` &lt;dbl&gt;,\n#   `2/1/20` &lt;dbl&gt;, `2/2/20` &lt;dbl&gt;, `2/3/20` &lt;dbl&gt;, `2/4/20` &lt;dbl&gt;,\n#   `2/5/20` &lt;dbl&gt;, `2/6/20` &lt;dbl&gt;, `2/7/20` &lt;dbl&gt;, `2/8/20` &lt;dbl&gt;,\n#   `2/9/20` &lt;dbl&gt;, `2/10/20` &lt;dbl&gt;, `2/11/20` &lt;dbl&gt;, `2/12/20` &lt;dbl&gt;,\n#   `2/13/20` &lt;dbl&gt;, `2/14/20` &lt;dbl&gt;, `2/15/20` &lt;dbl&gt;, `2/16/20` &lt;dbl&gt;,\n#   `2/17/20` &lt;dbl&gt;, `2/18/20` &lt;dbl&gt;, `2/19/20` &lt;dbl&gt;, `2/20/20` &lt;dbl&gt;, …\n\n\nBut clearly there are fewer rows\n\ndim(covid_bad)\n\n[1]  270 1147\n\n\nAnd the last rows of the tibble are not the same as the complete dataset.\n\ntail(covid_bad)\n\n# A tibble: 6 × 1,147\n  `Province/State`   `Country/Region`   Lat   Long `1/22/20` `1/23/20` `1/24/20`\n  &lt;chr&gt;              &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Anguilla           United Kingdom    18.2 -63.1          0         0         0\n2 Bermuda            United Kingdom    32.3 -64.8          0         0         0\n3 British Virgin Is… United Kingdom    18.4 -64.6          0         0         0\n4 Cayman Islands     United Kingdom    19.3 -81.3          0         0         0\n5 Channel Islands    United Kingdom    49.4  -2.36         0         0         0\n6 Falkland Islands … United Kingdom   -51.8 -59.5          0         0         0\n# ℹ 1,140 more variables: `1/25/20` &lt;dbl&gt;, `1/26/20` &lt;dbl&gt;, `1/27/20` &lt;dbl&gt;,\n#   `1/28/20` &lt;dbl&gt;, `1/29/20` &lt;dbl&gt;, `1/30/20` &lt;dbl&gt;, `1/31/20` &lt;dbl&gt;,\n#   `2/1/20` &lt;dbl&gt;, `2/2/20` &lt;dbl&gt;, `2/3/20` &lt;dbl&gt;, `2/4/20` &lt;dbl&gt;,\n#   `2/5/20` &lt;dbl&gt;, `2/6/20` &lt;dbl&gt;, `2/7/20` &lt;dbl&gt;, `2/8/20` &lt;dbl&gt;,\n#   `2/9/20` &lt;dbl&gt;, `2/10/20` &lt;dbl&gt;, `2/11/20` &lt;dbl&gt;, `2/12/20` &lt;dbl&gt;,\n#   `2/13/20` &lt;dbl&gt;, `2/14/20` &lt;dbl&gt;, `2/15/20` &lt;dbl&gt;, `2/16/20` &lt;dbl&gt;,\n#   `2/17/20` &lt;dbl&gt;, `2/18/20` &lt;dbl&gt;, `2/19/20` &lt;dbl&gt;, `2/20/20` &lt;dbl&gt;, …\n\n\nWhat is incredibly unlucky in my case was that not all of the United Kingdom rows are present in the shorter dataset.\n\nfilter(covid, `Country/Region` == \"United Kingdom\")\n\n# A tibble: 15 × 1,147\n   `Province/State`          `Country/Region`    Lat    Long `1/22/20` `1/23/20`\n   &lt;chr&gt;                     &lt;chr&gt;             &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 Anguilla                  United Kingdom    18.2   -63.1          0         0\n 2 Bermuda                   United Kingdom    32.3   -64.8          0         0\n 3 British Virgin Islands    United Kingdom    18.4   -64.6          0         0\n 4 Cayman Islands            United Kingdom    19.3   -81.3          0         0\n 5 Channel Islands           United Kingdom    49.4    -2.36         0         0\n 6 Falkland Islands (Malvin… United Kingdom   -51.8   -59.5          0         0\n 7 Gibraltar                 United Kingdom    36.1    -5.35         0         0\n 8 Guernsey                  United Kingdom    49.4    -2.59         0         0\n 9 Isle of Man               United Kingdom    54.2    -4.55         0         0\n10 Jersey                    United Kingdom    49.2    -2.14         0         0\n11 Montserrat                United Kingdom    16.7   -62.2          0         0\n12 Pitcairn Islands          United Kingdom   -24.4  -128.           0         0\n13 Saint Helena, Ascension … United Kingdom    -7.95  -14.4          0         0\n14 Turks and Caicos Islands  United Kingdom    21.7   -71.8          0         0\n15 &lt;NA&gt;                      United Kingdom    55.4    -3.44         0         0\n# ℹ 1,141 more variables: `1/24/20` &lt;dbl&gt;, `1/25/20` &lt;dbl&gt;, `1/26/20` &lt;dbl&gt;,\n#   `1/27/20` &lt;dbl&gt;, `1/28/20` &lt;dbl&gt;, `1/29/20` &lt;dbl&gt;, `1/30/20` &lt;dbl&gt;,\n#   `1/31/20` &lt;dbl&gt;, `2/1/20` &lt;dbl&gt;, `2/2/20` &lt;dbl&gt;, `2/3/20` &lt;dbl&gt;,\n#   `2/4/20` &lt;dbl&gt;, `2/5/20` &lt;dbl&gt;, `2/6/20` &lt;dbl&gt;, `2/7/20` &lt;dbl&gt;,\n#   `2/8/20` &lt;dbl&gt;, `2/9/20` &lt;dbl&gt;, `2/10/20` &lt;dbl&gt;, `2/11/20` &lt;dbl&gt;,\n#   `2/12/20` &lt;dbl&gt;, `2/13/20` &lt;dbl&gt;, `2/14/20` &lt;dbl&gt;, `2/15/20` &lt;dbl&gt;,\n#   `2/16/20` &lt;dbl&gt;, `2/17/20` &lt;dbl&gt;, `2/18/20` &lt;dbl&gt;, `2/19/20` &lt;dbl&gt;, …\n\n\nHere are the rows for United Kingdom in the truncated data\n\nfilter(covid_bad, `Country/Region` == \"United Kingdom\")\n\n# A tibble: 6 × 1,147\n  `Province/State`   `Country/Region`   Lat   Long `1/22/20` `1/23/20` `1/24/20`\n  &lt;chr&gt;              &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Anguilla           United Kingdom    18.2 -63.1          0         0         0\n2 Bermuda            United Kingdom    32.3 -64.8          0         0         0\n3 British Virgin Is… United Kingdom    18.4 -64.6          0         0         0\n4 Cayman Islands     United Kingdom    19.3 -81.3          0         0         0\n5 Channel Islands    United Kingdom    49.4  -2.36         0         0         0\n6 Falkland Islands … United Kingdom   -51.8 -59.5          0         0         0\n# ℹ 1,140 more variables: `1/25/20` &lt;dbl&gt;, `1/26/20` &lt;dbl&gt;, `1/27/20` &lt;dbl&gt;,\n#   `1/28/20` &lt;dbl&gt;, `1/29/20` &lt;dbl&gt;, `1/30/20` &lt;dbl&gt;, `1/31/20` &lt;dbl&gt;,\n#   `2/1/20` &lt;dbl&gt;, `2/2/20` &lt;dbl&gt;, `2/3/20` &lt;dbl&gt;, `2/4/20` &lt;dbl&gt;,\n#   `2/5/20` &lt;dbl&gt;, `2/6/20` &lt;dbl&gt;, `2/7/20` &lt;dbl&gt;, `2/8/20` &lt;dbl&gt;,\n#   `2/9/20` &lt;dbl&gt;, `2/10/20` &lt;dbl&gt;, `2/11/20` &lt;dbl&gt;, `2/12/20` &lt;dbl&gt;,\n#   `2/13/20` &lt;dbl&gt;, `2/14/20` &lt;dbl&gt;, `2/15/20` &lt;dbl&gt;, `2/16/20` &lt;dbl&gt;,\n#   `2/17/20` &lt;dbl&gt;, `2/18/20` &lt;dbl&gt;, `2/19/20` &lt;dbl&gt;, `2/20/20` &lt;dbl&gt;, …\n\n\nWhat’s worse is that the row containing the covid cases for mainland Uk are only present in the full dataset. This is where most of the cases occur.\n\nfilter(covid, `Country/Region` == \"United Kingdom\", is.na(`Province/State`))\n\n# A tibble: 1 × 1,147\n  `Province/State` `Country/Region`   Lat  Long `1/22/20` `1/23/20` `1/24/20`\n  &lt;chr&gt;            &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 &lt;NA&gt;             United Kingdom    55.4 -3.44         0         0         0\n# ℹ 1,140 more variables: `1/25/20` &lt;dbl&gt;, `1/26/20` &lt;dbl&gt;, `1/27/20` &lt;dbl&gt;,\n#   `1/28/20` &lt;dbl&gt;, `1/29/20` &lt;dbl&gt;, `1/30/20` &lt;dbl&gt;, `1/31/20` &lt;dbl&gt;,\n#   `2/1/20` &lt;dbl&gt;, `2/2/20` &lt;dbl&gt;, `2/3/20` &lt;dbl&gt;, `2/4/20` &lt;dbl&gt;,\n#   `2/5/20` &lt;dbl&gt;, `2/6/20` &lt;dbl&gt;, `2/7/20` &lt;dbl&gt;, `2/8/20` &lt;dbl&gt;,\n#   `2/9/20` &lt;dbl&gt;, `2/10/20` &lt;dbl&gt;, `2/11/20` &lt;dbl&gt;, `2/12/20` &lt;dbl&gt;,\n#   `2/13/20` &lt;dbl&gt;, `2/14/20` &lt;dbl&gt;, `2/15/20` &lt;dbl&gt;, `2/16/20` &lt;dbl&gt;,\n#   `2/17/20` &lt;dbl&gt;, `2/18/20` &lt;dbl&gt;, `2/19/20` &lt;dbl&gt;, `2/20/20` &lt;dbl&gt;, …\n\n\n\nfilter(covid_bad, `Country/Region` == \"United Kingdom\", is.na(`Province/State`))\n\n# A tibble: 0 × 1,147\n# ℹ 1,147 variables: Province/State &lt;chr&gt;, Country/Region &lt;chr&gt;, Lat &lt;dbl&gt;,\n#   Long &lt;dbl&gt;, 1/22/20 &lt;dbl&gt;, 1/23/20 &lt;dbl&gt;, 1/24/20 &lt;dbl&gt;, 1/25/20 &lt;dbl&gt;,\n#   1/26/20 &lt;dbl&gt;, 1/27/20 &lt;dbl&gt;, 1/28/20 &lt;dbl&gt;, 1/29/20 &lt;dbl&gt;, 1/30/20 &lt;dbl&gt;,\n#   1/31/20 &lt;dbl&gt;, 2/1/20 &lt;dbl&gt;, 2/2/20 &lt;dbl&gt;, 2/3/20 &lt;dbl&gt;, 2/4/20 &lt;dbl&gt;,\n#   2/5/20 &lt;dbl&gt;, 2/6/20 &lt;dbl&gt;, 2/7/20 &lt;dbl&gt;, 2/8/20 &lt;dbl&gt;, 2/9/20 &lt;dbl&gt;,\n#   2/10/20 &lt;dbl&gt;, 2/11/20 &lt;dbl&gt;, 2/12/20 &lt;dbl&gt;, 2/13/20 &lt;dbl&gt;, 2/14/20 &lt;dbl&gt;,\n#   2/15/20 &lt;dbl&gt;, 2/16/20 &lt;dbl&gt;, 2/17/20 &lt;dbl&gt;, 2/18/20 &lt;dbl&gt;, …\n\n\nWe can filter the data for United Kingdom and use select to show the last column. The helper function last_col() is incredibly useful for this as we don’t need to know the name of the column. Using summarise we can add all the values in the column. This is the cumulative number of casees at the last date in the dataset. Dividing by one million (1e6) makes the numbers a bit easier to read.\n\nfilter(covid, `Country/Region` == \"United Kingdom\") %&gt;% \n  select(last_col()) %&gt;% \n  summarise(sum(.) / 1e6)\n\n# A tibble: 1 × 1\n  `sum(.)/1e+06`\n           &lt;dbl&gt;\n1           24.7\n\n\nFor the truncated dataset, the number of counts is substantially lower as we are missing the mainland Uk data. So it is not surprisingly that the trend lines for the Uk did not look correct.\n\nfilter(covid_bad, `Country/Region` == \"United Kingdom\") %&gt;% \n  select(last_col()) %&gt;% \n  summarise(sum(.))\n\n# A tibble: 1 × 1\n  `sum(.)`\n     &lt;dbl&gt;\n1    63439\n\n\nIn this particular instance I think my poor internet connection on the train wi-fi must have caused me to not download the complete file. Since no errors were produced I might not have detected the problem if I wasn’t trying to visualise data for United Kingdom which happened to be located towards the bottom of the file.\nChecking the dimensions of the tibble help to diagnose the problem, but there are couple of other techniques. R has an in-built function to check the size of a file.\n\nfile.size(\"raw_data/time_series_covid19_confirmed_global.csv\")\n\n[1] 1820194\n\n\n\nfile.size(\"time_series_covid19_confirmed_global_BAD.csv\")\n\n[1] 1714255\n\n\nA bit more rigourous is to calculate a checksum. In short, this is a calculated value that represents the exact contents of a file. If the file changes — even by a single byte — the checksum changes as well. That’s why it’s often described as a “digital fingerprint” for data integrity. This can be done in R by first loading the tools library and using the md5sum function.\n\n# Load the tools package\nlibrary(tools)\n\n# Compute MD5 checksum for a file\nfile_path &lt;- \"path/to/your/file.txt\"\nchecksum &lt;- md5sum(\"raw_data/time_series_covid19_confirmed_global.csv\")\nchecksum\n\nraw_data/time_series_covid19_confirmed_global.csv \n               \"095dfec62d4981a32630bed5094296f7\" \n\n\n\nchecksum_bad &lt;- md5sum(\"time_series_covid19_confirmed_global_BAD.csv\")\nchecksum_bad\n\ntime_series_covid19_confirmed_global_BAD.csv \n          \"179af9d326d1c9451c49e5d9821c85e0\" \n\n\nIn practice if someone is sending data you large, especially if the file is large, they will also send a file containing checksums for you to check data integrity. The following would print TRUE if both files had exactly the same contents.\n\nchecksum == checksum_bad\n\nraw_data/time_series_covid19_confirmed_global.csv \n                                            FALSE"
  },
  {
    "objectID": "training/bulk-rnaseq_1/index.html#pre-amble",
    "href": "training/bulk-rnaseq_1/index.html#pre-amble",
    "title": "Introduction to RNA-Seq - Part 1",
    "section": "",
    "text": "High-throughput sequencing is now established as a standard technique for many functional genomics studies; allowing the researcher to compare and contrast the transcriptomes of many individuals to obtain biological insight. A high-volume of data are generated from these experimental techniques and thus require robust and reproducible tools to be employed in the analysis.\nIn this workshop, you will be learning how to analyse RNA-seq count data, using R. This will include reading the data into R, quality control and performing differential expression analysis and gene set testing, with a focus on the well-respected DESEq2 analysis workflow. You will learn how to generate common plots for analysis and visualisation of gene expression data, such as boxplots and heatmaps.\n\n\n\n\n\n\nNote\n\n\n\nWe will be discussing Bulk RNA-seq only, although some of the methods and techniques will be applicable to single-cell RNA-seq. I am planning some materials on single-cell RNA-seq in the future. In the meantime, the homepage for Seurat (a popular R package for single-cell analysis) has lots of useful tutorials.\n\nSeurat homepage"
  },
  {
    "objectID": "training/bulk-rnaseq_1/index.html#setup",
    "href": "training/bulk-rnaseq_1/index.html#setup",
    "title": "Introduction to RNA-Seq - Part 1",
    "section": "",
    "text": "You will need to install some R packages before you start, which I usually do this at RStudio’s console. See the below screenshot.\n\nRather than typing the location of the install script I suggest copying from here:-\n\nsource(\"https://raw.githubusercontent.com/markdunning/markdunning.github.com/refs/heads/master/files/training/bulk_rnaseq/install_bioc_packages.R\")\n\nIt may take a few minutes, but you will only have to do this once for a specific version of R. To check that everything worked, now copy and paste the following command. It should print messages to the screen to say that all the packages were installed\n\nsource(\"https://raw.githubusercontent.com/markdunning/markdunning.github.com/refs/heads/master/files/training/bulk_rnaseq/check_packages.R\")\n\nI also recommend creating a new project to work through the tutorial. You can do this via the file menu in Rstudio and it will ask you to choose a directory on your hard drive that you want the project to be located in. Briefly, using “projects” is a convenient way of keeping all the input data, R code, and outputs for a particular analysis together in the same place.\nFile -&gt; New Project -&gt; New Directory\n\nIn the screenshot, I am using a directory (which doesn’t exist at this point) called bulkrnaseq_tutorial in c:\\work\\personal_development. RStudio should now refresh and your working directory will be the location that you specified. Now we will use some code to download the example data. Again, I suggest you copy from below rather than typing manually.\n\n## Create two folders for the meta data and counts\ndir.create(\"meta_data\", showWarnings = FALSE)\ndir.create(\"raw_counts\",showWarnings = FALSE)\n\n\n## Download the raw data files\ndownload.file(\"https://raw.githubusercontent.com/markdunning/markdunning.github.com/refs/heads/master/files/training/bulk_rnaseq/meta_data/sampleInfo.csv\", destfile = \"meta_data/sampleInfo.csv\")\ndownload.file(\"https://raw.githubusercontent.com/markdunning/markdunning.github.com/refs/heads/master/files/training/bulk_rnaseq/raw_counts/raw_counts_matrix.tsv\", destfile = \"raw_counts/raw_counts_matrix.tsv\")\n\nHopefully your screen should look a bit like this:-\n\nFinally, create a new “Quarto Document” from the File menu\nFile -&gt; New File -&gt; Quarto Document\n\nClicking the “Create Empty Document” button on the pop-up that appears will create a blank quarto document that you can use to type the code from this tutorial, and any comments you may wish to make along the way. The Title and Author boxes can be filled with anything you like. They are used when you want to create a report document from your code. A new panel should appear in RStudio which is your bare bones analysis. The title and author lines should correspond to the text you entered (if any) when creating the document\nR code can be added to this document by clicking the “Insert Code Chunk” toolbar option\n\nThis will give you space to type or paste R code from the tutorial. In the below screenshot, R code can be entered between lines 8 and 10. Pressing ENTER between these lines will allow more code to be written. Pressing CTRL + ENTER causes the code to be run.\n\nLines outside of a code chunk can be used to write any explanations or interpretations of the plots, stats that you produce along the way.\nAt this point you are working on an untitled document that is not saved to disk. Go through the menu File -&gt; Save and choose a file name"
  },
  {
    "objectID": "training/bulk-rnaseq_1/index.html#quick-start",
    "href": "training/bulk-rnaseq_1/index.html#quick-start",
    "title": "Introduction to RNA-Seq - Part 1",
    "section": "",
    "text": "I will go through the setup in quite a bit of detail. If you are already fairly confident with R, you can probably skim this and proceed to the start of the pre-processing section.\n\nCreate a new RStudio project that you want to work in\nCreate folders called meta_data and raw_counts\nDownload the meta data and raw counts\n\nmeta data\nraw counts\n\nPlace the sampleInfo.csv and raw_counts_matrix.tsv files into meta_data and raw_counts folders respectively\nInstall some R packages using this command\n\n\nsource(\"https://raw.githubusercontent.com/markdunning/markdunning.github.com/refs/heads/master/files/training/bulk_rnaseq/install_bioc_packages.R\")"
  },
  {
    "objectID": "training/bulk-rnaseq_1/index.html#rna-seq-processing",
    "href": "training/bulk-rnaseq_1/index.html#rna-seq-processing",
    "title": "Introduction to RNA-Seq - Part 1",
    "section": "",
    "text": "In this short section, we will briefly describe the pre-processing of RNA-seq data which is not typically done in R. If you are only interested in using R, you may skip to the next section.\nThere are many steps involved in analysing an RNA-Seq experiment.\n\n(Workflow image from Harvard Bioinformatics Core)\nAnalysing an RNAseq experiment begins with sequencing reads. These are large (typically several Gb) that contain information on the sequences that have been generated for each biological sample; one fastq (or pair of fastqs) for each sample. Each set of four lines describe one sequence (called a “read”).\nA typical RNA-seq experiment will have 10 - 30 million reads in a fastq file, with each read about 100 bases long. e.g.\n@D0UW5ACXX120511:8:1204:6261:40047/1\nAATGTTTATGTTCTTAAATTTTAGTTGTATATGTGAATCTTTGTAGTTTTTGCTAAAATACTAAGTAATTTATATAAAAGTGAGTTAAGAGATTTTTCTGA\n+\nCCCFFFFFHHHHHJJJJJIJJJJJIJJHIIJIJIJJIJJJIJJHIIHIJJJJJJBEGIHIJICGIDICFGIJJJIIJJGJ&gt;F&gt;GAGCGEEHEHHEEFFFD&gt;\nAs the fastq files are large, we tend to analyse them using command-line software and a computing cluster. The traditional workflow for RNA-seq compares the sequences to a reference genome to see which genomic region each read matches the best.\n\nAgain, this requires more memory than a typical laptop or desktop machine so is performed on a remote computer with large memory. The resulting file is called a bam and records the best genomic match for each read. However, as we are interested in gene expression we want to relate these mappings to the positions of genes.\nA variety of different counting methods can determine how many reads overlap each known gene region. These are know as the raw counts and are the kind of data we will start with today.\n\nRecent tools for RNA-seq analysis (e.g. salmon, kallisto) do not require the time-consuming step of whole-genome alignment to be performed, and can therefore produce gene-level counts in a much faster time frame. They not require the creation of large bam files, which is useful if constrained by file space (e.g. if using Galaxy).\nMy strong recommendation would be to use the nextflow workflow system in conjunction with the nf.core pipeline to align and quantify your RNA-seq reads. My former colleague Dr. Lewis Quayle has a really good write-up of using nf.core on his pages\n\nhttps://www.lewisdoesdata.com/2024/05/01/bulk-rnaseq-end-to-end-part-1.html\n\n\n\n\n\n\n\nImportant\n\n\n\nUnless you are doing something extremely novel and bespoke I don’t believe it would be worth writing your own pipeline for processing RNA-seq, or any other sequencing data, from scratch rather than using what is available in nf.core."
  },
  {
    "objectID": "posts/2025_10_20_cellxgene/index.html",
    "href": "posts/2025_10_20_cellxgene/index.html",
    "title": "Exploring Single-cell RNA-seq",
    "section": "",
    "text": "How to grab public 10X single-cell RNA-seq and visualise in R\n\n\nUse the Bioconductor package cellxgenedp to download the dataset in Seurat format. More information about this package can be found here\n\nhttps://www.bioconductor.org/packages/release/bioc/vignettes/cellxgenedp/inst/doc/a_using_cellxgenedp.html\n\nIn particular the vignette demonstrates how to search for particular datasets of interest. Since we know the dataset we want, we can use the dataset id to download directly.\n\nlibrary(cellxgenedp)\ndb &lt;- db()\nlocal_file &lt;- files(db) |&gt;\nfilter(\n        dataset_id == \"9fddb063-056d-4202-8b8a-4b0ee531d3ce\",\n        filetype == \"RDS\"\n    ) |&gt;\n    files_download(dry.run = FALSE)\n\nfile.copy(local_file,\".\")\n\nOnce downloaded, we can read the local file into R.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(Seurat)\nbrca &lt;- readRDS(\"cbbb607f-578b-47ba-858b-407bb8be917f.rds\")\n#brca &lt;- NormalizeData(brca)\n\nAs a quick check we can take a look at the UMAP coloured by different subtypes - which should agree with the figures generated from the same dataset on single-cell atlas.\n\nDimPlot(brca,reduction = \"umap\",group.by = \"celltype_major\")\n\n\nVlnPlot(brca, \n        features = \"ENSG00000089041\",\n        split.by = \"subtype\",group.by = \"celltype_major\")\nggsave(\"P2RX7_cell_type_violins.png\")\n\n\np1 &lt;- FeaturePlot(brca, \"ENSG00000089041\")\np2 &lt;- DimPlot(brca,reduction = \"umap\",group.by = \"celltype_major\")\n\ncowplot::plot_grid(p1,p2)\nggsave(\"P2RX7_UMAPs.png\",width=8,height=6)\n\n\nhypoxia_genes &lt;- readxl::read_xls(\"Buffa Hypoxic score genes.xls\",skip = 4) %&gt;% \n  pull(`HUGO Symbol`) \n\nlibrary(org.Hs.eg.db)\nhypoxia_ens &lt;- AnnotationDbi::select(org.Hs.eg.db,\n                              keys = hypoxia_genes,\n                              keytype = \"SYMBOL\",\n                              columns = \"ENSEMBL\") %&gt;% \n                              pull(ENSEMBL) %&gt;% na.omit\n\n\nlibrary(ggpubr)\nbrca &lt;- AddModuleScore(brca, features = list(hypoxia_ens),name=\"hypoxia_score\")\nbrca$HypoxiaGroup &lt;- ifelse(brca$hypoxia_score1 &gt; quantile(brca$hypoxia_score1, 0.75), \"High\", \"Low\") # Example: Top 25% as \"High\"\n\n## get expression values for P2RX7\ngene_expression &lt;- GetAssayData(brca, slot = \"data\", assay = \"RNA\")[\"ENSG00000089041\", ]\n\nbrca$P2RX7 &lt;- gene_expression\n\n\nbrca_ce &lt;- subset(brca, celltype_major == \"Cancer Epithelial\")\n  \nbrca_ce@meta.data %&gt;% \nggplot(aes(x= subtype, y = hypoxia_score1)) + geom_violin() + stat_compare_means() + ylab(\"Hypoxia Score\")\n\nggsave(\"hypoxia_score_violins.png\")\n\n\nanno &lt;- select(org.Hs.eg.db, keys = rownames(brca),\n               keytype = \"ENSEMBL\",\n               columns = c(\"SYMBOL\",\"GENENAME\"))\nhypoxia_markers &lt;- FindMarkers(brca_ce, \n                               ident.1 = \"Low\",\n                               ident.2 = \"High\",\n                               group.by = \"HypoxiaGroup\") %&gt;% \n  tibble::rownames_to_column(\"ENSEMBL\") %&gt;% left_join(anno)\n\n\nAn immediate thought is, although the myeloid cells have higher expression of P2RX7 (which is expected), is there a significant difference in the expression levels between the Normal epithelial cells and cancer epithelial cells in either of the subtypes - could this be evaluated?\n\n\nplot_data &lt;- brca@meta.data %&gt;%\n  filter(grepl(\"Epithelial\", celltype_major)) %&gt;%\n  group_by(subtype, celltype_major) %&gt;%\n  mutate(total_cells = n()) %&gt;% \n  mutate(non_zero_cells = sum(P2RX7 &gt; 0)) %&gt;%\n  dplyr::select(total_cells, non_zero_cells,celltype_major, subtype, P2RX7) %&gt;% \n  ungroup()\n\nplot_data %&gt;% \n  ggplot(aes(x = celltype_major, y = P2RX7)) + geom_violin() + geom_jitter(width=0.1) + stat_compare_means() + facet_wrap(~subtype) +\n  geom_text(\n    aes(label = paste(\"n =\", total_cells, \"(non zero counts:\", non_zero_cells, \")\")),\n    y = max(plot_data$P2RX7) * 0.9, # Adjust vertical position\n    size = 3,\n    vjust = 0\n  )"
  },
  {
    "objectID": "posts/2025_10_20_cellxgene/index.html#download-the-data-from-cellxgene",
    "href": "posts/2025_10_20_cellxgene/index.html#download-the-data-from-cellxgene",
    "title": "Exploring Single-cell RNA-seq",
    "section": "",
    "text": "Use the Bioconductor package cellxgenedp to download the dataset in Seurat format. More information about this package can be found here\n\nhttps://www.bioconductor.org/packages/release/bioc/vignettes/cellxgenedp/inst/doc/a_using_cellxgenedp.html\n\nIn particular the vignette demonstrates how to search for particular datasets of interest. Since we know the dataset we want, we can use the dataset id to download directly.\n\nlibrary(cellxgenedp)\ndb &lt;- db()\nlocal_file &lt;- files(db) |&gt;\nfilter(\n        dataset_id == \"9fddb063-056d-4202-8b8a-4b0ee531d3ce\",\n        filetype == \"RDS\"\n    ) |&gt;\n    files_download(dry.run = FALSE)\n\nfile.copy(local_file,\".\")\n\nOnce downloaded, we can read the local file into R.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(Seurat)\nbrca &lt;- readRDS(\"cbbb607f-578b-47ba-858b-407bb8be917f.rds\")\n#brca &lt;- NormalizeData(brca)\n\nAs a quick check we can take a look at the UMAP coloured by different subtypes - which should agree with the figures generated from the same dataset on single-cell atlas.\n\nDimPlot(brca,reduction = \"umap\",group.by = \"celltype_major\")\n\n\nVlnPlot(brca, \n        features = \"ENSG00000089041\",\n        split.by = \"subtype\",group.by = \"celltype_major\")\nggsave(\"P2RX7_cell_type_violins.png\")\n\n\np1 &lt;- FeaturePlot(brca, \"ENSG00000089041\")\np2 &lt;- DimPlot(brca,reduction = \"umap\",group.by = \"celltype_major\")\n\ncowplot::plot_grid(p1,p2)\nggsave(\"P2RX7_UMAPs.png\",width=8,height=6)\n\n\nhypoxia_genes &lt;- readxl::read_xls(\"Buffa Hypoxic score genes.xls\",skip = 4) %&gt;% \n  pull(`HUGO Symbol`) \n\nlibrary(org.Hs.eg.db)\nhypoxia_ens &lt;- AnnotationDbi::select(org.Hs.eg.db,\n                              keys = hypoxia_genes,\n                              keytype = \"SYMBOL\",\n                              columns = \"ENSEMBL\") %&gt;% \n                              pull(ENSEMBL) %&gt;% na.omit\n\n\nlibrary(ggpubr)\nbrca &lt;- AddModuleScore(brca, features = list(hypoxia_ens),name=\"hypoxia_score\")\nbrca$HypoxiaGroup &lt;- ifelse(brca$hypoxia_score1 &gt; quantile(brca$hypoxia_score1, 0.75), \"High\", \"Low\") # Example: Top 25% as \"High\"\n\n## get expression values for P2RX7\ngene_expression &lt;- GetAssayData(brca, slot = \"data\", assay = \"RNA\")[\"ENSG00000089041\", ]\n\nbrca$P2RX7 &lt;- gene_expression\n\n\nbrca_ce &lt;- subset(brca, celltype_major == \"Cancer Epithelial\")\n  \nbrca_ce@meta.data %&gt;% \nggplot(aes(x= subtype, y = hypoxia_score1)) + geom_violin() + stat_compare_means() + ylab(\"Hypoxia Score\")\n\nggsave(\"hypoxia_score_violins.png\")\n\n\nanno &lt;- select(org.Hs.eg.db, keys = rownames(brca),\n               keytype = \"ENSEMBL\",\n               columns = c(\"SYMBOL\",\"GENENAME\"))\nhypoxia_markers &lt;- FindMarkers(brca_ce, \n                               ident.1 = \"Low\",\n                               ident.2 = \"High\",\n                               group.by = \"HypoxiaGroup\") %&gt;% \n  tibble::rownames_to_column(\"ENSEMBL\") %&gt;% left_join(anno)\n\n\nAn immediate thought is, although the myeloid cells have higher expression of P2RX7 (which is expected), is there a significant difference in the expression levels between the Normal epithelial cells and cancer epithelial cells in either of the subtypes - could this be evaluated?\n\n\nplot_data &lt;- brca@meta.data %&gt;%\n  filter(grepl(\"Epithelial\", celltype_major)) %&gt;%\n  group_by(subtype, celltype_major) %&gt;%\n  mutate(total_cells = n()) %&gt;% \n  mutate(non_zero_cells = sum(P2RX7 &gt; 0)) %&gt;%\n  dplyr::select(total_cells, non_zero_cells,celltype_major, subtype, P2RX7) %&gt;% \n  ungroup()\n\nplot_data %&gt;% \n  ggplot(aes(x = celltype_major, y = P2RX7)) + geom_violin() + geom_jitter(width=0.1) + stat_compare_means() + facet_wrap(~subtype) +\n  geom_text(\n    aes(label = paste(\"n =\", total_cells, \"(non zero counts:\", non_zero_cells, \")\")),\n    y = max(plot_data$P2RX7) * 0.9, # Adjust vertical position\n    size = 3,\n    vjust = 0\n  )"
  },
  {
    "objectID": "training/bulk-rnaseq_1/index.html#introducing-the-example-dataset",
    "href": "training/bulk-rnaseq_1/index.html#introducing-the-example-dataset",
    "title": "Introduction to RNA-Seq - Part 1",
    "section": "",
    "text": "The data for this tutorial comes from the paper, Induction of fibroblast senescence generates a non-fibrogenic myofibroblast phenotype that differentially impacts on cancer prognosis..\n\nCancer associated fibroblasts characterized by an myofibroblastic phenotype play a major role in the tumour microenvironment, being associated with poor prognosis. We found that this cell population is made in part by senescent fibroblasts in vivo. As senescent fibroblasts and myofibroblasts have been shown to share similar tumour promoting functions in vitro we compared the transcriptosomes of these two fibroblast types and performed RNA-seq of human foetal foreskin fibroblasts 2 (HFFF2) treated with 2ng/ml TGF-beta-1 to induce myofibroblast differentiation or 10Gy gamma irradiation to induce senescence. We isolated RNA 7 days upon this treatments changing the medium 3 days before the RNA extraction.\n\n\n\n\n\n\n\nNote\n\n\n\nA really useful resource for obtaining raw sequencing reads is sraexplorer. Given a GEO, ArrayExpress of SRA dataset name it will give you links to download the raw fastq files\n\nSRA Explorer\n\nAgain, Lewis’ page has some commands for downloading these data in fastq form.\n\nDownloading the example data"
  },
  {
    "objectID": "training/bulk-rnaseq_1/index.html#about-metadata",
    "href": "training/bulk-rnaseq_1/index.html#about-metadata",
    "title": "Introduction to RNA-Seq - Part 1",
    "section": "About “metadata”",
    "text": "About “metadata”\nModern sequencing devices are truly remarkable machines capable of taking biological material and generating vast amounts of biological sequence. However, they know nothing about the biological context of the experiment taking place - nor do they need to. If you are a researcher requesting sequencing from a Core facility you will typically submit a set of test tubes (containing prepared sequencing libraries) along with a spreadsheet giving a name to each sample. These names can be as simple as “Sample 1”, “Sample 2” etc. When the data come back from sequencing, the raw data will retain the labels “Sample 1”, “Sample 2”. We refer to metadata as the data that describes the biological and technical characteristics of the samples we have sequenced\n. Examples of variables recorded in the metadata might include.\n\ntumour / normal status\ncell line\nage\ngender\ndate of collection\nlitter\n\nWe include the sample groups that we want to compare, and any potential confounding factors that we might need to address as part of our quality assessment. The metadata is stored in a spreadsheet and typically entered by-hand. When creating such data we should be mindful of some best-practice guidelines that will make our data easier to read into R.\n\n\n\n\n\n\nNote\n\n\n\nSee here for a round-up of common errors to be avoiding when creating spreadsheets\n\nData Carpentry lesson on spreadsheet errors\n\n\n\nThe sampleInfo.csv in the meta_data folder contains basic information about the samples that we will need for the analysis today. This includes the ID for the sample ID assigned by the researcher (Run), the experimental condition (condition), shorter name for the sample (Name), the replicate number (Replicate) and whether the sample is a control or treated sample (Treated).\n\nsampleInfo &lt;- read.csv(\"meta_data/sampleInfo.csv\")\nsampleInfo\n\n           Run condition  Name Replicate Treated\n1  1_CTR_BC_2        CTR CTR_1         1       N\n2   2_TGF_BC_4       TGF TGF_1         1       Y\n3    3_IR_BC_5        IR  IR_1         1       Y\n4   4_CTR_BC_6       CTR CTR_2         2       N\n5   5_TGF_BC_7       TGF TGF_2         2       Y\n6   6_IR_BC_12        IR  IR_2         2       Y\n7 7_CTR_BC_13        ctr CTR_3         3       N\n8  8_TGF_BC_14       tgf TGF_3         3       Y\n9   9_IR_BC_15        ir  IR_3         3       Y\n\n\nIf you want to know how to create such a data frame based on the column names alone (i.e. if no csv file was provided), read the following note.\n\n\n\n\n\n\nInferring the sample information from the count column names\n\n\n\n\n\nIt is sometimes possible to create a meta data spreadsheet if none is available. This relies on the column names of the count matrix being named in a consistent fashion. In this example we can see that the biological group (CTR, TGF or IR) is encoded in the name and the column names contain a _ character. The stringr and tidyr packages include the functionality for dealing cleaning these data.\n\nstringr reference guide\n\n\ninstall.packages(\"stringr\")\ninstall.packages(\"tidyr\")\n\n\nlibrary(stringr)\nlibrary(tidyr)\nlibrary(dplyr)\n\n## Get the column names of counts - except the first column which is the gene name\ncols &lt;- colnames(counts)[-1]\ncols \n\n[1] \"1_CTR_BC_2\"  \"2_TGF_BC_4\"  \"3_IR_BC_5\"   \"4_CTR_BC_6\"  \"5_TGF_BC_7\" \n[6] \"6_IR_BC_12\"  \"7_CTR_BC_13\" \"8_TGF_BC_14\" \"9_IR_BC_15\" \n\n\nThe column names consists of various letters separated by a _. Some of these can be used to form the conditions and replicate information, and the separate function from tidyr is an efficient way of splitting the Run column into different components. The BC and number at the each of the name is not actually useful analysis, so we will remove in the next step. The remove = FALSE argument is used to keep the Run column in the data.\n\nmeta &lt;- data.frame(Run = cols) %&gt;% \n  separate(Run, into = c(\"Sample Number\", \"condition\", \"BC\", \"X\"),remove = FALSE)\nmeta\n\n          Run Sample Number condition BC  X\n1  1_CTR_BC_2             1       CTR BC  2\n2  2_TGF_BC_4             2       TGF BC  4\n3   3_IR_BC_5             3        IR BC  5\n4  4_CTR_BC_6             4       CTR BC  6\n5  5_TGF_BC_7             5       TGF BC  7\n6  6_IR_BC_12             6        IR BC 12\n7 7_CTR_BC_13             7       CTR BC 13\n8 8_TGF_BC_14             8       TGF BC 14\n9  9_IR_BC_15             9        IR BC 15\n\n\nNow we keep the columns we need, and add the replicate numbers using the rep function to repeat a sequence 1,2,3 three times. The shorter name is formed by pasting the condition and replicate number (using paste with a _ separator. Finally, Treated column is formed using an ifelse statement. This is used to test if a particular value of condition is equal to CTR or not. If it is, a value of N is used in the new Treated column. If not, the value of Y is used instead.\n\nmeta &lt;- select(meta, Run, condition) %&gt;% \n  mutate(Replicate = rep(c(1,2,3), 3)) %&gt;% \n  mutate(Name = paste(condition, Replicate, sep = \"_\")) %&gt;% \n  mutate(Treated = ifelse(condition == \"CTR\", \"N\", \"Y\"))\n\n\n\n\nAlthough we can use basic R commands to read the counts and sample information into R, it is much more common to use specialist software for further analysis. Packages such as DESeq2 (used in this tutorial), edgeR and limma have evolved other many years to offer an efficient way of storing and manipulating complex datasets.\n\nlibrary(DESeq2)\n\nThe DESeq2 allows data to be imported from various formats, and the option we will be using here is that of a counts matrix. The “vignette” for DESeq2 is incredibly detailed, and offers code for importing data produced by different software\n\nDESeq2 Guide\n\nHowever, as we said the count matrix is the most common. It is important to note, as described in the vignette, that your counts must be raw and not subjected to any kind of normalisation or calibration. These steps are included in the standard workflow as we will see.\nThe code to import our data as given below. We also have to specify at this point a design for analysis. In the simplest terms this is essentially telling DESeq2 which column in our dataset to use when making comparisons between groups. DESeq2 will not actually do any differential expression unless we tell it to, so this is just a placeholder value for now. We can also introduce more complicated designs that might more than one column.\nWe would normally think of a “count matrix” as containing just numeric data. Our counts contains gene identifiers which might usually be a problem but DESeq2 is still able to import the data if we set tidy=TRUE.\n\ndds &lt;- DESeqDataSetFromMatrix(counts, \n                                  colData = sampleInfo, \n                                  design = ~condition, tidy = TRUE)\n\nWarning in DESeqDataSet(se, design = design, ignoreRank): some variables in\ndesign formula are characters, converting to factors\n\n\n\n\n\n\n\n\nWarning about “converting to factors”\n\n\n\nYou will probably get a Warning message including the phrase some variables in design formula are characters, converting to factors. This is perfectly fine and just means that DESeq2 has converted some columns in your sample information from characters into factors. i.e. a column containing a categorical variable.\n\n\nPrinting the contents of dds to the screen gives some details of how the data are represented, and happily for us it does not print all the contents to the screen\n\ndds\n\nclass: DESeqDataSet \ndim: 57914 9 \nmetadata(1): version\nassays(1): counts\nrownames(57914): ENSG00000000003 ENSG00000000005 ... ENSG00000284747\n  ENSG00000284748\nrowData names(0):\ncolnames(9): 1_CTR_BC_2 2_TGF_BC_4 ... 8_TGF_BC_14 9_IR_BC_15\ncolData names(5): Run condition Name Replicate Treated\n\n\nThe object contains all the counts which can be retrieved using the counts function. The head function is used so that only the first six rows (genes) are shown.\nThe gene names that appear in the rows are clearly not very useful for us now, but these can be converted into something more manageable when we come to interpret our data.\n\nhead(counts(dds))\n\n                1_CTR_BC_2 2_TGF_BC_4 3_IR_BC_5 4_CTR_BC_6 5_TGF_BC_7\nENSG00000000003       1579       1547      1342       1704       1395\nENSG00000000005          0          0         0          0          0\nENSG00000000419       1774       1775      1866       1809       1921\nENSG00000000457        698        617       601        733        662\nENSG00000000460        246        309       116        224        255\nENSG00000000938         10          5        11          6          1\n                6_IR_BC_12 7_CTR_BC_13 8_TGF_BC_14 9_IR_BC_15\nENSG00000000003       1264        1556        1370       1269\nENSG00000000005          0           0           0          1\nENSG00000000419       1776        1797        1482       1980\nENSG00000000457        537         751         593        628\nENSG00000000460        108         245         289        127\nENSG00000000938          2          19           4          8\n\n\nWhereas colData will display the meta data that has been stored with the object.\n\ncolData(dds)\n\nDataFrame with 9 rows and 5 columns\n                     Run condition        Name Replicate     Treated\n             &lt;character&gt;  &lt;factor&gt; &lt;character&gt; &lt;integer&gt; &lt;character&gt;\n1_CTR_BC_2   1_CTR_BC_2        CTR       CTR_1         1           N\n2_TGF_BC_4    2_TGF_BC_4       TGF       TGF_1         1           Y\n3_IR_BC_5      3_IR_BC_5       IR         IR_1         1           Y\n4_CTR_BC_6    4_CTR_BC_6       CTR       CTR_2         2           N\n5_TGF_BC_7    5_TGF_BC_7       TGF       TGF_2         2           Y\n6_IR_BC_12    6_IR_BC_12       IR         IR_2         2           Y\n7_CTR_BC_13 7_CTR_BC_13        ctr       CTR_3         3           N\n8_TGF_BC_14  8_TGF_BC_14       tgf       TGF_3         3           Y\n9_IR_BC_15    9_IR_BC_15       ir         IR_3         3           Y\n\n\nIndividual columns from the metadata can also be accessed and printed using the $ notation. From the warning message that appeared when we created dds we saw that it already converted Treated into a factor\n\ndds$condition\n\n[1] CTR TGF IR  CTR TGF IR  ctr tgf ir \nLevels: ctr CTR ir IR tgf TGF\n\n\nWhereas Treated is still a character vector.\n\ndds$Treated\n\n[1] \"N\" \"Y\" \"Y\" \"N\" \"Y\" \"Y\" \"N\" \"Y\" \"Y\"\n\n\n\nVisualising library sizes\nWe can look at a few different plots to check that the data is good quality, and that the samples are behaving as we would expect. First, we can check how many reads we have for each sample in the DESeqDataSet. We will never get exactly the same total of total counts for each sample, but they should be roughly the same. Low total number of counts could be indicative of poor quality sample.\nThe counts themselves are accessed using the counts function; giving a matrix of counts. The sum of a particular column is therefore the total number of reads for that sample.\n\nsum(counts(dds)[,1])\n\n[1] 37966392\n\n\nA convenience function colSums exists for calculating the sum of each column in a matrix, returning a vector as a result.\n\ncolSums(counts(dds))\n\n 1_CTR_BC_2  2_TGF_BC_4   3_IR_BC_5  4_CTR_BC_6  5_TGF_BC_7  6_IR_BC_12 \n   37966392    42302453    33300002    39401879    37716366    32599748 \n7_CTR_BC_13 8_TGF_BC_14  9_IR_BC_15 \n   34273109    38522174    36478190 \n\n\n\n\nExercise\n\nUse an appropriate function from dplyr to add a column containing the number of reads for each sample to the sampleInfo data frame.\nProduce a bar plot to show the Millions of reads for each sample (see below)\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(ggplot2)\nmutate(sampleInfo, LibSize = colSums(assay(dds))/1e6) %&gt;% \n  ggplot(aes(x = Name, y = LibSize)) + geom_col(fill=\"steelblue\") + geom_hline(yintercept = 20,col=\"red\",lty=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising count distributions\nWe typically use a boxplot to visualise difference the distributions of the columns of a numeric data frame. Applying the boxplot function to the raw counts from our dataset reveals something about the nature of the data; the distributions are dominated by a few genes with very large counts. We are using the base boxplot function here for convenience because the count data are not in the long data format required by ggplot2.\n\nboxplot(counts(dds))\n\n\n\n\n\n\n\n\nWe can use the vst or rlog function from DESeq2to compensate for the effect of different library sizes and put the data on the log\\(_2\\) scale. The effect is to remove the dependence of the variance on the mean, particularly the high variance of the logarithm of count data when the mean is low. For more details see the DESeq2 vignette\n\n# Get log2 counts\nvsd &lt;- vst(dds)\n# Check distributions of samples using boxplots\nboxplot(assay(vsd), xlab=\"\", ylab=\"Log2 counts per million\",las=2,main=\"Normalised Distributions\")\n\nabline(h=median(assay(vsd)), col=\"blue\")\n\n\n\n\n\n\n\n\nWe can see that using vst has made the distributions more comparable, and indeed these transformed data will form the basis for most of our quality assessment. They are not however used for any differential analysis. DESeq2 has it’s own normalisation method"
  },
  {
    "objectID": "training/bulk-rnaseq_1/index.html#inferring-the-sample-information-from-the-count-column-names",
    "href": "training/bulk-rnaseq_1/index.html#inferring-the-sample-information-from-the-count-column-names",
    "title": "Introduction to RNA-Seq - Part 1",
    "section": "Inferring the sample information from the count column names",
    "text": "Inferring the sample information from the count column names\nIt is sometimes possible to create a meta data spreadsheet if none is available. This relies on the column names of the count matrix being named in a consistent fashion. In this example we can see that the biological group (CTR, TGF or IR) is encoded in the name and the column names contain a _ character. The stringr package includes many useful functions for dealing with strings in R. The columns have an X at the start, which is usually caused by R not being able to handle column names that start with a number\n\nstringr reference guide\n\n\ninstall.packages(\"stringr\")\n\n\nlibrary(stringr)\nlibrary(tidyr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n## Get the column names of counts - except the first column which is the gene name\ncols &lt;- colnames(counts)[-1]\ncols \n\n[1] \"X1_CTR_BC_2\"  \"X2_TGF_BC_4\"  \"X3_IR_BC_5\"   \"X4_CTR_BC_6\"  \"X5_TGF_BC_7\" \n[6] \"X6_IR_BC_12\"  \"X7_CTR_BC_13\" \"X8_TGF_BC_14\" \"X9_IR_BC_15\" \n\n\nFirst we can remove the unwanted X from the start of each column name\n\n## Remove the X from each name\n\ncols &lt;- str_remove_all(cols, \"X\")\ncols\n\n[1] \"1_CTR_BC_2\"  \"2_TGF_BC_4\"  \"3_IR_BC_5\"   \"4_CTR_BC_6\"  \"5_TGF_BC_7\" \n[6] \"6_IR_BC_12\"  \"7_CTR_BC_13\" \"8_TGF_BC_14\" \"9_IR_BC_15\" \n\n\nThe column names consists of various letters separated by a _. Some of these can be used to form the conditions and replicate information. The BC and number at the each of the name is not actually useful analysis, so we will remove in the next step.\n\nmeta &lt;- data.frame(Run = cols) %&gt;% \n  separate(Run, into = c(\"Sample Number\", \"condition\", \"BC\", \"X\"),remove = FALSE)\nmeta\n\n          Run Sample Number condition BC  X\n1  1_CTR_BC_2             1       CTR BC  2\n2  2_TGF_BC_4             2       TGF BC  4\n3   3_IR_BC_5             3        IR BC  5\n4  4_CTR_BC_6             4       CTR BC  6\n5  5_TGF_BC_7             5       TGF BC  7\n6  6_IR_BC_12             6        IR BC 12\n7 7_CTR_BC_13             7       CTR BC 13\n8 8_TGF_BC_14             8       TGF BC 14\n9  9_IR_BC_15             9        IR BC 15\n\n\nNow we keep the columns we need, and add the replicate numbers using the rep function to repeat a sequence 1,2,3 three times. The shorte\n\nmeta &lt;- select(meta, Run, condition) %&gt;% \n  mutate(Replicate = rep(c(1,2,3), 3)) %&gt;% \n  mutate(Name = paste(condition, Replicate, sep = \"_\")) %&gt;% \n  mutate(Treated = ifelse(condition == \"CTR\", \"N\", \"Y\"))\n\n:::"
  },
  {
    "objectID": "training/bulk-rnaseq_1/index.html#principal-components-analysis-pca",
    "href": "training/bulk-rnaseq_1/index.html#principal-components-analysis-pca",
    "title": "Introduction to RNA-Seq - Part 1",
    "section": "Principal components Analysis (PCA)",
    "text": "Principal components Analysis (PCA)\n\n\n\n\n\n\nNote\n\n\n\nSee here for a nice explanation of PCA\n\nhttps://www.youtube.com/watch?v=0Jp4gsfOLMs\n\n\n\nThe (Principal Components Analysis) PCA plot, shows the samples in the 2D plane spanned by their first two principal components. A PCA is an example of an unsupervised analysis, where we don’t need to specify the groups. Each point in the plot is a biological sample in your dataset, and crucially the points can be coloured and labeled according to your metadata. If your experiment is well-controlled and has worked well, what we hope to see is that the greatest sources of variation in the data correspond to the treatments/groups we are interested in. In other words, the points on the plot should separate according to biological conditions.\nIt is also an incredibly useful tool for quality control and checking for outliers, and DESeq2 has a convenient plotPCA function for making the PCA plot, which makes use of the ggplot2 graphics package.\n\nplotPCA(vsd,intgroup=\"Treated\")\n\nusing ntop=500 top features by variance\n\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\nℹ The deprecated feature was likely used in the DESeq2 package.\n  Please report the issue to the authors.\n\n\n\n\n\n\n\n\n\nThere is also an option to return the values used in the plot for further exploration and customisation. The ggplot2 package is a natural choice for plotting.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nplotPCA(vsd,intgroup=\"Treated\",returnData = TRUE) %&gt;% \n  ggplot(aes(x = PC1, y = PC2,col=group)) + geom_point(size=3)\n\nusing ntop=500 top features by variance\n\n\n\n\n\n\n\n\n\nThe PCA shown above is coloured according to the Treated status of each sample. An interpretation of this plot would be that samples separate cleanly based on Treated on the y-axis (PC2), with the non-treated samples (coloured red) having y coordinates around 10 and treated samples (coloured blue) having y coordinate around -5.\nHowever, the PCA method definition means that main source of variance cannot be explained by the Treated variable. Along the x-axis (PC1) we see three distinct groups. You would hope that these correspond to the condition, and we will assess this in the following exercise."
  },
  {
    "objectID": "training/bulk-rnaseq_1/index.html#exercise-1",
    "href": "training/bulk-rnaseq_1/index.html#exercise-1",
    "title": "Introduction to RNA-Seq - Part 1",
    "section": "Exercise",
    "text": "Exercise\n\n\nIs the plotPCA plot based on all genes in the dataset? How can we change how many genes are used for the PCA analysis? Does this significantly change the plot? (HINT: check the documentation for the plotPCA function.)\nVerify that the samples are separated based on the condition.\nWhat problems can you see with the metadata?\nCan you label the identify of each sample? Look for help on geom_textif you haven’t used it before\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe plotPCA function has an argument ntop which is used to specify how many genes are used to perform the PCA - so it is based on a limited set of genes in the data. The genes used are those with the highest variance.\nTo make a PCA based on condition we need to alter the intgroup argument\n\nplotPCA(vsd,intgroup=\"condition\",returnData = TRUE) %&gt;% \n  ggplot(aes(x = PC1, y = PC2,col=condition)) + geom_point()\n\nusing ntop=500 top features by variance\n\n\n\n\n\n\n\n\n\nThis almost shows what we expect, but there are far too many colours used in the plot. This is because of discrepancies in the notation used for condition.\nIn order for geom_text to work you need to include an aesthetic mapping label in the initial ggplot call.\n\nplotPCA(vsd,intgroup=\"condition\",returnData = TRUE) %&gt;% \n  ggplot(aes(x = PC1, y = PC2,col=condition, label=Name)) + geom_point() + geom_text()\n\nusing ntop=500 top features by variance\n\n\n\n\n\n\n\n\n\nHowever, the placement of the labels is sometimes not very satisfactory. Better placement can be achieved by using the ggrepel package. The geom_text_repel function can then be used instead of geom_text.\n\nif(!require(ggrepel)) install.packages(\"ggrepel\")\n\nLoading required package: ggrepel\n\nplotPCA(vsd,intgroup=\"condition\",returnData = TRUE) %&gt;% \n  ggplot(aes(x = PC1, y = PC2,col=condition, label=Name)) + geom_point() + ggrepel::geom_text_repel()\n\nusing ntop=500 top features by variance"
  },
  {
    "objectID": "training/bulk-rnaseq_1/index.html#note-about-batch-effects",
    "href": "training/bulk-rnaseq_1/index.html#note-about-batch-effects",
    "title": "Introduction to RNA-Seq - Part 1",
    "section": "Note about batch effects",
    "text": "Note about batch effects\nIn our unsupervised analysis we should see that the main source of variation is due to biological effects, and not technical variation such as when the libraries were sequenced. If we do observe high technical variation in our data, it is not a complete disaster provided that we have designed our experiment properly. In particular the sva Bioconductor package can correct for batch effects provided that representatives of the groups of interest appear in each batch. Alternatively, the batch or confounding factor may be incorporated into the differential expression analysis.\nBelow is an example of a PCA that might potentially worrying:-"
  },
  {
    "objectID": "training/bulk-rnaseq_1/index.html#correcting-the-sample-information",
    "href": "training/bulk-rnaseq_1/index.html#correcting-the-sample-information",
    "title": "Introduction to RNA-Seq - Part 1",
    "section": "Correcting the sample information",
    "text": "Correcting the sample information\nThe person creating the sample sheet has been inconsistent about the way that values of Treated have been entered into the metadata. Such errors can be annoying when labeling plots, but have more serious consequences when attempting to fit statistical models to the data.\nHere are a set of commands to create an updated sample sheet using the stringr package (part of tidyverse). We write a new file rather than over-writing the existing one.\n\nlibrary(stringr)\nlibrary(dplyr)\nsampleInfo %&gt;% \n  mutate(condition = str_to_upper(condition)) %&gt;% \n  readr::write_tsv(file=\"meta_data/sampleInfo_corrected.txt\")\n\n\nExercise\n\nRe-create the DESeqDataset object to include the corrected sample information\nRe-run the plotPCA function on the new data and verify that the sample groups now look correct\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe only actually need to change the code to read the meta data - the rest of the code is identical.\n\nsampleinfo_corrected &lt;- read.delim(\"meta_data/sampleInfo_corrected.txt\")\ndds &lt;- DESeqDataSetFromMatrix(counts, \n                                colData = sampleinfo_corrected,\n                                design = ~condition, tidy=TRUE)\n\nWarning in DESeqDataSet(se, design = design, ignoreRank): some variables in\ndesign formula are characters, converting to factors\n\nvsd &lt;- vst(dds)\n\nplotPCA(vsd, intgroup = \"condition\")\n\n\n\n\n\n\n\n\nNotice that the only that has changed in the PCA is the colouring of the points. As our counts are unchanged, the coordinates on the PCA remain the same.\n\n\n\nThe PCA produced when we used the corrected meta data gives us some confidence that our experiment has worked. Although it is tempting, we shouldn’t blindly correct errors in our meta data based on what we see on a PCA. Some detective work is required to make sure that the sample labeling is truly a mistake. Perhaps contact the lab personnel who processed the samples and review their lab notebook/LIMS records for the sample’s journey: RNA extraction -&gt; library preparation -&gt; index assignment.\n\n\n\n\n\n\n\n\n\nSample swaps can also happen during the indexing/barcoding step of library preparation. The sample might have physically clustered with the intended group, but the sequencing machine labeled it with the wrong index.\nPCA is a critical diagnostic and exploratory tool that informs and guides the inferential analysis, not just a simple visualization technique. However, it is a descriptive technique, not an inferential one. In other words, it cannot actually be used to prove anything about your data such as a biological hypothesis. This will come in the next section when we look at differential expression.\nAnyway, now that we are happy with the pre-processing we will save our DESeq object to disk so we can start from this point\n\ndir.create(\"Robjects\", showWarnings = FALSE)\nsaveRDS(dds, \"Robjects/dds.rds\")\n\n\n\nWhat about tSNE and UMAP? Aren’t these the modern methods for visualising RNA-seq\nSort of. These are methods recommended for single-cell RNA-seq which are much sparser (have many 0 counts), more complex, and contains distinct cell types that often have non-linear relationships. For bulk RNA-seq though good old PCA is perfectly adequate as an exploratory method for visualising variation in your data\nIf you wish to explore PCA in a bit more detail, expand the next section.\n\n\n\n\n\n\nGreater control over PCA\n\n\n\n\n\nThe plotPCA function is perfectly serviceable and gives a good overview of your data. However, sometimes you might want a bit more flexibility. Whilst not wishing to cover PCA in great depth we can go over the main steps.\nFirstly get the transformed VST values as a matrix\n\nvst_values &lt;- assay(vsd)\nhead(vst_values)\n\n                1_CTR_BC_2 2_TGF_BC_4 3_IR_BC_5 4_CTR_BC_6 5_TGF_BC_7\nENSG00000000003  10.611890  10.428009 10.617695  10.660566  10.445435\nENSG00000000005   5.107359   5.107359  5.107359   5.107359   5.107359\nENSG00000000419  10.773025  10.617265 11.075708  10.743334  10.887778\nENSG00000000457   9.509690   9.202704  9.532294   9.520157   9.444276\nENSG00000000460   8.219870   8.352648  7.590207   8.067853   8.264558\nENSG00000000938   5.855476   5.610337  5.957485   5.677762   5.346678\n                6_IR_BC_12 7_CTR_BC_13 8_TGF_BC_14 9_IR_BC_15\nENSG00000000003  10.582573   10.737816   10.404334  10.414804\nENSG00000000005   5.107359    5.107359    5.107359   5.355472\nENSG00000000419  11.054746   10.937878   10.512279  11.030753\nENSG00000000457   9.429903    9.746221    9.285840   9.469935\nENSG00000000460   7.553175    8.337201    8.396371   7.589312\nENSG00000000938   5.480466    6.181764    5.581574   5.803230\n\n\nWe now select the “most variable” features in the dataset by firstly using the rowVars function to calculate the variance of each row (“gene”) and ordering the result from largest to smallest. The result of this are row indices from the most variable to least variable. The ntop (e.g. 500) most variable genes can then be created by subsetting the row_var_order vector.\n\nntop &lt;- 500\n\nrow_var_order &lt;- rowVars(vst_values) %&gt;% order(decreasing = TRUE)\n\n\nrow_var_order[1]\n\n[1] 6715\n\n### Looks to be highly variable\nassay(vsd)[row_var_order[1],]\n\n 1_CTR_BC_2  2_TGF_BC_4   3_IR_BC_5  4_CTR_BC_6  5_TGF_BC_7  6_IR_BC_12 \n  10.527271   14.796726    9.922513   10.589045   14.928894   10.834244 \n7_CTR_BC_13 8_TGF_BC_14  9_IR_BC_15 \n  11.368250   14.375299   10.114217 \n\n## not variable at all\nassay(vsd)[row_var_order[length(row_var_order)],]\n\n 1_CTR_BC_2  2_TGF_BC_4   3_IR_BC_5  4_CTR_BC_6  5_TGF_BC_7  6_IR_BC_12 \n   5.107359    5.107359    5.107359    5.107359    5.107359    5.107359 \n7_CTR_BC_13 8_TGF_BC_14  9_IR_BC_15 \n   5.107359    5.107359    5.107359 \n\n## Get the 'ntop' most variable rows\nmost_var_rows &lt;- row_var_order[1:ntop]\n\nThe R function prcomp is used to perform the PCA but we need to “transpose” the data (using the t function) otherwise the function will attempt to do PCA on the genes rather than our samples. Scaling is also recommended\n\npca_results &lt;- prcomp(t(vst_values[most_var_rows,]),scale = TRUE)\n\nnames(pca_results)\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n\nWithout digging too much into the meaning of the resulting object, the % of variance explained (as seen on the plotPCA output) is calculated using the formula pca_results$sdev^2 / sum(pca_results$sdev^2)*100. These can then be plotted on a bar plot and due to the nature of the PCA method the variance should decrease rapidly.\n\nhttps://scienceparkstudygroup.github.io/rna-seq-lesson/05-descriptive-plots/index.html\n\n\nvar_explained &lt;- pca_results$sdev^2 / sum(pca_results$sdev^2)*100\n\ndata.frame(PC = 1:9, var_explained) %&gt;% \n  ggplot(aes(y = var_explained, x =PC)) + geom_col(fill=\"steelblue\")\n\n\n\n\n\n\n\n\nThe values of the principal components can be found in the conveniently-named (!) x slot of the output. The dimensions of this matrix is 9 x 9 as we have 9 biological samples and hence 9 Principal Components\n\npca_results$x\n\n                    PC1       PC2        PC3       PC4        PC5         PC6\n1_CTR_BC_2   -4.6468927 10.989092 -6.3812748 -1.042373  0.1213624  1.65953033\n2_TGF_BC_4  -20.5235074 -8.365185  0.3891126 -1.496022  1.0603908  2.40505458\n3_IR_BC_5    22.0295945 -4.381020 -1.8413407 -1.325310 -8.0046029  4.10182976\n4_CTR_BC_6   -3.9922398 11.146824 -6.0463553 -3.144462  0.1002104 -4.40687268\n5_TGF_BC_7  -18.5278032 -7.460858  4.6149728 -0.903531 -4.6083436 -5.81192771\n6_IR_BC_12   20.0156744 -3.158754  5.2006785 -7.561357  5.5530510  0.03124602\n7_CTR_BC_13  -0.4709555 14.521663  9.9477207  4.780599 -0.3422440  1.77850878\n8_TGF_BC_14 -16.4022721 -6.462752 -2.9120981  3.155782  3.4799806  4.20899313\n9_IR_BC_15   22.5184017 -6.829011 -2.9714155  7.536674  2.6401953 -3.96636221\n                   PC7        PC8          PC9\n1_CTR_BC_2   5.4530609  4.0388908 1.151856e-14\n2_TGF_BC_4  -5.1314480  4.6771990 5.186823e-15\n3_IR_BC_5   -0.9596799 -1.0912456 1.425249e-14\n4_CTR_BC_6  -4.5084323 -3.0518958 1.183775e-14\n5_TGF_BC_7   3.4990387 -0.4737049 5.900662e-15\n6_IR_BC_12   1.4423047 -0.4688147 1.668110e-14\n7_CTR_BC_13 -1.1907042 -0.1826762 1.331574e-14\n8_TGF_BC_14  1.8721518 -5.2613527 7.879114e-15\n9_IR_BC_15  -0.4762919  1.8136002 1.364187e-14\n\n\nThese PC values can be combined with the (corrected) sample information and used to make a familiar plot\n\npca_plot &lt;- pca_results$x %&gt;% \n  bind_cols(sampleinfo_corrected) %&gt;% \n  ggplot(aes(x = PC1, y = PC2, col = condition)) + geom_point()\n\npca_plot\n\n\n\n\n\n\n\n\nThe percentage of variance explained can be added as a label on the x- and y- axes.\n\nxlabel &lt;- paste0(\"PC1(\", round(var_explained[1],1), \"%)\")\nxlabel\n\n[1] \"PC1(61.6%)\"\n\nylabel &lt;- paste0(\"PC2(\", round(var_explained[2],1), \"%)\")\nylabel\n\n[1] \"PC2(17.5%)\"\n\npca_plot + xlab(xlabel) + ylab(xlabel)\n\n\n\n\n\n\n\n\nIf your data are complex and involve multiple experimental factors and conditions you may wish to expand the PCA beyond the first two components. This may show more subtle relationships between your samples. Here we show PC1 and PC3.\n\npca_results$x %&gt;% \n  bind_cols(sampleinfo_corrected) %&gt;% \n  ggplot(aes(x = PC1, y = PC3, col = condition)) + geom_point()"
  },
  {
    "objectID": "training/bulk-rnaseq_1/index.html#solution-2",
    "href": "training/bulk-rnaseq_1/index.html#solution-2",
    "title": "Introduction to RNA-Seq - Part 1",
    "section": "Solution",
    "text": "Solution\n\nsampleinfo_corrected &lt;- read.delim(\"meta_data/sampleInfo_corrected.txt\")\ndds &lt;- DESeqDataSetFromMatrix(counts, \n                                colData = sampleinfo_corrected,\n                                design = ~condition, tidy=TRUE)\n\nWarning in DESeqDataSet(se, design = design, ignoreRank): some variables in\ndesign formula are characters, converting to factors\n\ndds\n\nclass: DESeqDataSet \ndim: 57914 9 \nmetadata(1): version\nassays(1): counts\nrownames(57914): ENSG00000000003 ENSG00000000005 ... ENSG00000284747\n  ENSG00000284748\nrowData names(0):\ncolnames(9): X1_CTR_BC_2 X2_TGF_BC_4 ... X8_TGF_BC_14 X9_IR_BC_15\ncolData names(5): Run condition Name Replicate Treated\n\nvsd &lt;- vst(dds)\n\nplotPCA(vsd, intgroup = \"condition\")\n\nusing ntop=500 top features by variance\n\n\n\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "training/bulk-rnaseq_2/index.html",
    "href": "training/bulk-rnaseq_2/index.html",
    "title": "Introduction to RNA-Seq - Part 2",
    "section": "",
    "text": "Performing differential expression on Bulk RNA-seq data using DESeq2\n\n\nThis section follows on from Part 1 where we saw how to import raw RNA-seq counts into DESeq2 and perform some quality assessment. Several packages are required, which can be downloaded with this code:-\n\nsource(\"https://raw.githubusercontent.com/markdunning/markdunning.github.com/refs/heads/master/files/training/bulk_rnaseq/install_bioc_packages.R\")\n\nThe following will also assume you have created a DESeq2 object in a folder called Robjects in your working directory. This can be downloaded with the following.\n\ndir.create(\"Robjects/\",showWarnings = FALSE)\ndownload.file(\"https://github.com/markdunning/markdunning.github.com/raw/refs/heads/master/files/training/bulk_rnaseq/dds.rds\",destfile = \"Robjects/dds.rds\")"
  },
  {
    "objectID": "training/bulk-rnaseq_2/index.html#quick-start",
    "href": "training/bulk-rnaseq_2/index.html#quick-start",
    "title": "Introduction to RNA-Seq - Part 2",
    "section": "",
    "text": "This section follows on from Part 1 where we saw how to import raw RNA-seq counts into DESeq2 and perform some quality assessment. Several packages are required, which can be downloaded with this code:-\n\nsource(\"https://raw.githubusercontent.com/markdunning/markdunning.github.com/refs/heads/master/files/training/bulk_rnaseq/install_bioc_packages.R\")\n\nThe following will also assume you have created a DESeq2 object in a folder called Robjects in your working directory. This can be downloaded with the following.\n\ndir.create(\"Robjects/\",showWarnings = FALSE)\ndownload.file(\"https://github.com/markdunning/markdunning.github.com/raw/refs/heads/master/files/training/bulk_rnaseq/dds.rds\",destfile = \"Robjects/dds.rds\")"
  },
  {
    "objectID": "training/bulk-rnaseq_2/index.html#recap-of-pre-processing",
    "href": "training/bulk-rnaseq_2/index.html#recap-of-pre-processing",
    "title": "Introduction to RNA-Seq - Part 2",
    "section": "Recap of pre-processing",
    "text": "Recap of pre-processing\nThe previous section walked-through the pre-processing and transformation of the count data. Here, for completeness, we list the minimal steps required to process the data prior to differential expression analysis.\nNote that although we spent some time looking at the quality of our data , these steps are not strictly necessary prior to performing differential expression so are not shown here for the sake of brevity. Remember, DESeq2 requires raw counts so the vst transformation is not shown as part of this basic protocol.\n\nlibrary(DESeq2)\n\ncount_file &lt;- \"raw_counts/raw_counts_matrix.tsv\"\ncounts &lt;- read.delim(count_file)\n\n## Step needed for this data to tidy the column names\ncolnames(counts)[-1] &lt;- stringr::str_remove_all(colnames(counts)[-1], \"X\")\n\nsampleinfo_corrected &lt;- read.delim(\"meta_data/sampleInfo_corrected.txt\")\ndds &lt;- DESeqDataSetFromMatrix(counts, \n                                colData = sampleinfo_corrected,\n                                design = ~condition, tidy=TRUE)\n\nsaveRDS(dds, file=\"Robjects/dds.rds\")\n\nWe will be using these raw counts throughout the workshop and transforming them using methods in the DESeq2 package. If you want to know about alternative methods for count normalisation they are covered on this page."
  },
  {
    "objectID": "training/bulk-rnaseq_2/index.html#exercise",
    "href": "training/bulk-rnaseq_2/index.html#exercise",
    "title": "Introduction to RNA-Seq - Part 2",
    "section": "Exercise",
    "text": "Exercise\n\nRe-run the analysis to find differentially-expressed genes between the IR treated samples and CTR\nWrite a csv file that contains results for the genes that have an adjusted p-value less than 0.05 and a log2 fold change more than 1, or less than -1 in the contrast of TGF vs CTRL.\nUse the plotCounts function to visually-inspect the most statistically-significant gene identified\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n## Like to run the `results` function without `tidy=TRUE` to check everything is working\nresults(de_condition, contrast = c(\"condition\", \"IR\", \"CTR\"))\n\nlog2 fold change (MLE): condition IR vs CTR \nWald test p-value: condition IR vs CTR \nDataFrame with 57914 rows and 6 columns\n                   baseMean log2FoldChange     lfcSE      stat      pvalue\n                  &lt;numeric&gt;      &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;   &lt;numeric&gt;\nENSG00000000003 1430.562846      -0.136193 0.0817196 -1.666593 9.55953e-02\nENSG00000000005    0.113566       1.143864 4.0804559  0.280328 7.79226e-01\nENSG00000000419 1790.537536       0.241934 0.1006649  2.403361 1.62451e-02\nENSG00000000457  640.692302      -0.128599 0.1140639 -1.127428 2.59561e-01\nENSG00000000460  206.179026      -0.850453 0.1675739 -5.075092 3.87308e-07\n...                     ...            ...       ...       ...         ...\nENSG00000284744    8.307038       0.843943  0.772014  1.093171    0.274319\nENSG00000284745    0.000000             NA        NA        NA          NA\nENSG00000284746    0.101097      -0.779681  4.080456 -0.191077    0.848465\nENSG00000284747   28.783710      -0.391963  0.456290 -0.859021    0.390329\nENSG00000284748    0.548323       2.942851  3.977407  0.739892    0.459366\n                       padj\n                  &lt;numeric&gt;\nENSG00000000003 2.18208e-01\nENSG00000000005          NA\nENSG00000000419 5.32619e-02\nENSG00000000457 4.47487e-01\nENSG00000000460 3.99611e-06\n...                     ...\nENSG00000284744          NA\nENSG00000284745          NA\nENSG00000284746          NA\nENSG00000284747    0.586086\nENSG00000284748          NA\n\nresults(de_condition, contrast = c(\"condition\", \"IR\", \"CTR\"), tidy = TRUE) %&gt;% \n  arrange(padj) %&gt;% \n  filter(padj &lt; 0.05, abs(log2FoldChange) &gt; 1) %&gt;% \n  readr::write_csv(\"de_analysis/condition_IR_vs_CTR_sig_genes.csv\")"
  },
  {
    "objectID": "training/bulk-rnaseq_2/index.html#exercise-1",
    "href": "training/bulk-rnaseq_2/index.html#exercise-1",
    "title": "Introduction to RNA-Seq - Part 2",
    "section": "Exercise",
    "text": "Exercise\n\nThe publication gives examples of COL1A1, COL1A2 and COL3A1 as genes that are up-regulated in TGF-treated samples vs controls (Figure 6C). Use your data to verify this by\n\nextracting their p-values\nplotting the counts for these genes"
  },
  {
    "objectID": "training/bulk-rnaseq_2/index.html#the-volcano-plot",
    "href": "training/bulk-rnaseq_2/index.html#the-volcano-plot",
    "title": "Introduction to RNA-Seq - Part 2",
    "section": "The volcano plot",
    "text": "The volcano plot\n\nlibrary(EnhancedVolcano)\n\nLoading required package: ggplot2\n\n\nLoading required package: ggrepel\n\nEnhancedVolcano(results_annotated, x = \"log2FoldChange\", y = \"padj\", lab = results_annotated$SYMBOL)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nℹ The deprecated feature was likely used in the EnhancedVolcano package.\n  Please report the issue to the authors.\n\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\nℹ The deprecated feature was likely used in the EnhancedVolcano package.\n  Please report the issue to the authors.\n\n\n\n\n\n\n\n\n\n\nExporting normalized counts\nThe DESeq workflow applies median of ratios normalization that accounts for differences in sequencing depth between samples. The user does not usually need to run this step. However, if you want a matrix of counts for some application outside of Bioconductor the values can be extracted from the dds object.\n\ndds &lt;- estimateSizeFactors(dds) \ncountMatrix &lt;-counts(dds, normalized=TRUE) \nhead(countMatrix)\n\n                 1_CTR_BC_2  2_TGF_BC_4  3_IR_BC_5  4_CTR_BC_6   5_TGF_BC_7\nENSG00000000003 1496.753711 1309.582900 1503.06012 1550.428865 1326.3138781\nENSG00000000005    0.000000    0.000000    0.00000    0.000000    0.0000000\nENSG00000000419 1681.596633 1502.591885 2089.94798 1645.965855 1826.4150250\nENSG00000000457  661.642869  522.309405  673.12901  666.939177  629.4048655\nENSG00000000460  233.186455  261.577968  129.92174  203.812245  242.4444723\nENSG00000000938    9.479124    4.232653   12.32016    5.459257    0.9507626\n                 6_IR_BC_12 7_CTR_BC_13 8_TGF_BC_14  9_IR_BC_15\nENSG00000000003 1465.288919  1639.42512 1287.173576 1297.038521\nENSG00000000005    0.000000     0.00000    0.000000    1.022095\nENSG00000000419 2058.823670  1893.34636 1392.402365 2023.748047\nENSG00000000457  622.515941   791.26495  557.148855  641.875643\nENSG00000000460  125.198737   258.13570  271.527857  129.806062\nENSG00000000938    2.318495    20.01869    3.758171    8.176760\n\nwrite.csv(countMatrix,file=\"normalized_counts.csv\")"
  },
  {
    "objectID": "posts/2025_10_23_10X_spatial_from_GEO/index.html",
    "href": "posts/2025_10_23_10X_spatial_from_GEO/index.html",
    "title": "Reading 10X Spatial Data from GEO into R",
    "section": "",
    "text": "Lots of spatial transcriptomics data is becoming available in R, but how do you load it into R for analysis? I will discuss an example of a dataset uploaded to Gene Expression Omnibus\n\n\nThe dataset I will be looking comes from Spatial single cell analysis of tumor microenvironment remodeling pattern in primary central nervous system lymphoma\nHere is an overview of the study.\n\nIf we are interested in tumour microenvironment we might want to explore the dataset for ourselves and test some hypotheses. The data are available in Gene Expression Onmibus (GEO), but unfortunately not in a form that we can immediately use.\n\nThe GEO entry for the dataset\n\n\n\n\nThe following packages will handle the download and re-organisation of the data\n\nif(!require(\"BiocManager\")) install.packages(\"BiocManager\")\nBiocManager::install(\"GEOquery\")\ninstall.packages(\"R.utils\")\n\nOur eventual aim will be to load the data into Seurat, so if you want to start looking at the data you can download this too.\n\ninstall.packages(\"Seurat\")\n\n\n\n\nThe GEOquery package has long been a favourite of mine for downloading data from GEO. However, don’t get too excited because it was developed during the days of microarrays and has limited functionality for sequencing datasets. The getGEO function will download some meta data about this dataset.\n\nlibrary(GEOquery)\ngeo &lt;- getGEO(\"GSE230207\")\ngeo\n\n$GSE230207_series_matrix.txt.gz\nExpressionSet (storageMode: lockedEnvironment)\nassayData: 0 features, 4 samples \n  element names: exprs \nprotocolData: none\nphenoData\n  sampleNames: GSM7192449 GSM7192450 GSM7192451 GSM7192453\n  varLabels: title geo_accession ... tissue:ch1 (43 total)\n  varMetadata: labelDescription\nfeatureData: none\nexperimentData: use 'experimentData(object)'\n  pubMedIds: 37120690 \nAnnotation: GPL18573 \n\n\nSince getGEO is also capable of downloading datasets that comprise multiple different technologies, the result is in the form a list. The particular dataset we want only have one type of data (spatial transcriptomics), so we subset the first item in the list. We can access the meta, or phenotypic data, using the pData function.\n\nmeta &lt;- pData(geo[[1]])\nmeta[,1:5]\n\n           title geo_accession                status submission_date\nGSM7192449   hot    GSM7192449 Public on May 01 2023     Apr 20 2023\nGSM7192450  cold    GSM7192450 Public on May 01 2023     Apr 20 2023\nGSM7192451   IME    GSM7192451 Public on May 01 2023     Apr 20 2023\nGSM7192453   IMS    GSM7192453 Public on May 01 2023     Apr 20 2023\n           last_update_date\nGSM7192449      May 08 2024\nGSM7192450      May 08 2024\nGSM7192451      May 08 2024\nGSM7192453      May 08 2024\n\n\nThe data we are interested have been uploaded as supplementary material, and conveniently GEOquery has a function for downloading these data. Depending on the speed of your network connection this may take a while. If the download fails, you might need to increase the timeout option in R.\n\n# You may need to un-comment the next line to increase the download timeout\n#options(timeout = 10000)\n\ngetGEOSuppFiles(\"GSE230207\")\n\nYou should now have a file called GSE230207_RAW.tar in a folder called GSE230207. This tar file is essentially a way of transferring a dataset consisting many separate files into a single download.\n\n\n\n\n\n\nNote\n\n\n\nThe name tar is short for Tape ARchive and harks back to the early days of computing.\nThe format and its associated command-line utility were originally developed for archiving files onto magnetic tape drives on Unix systems in the early days of computing.\nAlthough its original purpose was tape backup, the tar utility is now widely used on Unix-like systems (like Linux) as a general-purpose archive format (often called a tarball) to bundle multiple files and directories into a single file for distribution, backup, and transport, regardless of the storage medium.\n\n\n\nfile.exists(\"GSE230207/GSE230207_RAW.tar\")\n\n[1] TRUE\n\n\nR has a built-in function, untar for extracting all the files contained in the archive to a folder of our choosing (exdir). We’ll just extract to the current working directory. After untar has finished we can list the contents of the working directory.\n\nuntar(\"GSE230207/GSE230207_RAW.tar\", exdir = \".\")\nlist.files(\"./\")\n\nYou’ll notice that lots of files have appeared, and futhermore they are prefixed by the GEO IDs that we discovered using GEOquery (GSM7192449, GSM7192450, GSM7192451 and GSM7192452)\nWith list.files we can list everything that contains one of these IDs and notice that the files are named very predictably. This means for a given GEO ID we know what files to expect, which will be helpful to organise our files in a way that Seurat expects.\n\nlist.files(pattern = \"GSM7192449\")\n\n[1] \"GSM7192449_aligned_fiducials.jpg.gz\"    \n[2] \"GSM7192449_detected_tissue_image.jpg.gz\"\n[3] \"GSM7192449_hot.h5\"                      \n[4] \"GSM7192449_scalefactors_json.json.gz\"   \n[5] \"GSM7192449_tissue_hires_image.png.gz\"   \n[6] \"GSM7192449_tissue_lowres_image.png.gz\"  \n[7] \"GSM7192449_tissue_positions_list.csv.gz\"\n\n\n\nlist.files(pattern = \"GSM7192450\")\n\n[1] \"GSM7192450_aligned_fiducials.jpg.gz\"    \n[2] \"GSM7192450_cold.h5\"                     \n[3] \"GSM7192450_detected_tissue_image.jpg.gz\"\n[4] \"GSM7192450_scalefactors_json.json.gz\"   \n[5] \"GSM7192450_tissue_hires_image.png.gz\"   \n[6] \"GSM7192450_tissue_lowres_image.png.gz\"  \n[7] \"GSM7192450_tissue_positions_list.csv.gz\"\n\n\n\n\n\nOur eventual goal is to use a function called Load10X_Spatial in the Seurat package, which data to be organised and named in a very specific manner. For a sample called Sample_1 it should look like this:-\nSample_1\n├── filtered_feature_bc_matrix.h5/ \n└── spatial/                   \n    ├── tissue_hires_image.png\n    ├── tissue_lowres_image.png\n    ├── scalefactors_json.json\n    └── tissue_positions_list.csv\n    └── ... + other images files if required\n    \n\nIn other words, we need to create a separate folder for each of the four samples named according to the sample name. The “h5” file should be placed here and all other image data should be in a folder named spatial. As the IDs created by GEO are completely arbitrary and not related to the underlying question we will rename to some relating to the sample groups. This labels for this are found in the title column of the meta data."
  },
  {
    "objectID": "posts/2025_10_23_10X_spatial_from_GEO/index.html#the-data",
    "href": "posts/2025_10_23_10X_spatial_from_GEO/index.html#the-data",
    "title": "Reading 10X Spatial Data from GEO into R",
    "section": "",
    "text": "The dataset I will be looking comes from Spatial single cell analysis of tumor microenvironment remodeling pattern in primary central nervous system lymphoma\nHere is an overview of the study.\n\nIf we are interested in tumour microenvironment we might want to explore the dataset for ourselves and test some hypotheses. The data are available in Gene Expression Onmibus (GEO), but unfortunately not in a form that we can immediately use.\n\nThe GEO entry for the dataset"
  },
  {
    "objectID": "posts/2025_10_23_10X_spatial_from_GEO/index.html#packages-required",
    "href": "posts/2025_10_23_10X_spatial_from_GEO/index.html#packages-required",
    "title": "Reading 10X Spatial Data from GEO into R",
    "section": "",
    "text": "The following packages will handle the download and re-organisation of the data\n\nif(!require(\"BiocManager\")) install.packages(\"BiocManager\")\nBiocManager::install(\"GEOquery\")\ninstall.packages(\"R.utils\")\n\nOur eventual aim will be to load the data into Seurat, so if you want to start looking at the data you can download this too.\n\ninstall.packages(\"Seurat\")"
  },
  {
    "objectID": "posts/2025_10_23_10X_spatial_from_GEO/index.html#downloading-using-geoquery",
    "href": "posts/2025_10_23_10X_spatial_from_GEO/index.html#downloading-using-geoquery",
    "title": "Reading 10X Spatial Data from GEO into R",
    "section": "",
    "text": "The GEOquery package has long been a favourite of mine for downloading data from GEO. However, don’t get too excited because it was developed during the days of microarrays and has limited functionality for sequencing datasets. The getGEO function will download some meta data about this dataset.\n\nlibrary(GEOquery)\ngeo &lt;- getGEO(\"GSE230207\")\ngeo\n\n$GSE230207_series_matrix.txt.gz\nExpressionSet (storageMode: lockedEnvironment)\nassayData: 0 features, 4 samples \n  element names: exprs \nprotocolData: none\nphenoData\n  sampleNames: GSM7192449 GSM7192450 GSM7192451 GSM7192453\n  varLabels: title geo_accession ... tissue:ch1 (43 total)\n  varMetadata: labelDescription\nfeatureData: none\nexperimentData: use 'experimentData(object)'\n  pubMedIds: 37120690 \nAnnotation: GPL18573 \n\n\nSince getGEO is also capable of downloading datasets that comprise multiple different technologies, the result is in the form a list. The particular dataset we want only have one type of data (spatial transcriptomics), so we subset the first item in the list. We can access the meta, or phenotypic data, using the pData function.\n\nmeta &lt;- pData(geo[[1]])\nmeta[,1:5]\n\n           title geo_accession                status submission_date\nGSM7192449   hot    GSM7192449 Public on May 01 2023     Apr 20 2023\nGSM7192450  cold    GSM7192450 Public on May 01 2023     Apr 20 2023\nGSM7192451   IME    GSM7192451 Public on May 01 2023     Apr 20 2023\nGSM7192453   IMS    GSM7192453 Public on May 01 2023     Apr 20 2023\n           last_update_date\nGSM7192449      May 08 2024\nGSM7192450      May 08 2024\nGSM7192451      May 08 2024\nGSM7192453      May 08 2024\n\n\nThe data we are interested have been uploaded as supplementary material, and conveniently GEOquery has a function for downloading these data. Depending on the speed of your network connection this may take a while. If the download fails, you might need to increase the timeout option in R.\n\n# You may need to un-comment the next line to increase the download timeout\n#options(timeout = 10000)\n\ngetGEOSuppFiles(\"GSE230207\")\n\nYou should now have a file called GSE230207_RAW.tar in a folder called GSE230207. This tar file is essentially a way of transferring a dataset consisting many separate files into a single download.\n\n\n\n\n\n\nNote\n\n\n\nThe name tar is short for Tape ARchive and harks back to the early days of computing.\nThe format and its associated command-line utility were originally developed for archiving files onto magnetic tape drives on Unix systems in the early days of computing.\nAlthough its original purpose was tape backup, the tar utility is now widely used on Unix-like systems (like Linux) as a general-purpose archive format (often called a tarball) to bundle multiple files and directories into a single file for distribution, backup, and transport, regardless of the storage medium.\n\n\n\nfile.exists(\"GSE230207/GSE230207_RAW.tar\")\n\n[1] TRUE\n\n\nR has a built-in function, untar for extracting all the files contained in the archive to a folder of our choosing (exdir). We’ll just extract to the current working directory. After untar has finished we can list the contents of the working directory.\n\nuntar(\"GSE230207/GSE230207_RAW.tar\", exdir = \".\")\nlist.files(\"./\")\n\nYou’ll notice that lots of files have appeared, and futhermore they are prefixed by the GEO IDs that we discovered using GEOquery (GSM7192449, GSM7192450, GSM7192451 and GSM7192452)\nWith list.files we can list everything that contains one of these IDs and notice that the files are named very predictably. This means for a given GEO ID we know what files to expect, which will be helpful to organise our files in a way that Seurat expects.\n\nlist.files(pattern = \"GSM7192449\")\n\n[1] \"GSM7192449_aligned_fiducials.jpg.gz\"    \n[2] \"GSM7192449_detected_tissue_image.jpg.gz\"\n[3] \"GSM7192449_hot.h5\"                      \n[4] \"GSM7192449_scalefactors_json.json.gz\"   \n[5] \"GSM7192449_tissue_hires_image.png.gz\"   \n[6] \"GSM7192449_tissue_lowres_image.png.gz\"  \n[7] \"GSM7192449_tissue_positions_list.csv.gz\"\n\n\n\nlist.files(pattern = \"GSM7192450\")\n\n[1] \"GSM7192450_aligned_fiducials.jpg.gz\"    \n[2] \"GSM7192450_cold.h5\"                     \n[3] \"GSM7192450_detected_tissue_image.jpg.gz\"\n[4] \"GSM7192450_scalefactors_json.json.gz\"   \n[5] \"GSM7192450_tissue_hires_image.png.gz\"   \n[6] \"GSM7192450_tissue_lowres_image.png.gz\"  \n[7] \"GSM7192450_tissue_positions_list.csv.gz\""
  },
  {
    "objectID": "posts/2025_10_23_10X_spatial_from_GEO/index.html#fun-fact",
    "href": "posts/2025_10_23_10X_spatial_from_GEO/index.html#fun-fact",
    "title": "Reading 10X Spatial Data from GEO into R",
    "section": "",
    "text": "The name tar is short for Tape ARchive and harks back to the early days of computing.\nThe format and its associated command-line utility were originally developed for archiving files onto magnetic tape drives on Unix systems in the early days of computing.\nAlthough its original purpose was tape backup, the tar utility is now widely used on Unix-like systems (like Linux) as a general-purpose archive format (often called a tarball) to bundle multiple files and directories into a single file for distribution, backup, and transport, regardless of the storage medium."
  },
  {
    "objectID": "posts/2025_10_23_10X_spatial_from_GEO/index.html#folder-structure-expected-by-seurat",
    "href": "posts/2025_10_23_10X_spatial_from_GEO/index.html#folder-structure-expected-by-seurat",
    "title": "Reading 10X Spatial Data from GEO into R",
    "section": "",
    "text": "Our eventual goal is to use a function called Load10X_Spatial in the Seurat package, which data to be organised and named in a very specific manner. For a sample called Sample_1 it should look like this:-\nSample_1\n├── filtered_feature_bc_matrix.h5/ \n└── spatial/                   \n    ├── tissue_hires_image.png\n    ├── tissue_lowres_image.png\n    ├── scalefactors_json.json\n    └── tissue_positions_list.csv\n    └── ... + other images files if required\n    \n\nIn other words, we need to create a separate folder for each of the four samples named according to the sample name. The “h5” file should be placed here and all other image data should be in a folder named spatial. As the IDs created by GEO are completely arbitrary and not related to the underlying question we will rename to some relating to the sample groups. This labels for this are found in the title column of the meta data."
  }
]